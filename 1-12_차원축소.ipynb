{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35bff4f3",
      "metadata": {},
      "source": [
        "# 차원 축소 (Dimensionality Reduction)\n",
        "\n",
        "## 1. 개요\n",
        "\n",
        "### 배경\n",
        "현실 데이터는 수십, 수백 개의 특성을 가진 **고차원 데이터**인 경우가 많습니다.  \n",
        "차원이 높아지면 → 시각화가 어려워지고, 계산량이 폭발하며, 모델이 과적합되는 문제가 발생합니다.  \n",
        "\n",
        "👉 **차원 축소(Dimensionality Reduction)** 는  \n",
        "- 데이터의 중요한 정보를 유지하면서,  \n",
        "- 불필요하거나 중복된 차원을 줄여  \n",
        "- 시각화, 처리 효율성, 모델 성능 향상에 기여합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c2ed8c",
      "metadata": {},
      "source": [
        "## 학습 목표\n",
        "이 토픽을 수강한 뒤, 수강생은 다음을 할 수 있어야 합니다:\n",
        "\n",
        "- 차원 축소의 필요성과 이점을 설명할 수 있다.  \n",
        "- PCA와 LDA의 원리를 이해하고 실제 데이터에 적용할 수 있다.  \n",
        "- 차원 축소를 통해 데이터 시각화 및 모델 성능 평가를 수행할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db90a64",
      "metadata": {},
      "source": [
        "## 주요 학습 개념\n",
        "\n",
        "- **차원 축소란?**\n",
        "  - 차원 축소의 정의와 필요성  \n",
        "  - 차원 축소의 이점 (시각화, 처리 효율성, 성능 향상 등)  \n",
        "\n",
        "- **PCA (Principal Component Analysis)**  \n",
        "  - PCA의 기본 개념  \n",
        "  - 공분산 행렬과 고유벡터  \n",
        "  - 주성분 선택 원리  \n",
        "  - scikit-learn을 사용한 PCA 구현  \n",
        "\n",
        "- **LDA (Linear Discriminant Analysis)**  \n",
        "  - LDA의 기본 개념  \n",
        "  - 클래스 분리 기준 (between-class variance / within-class variance)  \n",
        "  - scikit-learn을 사용한 LDA 구현  \n",
        "\n",
        "- **차원 축소 활용**  \n",
        "  - 차원 축소를 통한 데이터 시각화 (2D, 3D 표현)  \n",
        "  - 차원 축소 후 모델 성능 비교  \n",
        "  - 차원 축소 vs 피처 선택 차이점  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb6783b",
      "metadata": {},
      "source": [
        "## 0. 들어가기\n",
        "\n",
        "데이터가 가진 **차원(dimension)** 이란, 하나의 데이터가 가지고 있는 **특성(feature)** 의 개수를 의미합니다.  \n",
        "예:  \n",
        "- 키, 몸무게, 나이 → 3차원 데이터  \n",
        "- 픽셀 784개로 이루어진 MNIST 숫자 이미지 → 784차원 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0565c63",
      "metadata": {},
      "source": [
        "👉 차원이 커질수록 나타나는 문제:  \n",
        "- **시각화 불가능**: 3차원 이상은 인간이 직관적으로 보기 어렵다.  \n",
        "- **계산량 폭발**: 차원이 늘어나면 거리 계산, 행렬 연산이 급격히 늘어난다.  \n",
        "- **차원의 저주(Curse of Dimensionality)**: 차원이 커질수록 데이터가 희소해져 모델 학습이 어려워진다.  \n",
        "\n",
        "  <img src=\"image/dimensionality.png\">  \n",
        "\n",
        "이미지 출처 : https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/#google_vignette\n",
        "\n",
        "따라서 차원을 줄이는 기술, 즉 **차원 축소(Dimensionality Reduction)** 가 필요하다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ddc72f",
      "metadata": {},
      "source": [
        "## 1. 차원 축소 기본 개념\n",
        "\n",
        "### 1.1 차원 축소란?\n",
        "- 원래 데이터의 중요한 정보를 유지하면서 **특성의 개수를 줄이는 과정**  \n",
        "- 불필요한 변수 제거, 데이터 압축, 시각화에 활용  \n",
        "\n",
        "<img src=\"image/Dimensionality_Reduction.jpg\">  \n",
        "\n",
        "이미지 출처 : https://blog.roboflow.com/what-is-dimensionality-reduction/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa16c4d",
      "metadata": {},
      "source": [
        "### 1.2 차원 축소의 이점\n",
        "1. **시각화 용이**  \n",
        "   - 2D, 3D로 축소하여 데이터 패턴을 그림으로 확인 가능  \n",
        "2. **처리 효율성 향상**  \n",
        "   - 계산량이 줄어 모델 훈련 속도 증가  \n",
        "3. **잡음 제거**  \n",
        "   - 중요하지 않은 특성을 줄여 모델 성능 향상  \n",
        "4. **과적합 방지**  \n",
        "   - 복잡성을 줄여 일반화 성능 향상"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79dd1e32",
      "metadata": {},
      "source": [
        "### 1.3 차원 축소 방법\n",
        "- **PCA (주성분 분석)**: 분산이 가장 큰 방향을 찾아 차원 축소  \n",
        "- **LDA (선형 판별 분석)**: 클래스 간 분리를 최대화하는 방향으로 축소  \n",
        "- (심화) t-SNE, UMAP: 비선형 데이c터 시각화에 강력"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d088f717",
      "metadata": {},
      "source": [
        "### 1.4 간단한 예시 (Iris 데이터셋 시각화 전후)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b373adfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) # FutureWarning 숨기기\n",
        "\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df[\"target\"] = iris.target\n",
        "\n",
        "# ---------------------\n",
        "# 전(before): 4D → pairplot\n",
        "# ---------------------\n",
        "sns.pairplot(df, hue=\"target\", vars=iris.feature_names)\n",
        "plt.suptitle(\"Before Dimensionality Reduction (4D Feature Space)\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# ---------------------\n",
        "# 후(after): PCA → 2D\n",
        "# ---------------------\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(iris.data)\n",
        "\n",
        "df_pca = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
        "df_pca[\"target\"] = iris.target\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"target\", data=df_pca, palette=\"Set1\")\n",
        "plt.title(\"After Dimensionality Reduction (PCA 2D)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f849f3a",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 차원 = 데이터의 특성 개수  \n",
        "- 차원이 높으면 계산량과 복잡성이 증가 → 차원의 저주 발생  \n",
        "- 차원 축소는 시각화, 효율성, 성능 향상에 도움을 준다  \n",
        "- 대표 기법: PCA, LDA (→ 다음 장에서 학습)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a654f113",
      "metadata": {},
      "source": [
        "## 2. PCA (Principal Component Analysis)\n",
        "\n",
        "### 2.1 PCA란?\n",
        "- **고차원 데이터를 저차원으로 축소**하는 가장 대표적인 기법  \n",
        "- 데이터의 **분산(variance)** 이 가장 큰 방향을 찾아 새로운 축(주성분, Principal Component)을 만든다  \n",
        "- 이렇게 만든 축을 따라 데이터를 투영하여 차원을 줄인다  \n",
        "\n",
        "👉 핵심 아이디어:  \n",
        "\"데이터를 가장 잘 설명할 수 있는 방향(축)을 찾아, 그 축만 남기고 나머지는 버리자\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8776fbed",
      "metadata": {},
      "source": [
        "### 2.2 PCA 절차 (상세 설명)\n",
        "\n",
        "#### **1. 데이터 표준화 (평균=0, 표준편차=1)**  \n",
        "\n",
        "PCA는 **변수의 스케일(단위)** 에 매우 민감하다.  \n",
        "예를 들어, 키(cm)와 몸무게(kg)를 함께 쓰면, 단위가 큰 쪽이 분산을 지배하게 된다.  \n",
        "이를 방지하기 위해 **평균을 0, 표준편차를 1**로 맞추는 과정을 거친다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4605cb72",
      "metadata": {},
      "source": [
        "#### **2. 공분산 행렬 계산 (Covariance Matrix)**  \n",
        "\n",
        "표준화된 데이터의 변수들 간의 관계(변동 방향과 크기)를 보기 위해 **공분산행렬**을 만든다.\n",
        "\n",
        "공분산의 정의:\n",
        "\n",
        "$\n",
        "\\text{Cov}(X, Y) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n",
        "$\n",
        "\n",
        "- Cov > 0 → 두 변수가 **같은 방향으로 변함**  \n",
        "- Cov < 0 → 두 변수가 **반대 방향으로 변함**  \n",
        "- Cov ≈ 0 → **거의 무관함**\n",
        "#### **공분산행렬의 형태 예시**\n",
        "\n",
        "2개의 변수 \\(X_1, X_2\\)에 대해:\n",
        "\n",
        "$$\n",
        "\\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) \\\\\n",
        "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "예를 들어:\n",
        "$$\n",
        "\\Sigma =\n",
        "\\begin{bmatrix}\n",
        "4 & 2 \\\\\n",
        "2 & 3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- **대각선 원소**는 각 변수의 **분산(Var)**  \n",
        "- **비대각선 원소**는 변수 간의 **공분산(Cov)**  \n",
        "- Cov가 양수 → 두 변수가 같은 방향으로 변동  \n",
        "- Cov가 음수 → 반대 방향으로 변동  \n",
        "- Cov가 0에 가까움 → 두 변수가 독립적\n",
        "\n",
        "공분산이란,  \n",
        "> “각 표본이 평균으로부터 얼마나 벗어나 있는지를 곱해본 뒤,  \n",
        "> 그 값이 일관되게 양수인지 음수인지 확인하는 것”  \n",
        "만약 +와 −가 섞여서 상쇄되면 Cov≈0 → 상관 없음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370cf722",
      "metadata": {},
      "source": [
        "\n",
        "#### **3. 공분산 행렬의 고유벡터와 고유값 계산** \n",
        "공분산행렬 $\\Sigma$에 대해 **고유값 분해(Eigen Decomposition)** 를 수행한다.\n",
        "\n",
        "$\n",
        "\\Sigma v = \\lambda v\n",
        "$\n",
        "\n",
        "- $(v$): 고유벡터(Eigenvector) → 데이터가 가장 퍼진 방향 (새 축, 주성분)\n",
        "- $(\\lambda$): 고유값(Eigenvalue) → 그 방향의 분산 크기\n",
        "\n",
        "즉, 고유값이 클수록 **데이터가 그 축을 따라 더 많이 퍼져 있다**는 의미이다.\n",
        "\n",
        "#### 4. 고유값이 큰 순서대로 주성분 선택  \n",
        "고유값이 가장 큰 벡터부터 차례로 정렬하면  \n",
        "데이터의 분산(정보량)을 가장 잘 설명하는 축 순서대로 정렬된다.\n",
        "\n",
        "예를 들어:\n",
        "\n",
        "| 주성분 | 고유값(분산 비율) | 설명 |\n",
        "|----------|--------------------|--------|\n",
        "| PC1 | 3.2 (80%) | 데이터의 대부분 변동을 설명 |\n",
        "| PC2 | 0.8 (20%) | 잔여 변동 설명 |\n",
        "\n",
        "차원을 2 → 1로 줄일 경우, PC1만 사용한다.\n",
        "#### 5. 원래 데이터를 주성분 축으로 변환\n",
        "\n",
        "원래 데이터를 고유벡터(주성분 축)에 투영(projection)하여  \n",
        "새로운 좌표로 표현한다.\n",
        "\n",
        "$ Z = X W $\n",
        "\n",
        "- X: 표준화된 원본 데이터  \n",
        "- W: 선택된 주성분 벡터들  \n",
        "- Z: 변환된 데이터 (차원 축소 결과)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10aa9ae5",
      "metadata": {},
      "source": [
        "### 2.3 PCA 시각적 직관\n",
        "\n",
        "2차원 데이터를 예로 들어보자.  \n",
        "데이터가 다음과 같이 **대각선 방향(↗)** 으로 퍼져 있다고 가정하자.\n",
        "\n",
        "```markdown\n",
        "y ↑\n",
        "  |\n",
        "  |         ●\n",
        "  |      ●\n",
        "  |   ●\n",
        "  |●\n",
        "  |__________________________→ x\n",
        "```\n",
        "\n",
        "이 경우, 원래의 x·y축은 데이터의 분포를 제대로 설명하지 못한다.  \n",
        "데이터가 실제로 퍼진 방향(대각선 방향)을 기준으로 보면, 데이터의 **변동성(분산)** 이 훨씬 크다.\n",
        "\n",
        "👉 따라서, PCA는 다음과 같은 판단을 내린다.\n",
        "\n",
        "> “데이터가 실제로 퍼진 방향이 정보(분산)를 가장 많이 담고 있으니, 그 방향을 새 축으로 삼자.”\n",
        "\n",
        "\n",
        "####  PCA가 하는 일\n",
        "1. 데이터의 분산이 가장 큰 방향(↗)을 찾아 **새로운 축(PC1)** 으로 설정한다.  \n",
        "2. PC1에 **직교하는 방향(↘)** 을 **두 번째 축(PC2)** 으로 설정한다.  \n",
        "3. 데이터를 이 두 축으로 **회전(변환)** 시켜 새 좌표계로 표현한다.  \n",
        "4. 정보 손실을 최소화하기 위해 **분산이 큰 축(PC1)** 만 남기고  \n",
        "   나머지 축(PC2)은 **제거(차원 축소)** 한다.\n",
        "\n",
        "\n",
        "####  핵심 개념 정리\n",
        "\n",
        "- **PC1 (첫 번째 주성분)**:  \n",
        "  데이터가 가장 넓게 퍼진 방향 (분산 최대, 핵심 정보 축)\n",
        "\n",
        "- **PC2 (두 번째 주성분)**:  \n",
        "  PC1에 직교하는 방향 (잔여 변동, 미세한 노이즈)\n",
        "\n",
        "- PCA는 데이터를 이 두 축으로 다시 표현하고,  \n",
        "  정보가 많은 **PC1만 남겨** 데이터의 차원을 줄인다.\n",
        "\n",
        "\n",
        "> **요약**\n",
        ">  \n",
        "> PCA는 “공분산행렬을 고유값분해하여,  \n",
        "> 데이터의 분산이 가장 큰 방향(즉, 정보가 가장 많은 방향)을 찾아  \n",
        "> 그 축으로 데이터를 재표현하고,  \n",
        "> 필요에 따라 저차원으로 축소하는 기법”이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5643d04",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "### 2.4 Scikit-learn으로 PCA 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e41e5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 1) 데이터 표준화\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# 2) PCA (2차원으로 축소)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 3) DataFrame 변환\n",
        "df_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
        "df_pca[\"target\"] = y\n",
        "\n",
        "# 4) 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "for target in set(y):\n",
        "    subset = df_pca[df_pca[\"target\"] == target]\n",
        "    plt.scatter(subset[\"PC1\"], subset[\"PC2\"], label=iris.target_names[target])\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.legend()\n",
        "plt.title(\"PCA on Iris Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68417727",
      "metadata": {},
      "source": [
        "### 2.5 주성분의 분산 설명력\n",
        "PCA는 각 주성분이 원래 데이터의 분산을 얼마나 설명하는지도 알려준다.  \n",
        "\n",
        "```python\n",
        "print(\"주성분별 설명된 분산 비율:\", pca.explained_variance_ratio_)\n",
        "print(\"설명된 총 분산 비율:\", sum(pca.explained_variance_ratio_))\n",
        "```\n",
        "\n",
        "👉 일반적으로, **앞 몇 개의 주성분만으로도 대부분의 정보를 유지**할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fe98cd",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- PCA는 데이터의 **분산이 큰 방향**을 찾아 차원을 줄이는 기법이다.  \n",
        "- 주성분은 공분산 행렬의 고유벡터로 정의된다.  \n",
        "- Scikit-learn으로 손쉽게 PCA를 적용할 수 있다.  \n",
        "- `explained_variance_ratio_`를 통해 정보 보존 정도를 확인할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e04f63d",
      "metadata": {},
      "source": [
        "## 3. LDA (Linear Discriminant Analysis)\n",
        "\n",
        "### 3.1 LDA란?\n",
        "- **지도 학습 기반 차원 축소 기법**  \n",
        "- PCA는 클래스(label)를 고려하지 않지만, **LDA는 클래스 정보를 사용**  \n",
        "- 목표: 서로 다른 클래스 간 분리가 잘 되도록 새로운 축을 찾는 것  \n",
        "\n",
        "👉 직관적으로, \"클래스 간의 거리는 최대화, 클래스 내부의 분산은 최소화\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4aea777",
      "metadata": {},
      "source": [
        "### 3.2 LDA 절차\n",
        "1. 클래스별 평균과 전체 평균 계산  \n",
        "2. 클래스 간 분산(between-class scatter)과 클래스 내 분산(within-class scatter) 계산  \n",
        "3. 두 분산 비율을 최대화하는 축 찾기  \n",
        "4. 데이터를 그 축에 투영  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c0a350",
      "metadata": {},
      "source": [
        "### 3.3 LDA와 PCA 차이점\n",
        "- PCA: **데이터의 분산**을 가장 잘 설명하는 축 찾기 (비지도 학습)  \n",
        "- LDA: **클래스 간 분리**를 가장 잘 설명하는 축 찾기 (지도 학습)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd56747a",
      "metadata": {},
      "source": [
        "### 3.4 Scikit-learn으로 LDA 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769db1dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# LDA (2차원으로 축소)\n",
        "lda = LDA(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "# DataFrame 변환\n",
        "df_lda = pd.DataFrame(X_lda, columns=[\"LD1\", \"LD2\"])\n",
        "df_lda[\"target\"] = y\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "for target in set(y):\n",
        "    subset = df_lda[df_lda[\"target\"] == target]\n",
        "    plt.scatter(subset[\"LD1\"], subset[\"LD2\"], label=iris.target_names[target])\n",
        "plt.xlabel(\"LD1\")\n",
        "plt.ylabel(\"LD2\")\n",
        "plt.legend()\n",
        "plt.title(\"LDA on Iris Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e467faf",
      "metadata": {},
      "source": [
        "### 3.5 LDA의 활용\n",
        "- **시각화**: 고차원 데이터를 2D/3D로 줄여 시각적으로 클래스 분포 확인  \n",
        "- **분류 성능 향상**: 차원을 줄이면서도 클래스 정보를 반영하여 더 잘 분리된 데이터 생성  \n",
        "- **전처리 단계**: 분류 모델 적용 전 데이터 축소"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61d9d94b",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- LDA는 클래스 정보를 활용하는 지도학습 기반 차원 축소 기법이다.  \n",
        "- 클래스 간 분산은 최대화, 클래스 내 분산은 최소화한다.  \n",
        "- PCA와 달리 LDA는 \"클래스 분리\"에 초점이 있다.  \n",
        "- Scikit-learn을 통해 손쉽게 LDA를 적용하고 시각화할 수 있다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5968b0e3",
      "metadata": {},
      "source": [
        "## 4. 차원 축소 활용\n",
        "\n",
        "### 4.1 데이터 시각화\n",
        "차원 축소의 가장 직관적인 효과는 **고차원 데이터를 2D 또는 3D로 시각화**할 수 있다는 점입니다.  \n",
        "\n",
        "예: Iris 데이터셋을 PCA와 LDA로 축소 후 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e635e35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 이미 PCA, LDA로 변환된 데이터 (X_pca, X_lda)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCA 결과 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=\"viridis\", edgecolor=\"k\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Visualization (Iris)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# LDA 결과 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_lda[:,0], X_lda[:,1], c=y, cmap=\"rainbow\", edgecolor=\"k\")\n",
        "plt.xlabel(\"LD1\")\n",
        "plt.ylabel(\"LD2\")\n",
        "plt.title(\"LDA Visualization (Iris)\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56cf80df",
      "metadata": {},
      "source": [
        "👉 결과:  \n",
        "- PCA는 데이터의 분산을 최대한 보존 → 전체 구조 파악  \n",
        "- LDA는 클래스 분리를 최적화 → 그룹이 더 명확히 구분"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8f62f2",
      "metadata": {},
      "source": [
        "### 4.2 모델 성능 비교\n",
        "차원 축소 후 **모델을 학습**하여 성능 차이를 비교할 수 있습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d979ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터 분할 (원본 데이터 vs PCA 변환 데이터)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 원본 데이터\n",
        "lr = LogisticRegression(max_iter=200)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "print(\"원본 데이터 정확도:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# PCA 데이터\n",
        "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_pca_train, y_train)\n",
        "y_pred_pca = lr.predict(X_pca_test)\n",
        "print(\"PCA 데이터 정확도:\", accuracy_score(y_test, y_pred_pca))\n",
        "\n",
        "# LDA 데이터\n",
        "X_lda_train, X_lda_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_lda_train, y_train)\n",
        "y_pred_lda = lr.predict(X_lda_test)\n",
        "print(\"LDA 데이터 정확도:\", accuracy_score(y_test, y_pred_lda))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7671edd5",
      "metadata": {},
      "source": [
        "👉 보통:  \n",
        "- PCA → 데이터 압축 후에도 성능 유지 또는 향상 가능  \n",
        "- LDA → 클래스 분리도를 높여 분류 성능 개선 \n",
        "\n",
        "### 4.3 피처 선택 vs 차원 축소\n",
        "- **피처 선택 (Feature Selection)**  \n",
        "  - 원래의 변수 중 중요한 것만 고름  \n",
        "  - 해석이 쉽다 (예: “꽃받침 길이와 꽃잎 폭이 중요하다”)  \n",
        "\n",
        "- **차원 축소 (Dimensionality Reduction)**  \n",
        "  - 원래의 변수를 조합해 새로운 변수(주성분, 판별축)를 생성  \n",
        "  - 해석은 어렵지만 더 효율적일 수 있다  \n",
        "\n",
        "👉 즉, 피처 선택은 \"원래 특성 중 일부만 고르기\", 차원 축소는 \"새로운 특성 만들기\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c75059",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 차원 축소는 데이터 시각화, 계산 효율, 성능 향상에 유용하다.  \n",
        "- PCA는 분산 보존, LDA는 클래스 분리에 초점.  \n",
        "- 차원 축소 후에도 모델 성능을 반드시 다시 평가해야 한다.  \n",
        "- 피처 선택과 차원 축소는 다른 접근 방식이므로 구분해야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cbcadf7",
      "metadata": {},
      "source": [
        "### 차원 축소 – 실습 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc90400a",
      "metadata": {},
      "source": [
        "### 문제 1. PCA 변환\n",
        "Iris 데이터셋을 불러와서,  \n",
        "`n_components=2`인 PCA를 적용한 뒤 2차원 산점도로 시각화하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 표준화\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# PCA 적용\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 시각화\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=\"viridis\", edgecolor=\"k\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA on Iris\")\n",
        "plt.show()\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5a0afd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요 \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537786a1",
      "metadata": {},
      "source": [
        "### 문제 2. 주성분 분산 설명력\n",
        "문제 1에서 학습한 PCA 객체의  \n",
        "각 주성분별 **설명된 분산 비율**과 **누적 설명된 분산 비율**을 출력하세요.  \n",
        "\n",
        "#### 💡 개념 설명\n",
        "- **설명된 분산 비율(Explained Variance Ratio)**  \n",
        "  각 **주성분(Principal Component)** 이  \n",
        "  원래 데이터의 전체 분산(정보량) 중에서 **얼마나 많은 부분을 설명하는가**를 의미합니다.  \n",
        "  - 예를 들어 `[0.65, 0.25, 0.10]`이면  \n",
        "    - 첫 번째 주성분이 전체 데이터 변동의 65%  \n",
        "    - 두 번째가 25%  \n",
        "    - 세 번째가 10%를 설명한다는 뜻입니다.  \n",
        "\n",
        "- **누적 설명된 분산 비율(Cumulative Explained Variance Ratio)**  \n",
        "  첫 번째부터 n번째 주성분까지 합쳤을 때  \n",
        "  **전체 데이터 분산 중 얼마나 보존되는가**를 의미합니다.  \n",
        "  - 예: `[0.65, 0.25, 0.10]` → 누적값 `[0.65, 0.90, 1.00]`  \n",
        "  - 따라서, “누적 설명된 분산 비율이 0.9 이상이 되는 최소 주성분 수”는  \n",
        "    **데이터 정보의 90% 이상을 유지하면서 차원을 줄일 수 있는 기준**이 됩니다.\n",
        "    \n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "print(\"주성분별 설명된 분산 비율:\", pca.explained_variance_ratio_)\n",
        "print(\"누적 설명된 분산 비율:\", sum(pca.explained_variance_ratio_))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3199ff9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"주성분별 설명된 분산 비율:\", pca.explained_variance_ratio_)\n",
        "print(\"누적 설명된 분산 비율:\", sum(pca.explained_variance_ratio_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5496ea1e",
      "metadata": {},
      "source": [
        "### 문제 3. LDA 변환\n",
        "Iris 데이터셋에 LDA를 적용해 2차원으로 축소하고,  \n",
        "클래스별로 다른 색으로 시각화하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "lda = LDA(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "plt.scatter(X_lda[:,0], X_lda[:,1], c=y, cmap=\"rainbow\", edgecolor=\"k\")\n",
        "plt.xlabel(\"LD1\")\n",
        "plt.ylabel(\"LD2\")\n",
        "plt.title(\"LDA on Iris\")\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c4c6b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요 \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6717697c",
      "metadata": {},
      "source": [
        "### 문제 4. 차원 축소 후 분류 모델 성능 비교\n",
        "Iris 데이터셋에서 **로지스틱 회귀** 모델을 적용했을 때,  \n",
        "- 원본 데이터,  \n",
        "- PCA 변환 데이터,  \n",
        "- LDA 변환 데이터  \n",
        "\n",
        "의 정확도를 비교하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 원본 데이터\n",
        "iris = load_iris()\n",
        "X,y = iris.data, iris.target\n",
        "\n",
        "\n",
        "# PCA 적용\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# LDA 적용\n",
        "lda = LDA(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "lr = LogisticRegression(max_iter=200)\n",
        "lr.fit(X_train, y_train)\n",
        "print(\"원본 데이터 정확도:\", accuracy_score(y_test, lr.predict(X_test)))\n",
        "\n",
        "# PCA 데이터\n",
        "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_pca_train, y_train)\n",
        "print(\"PCA 데이터 정확도:\", accuracy_score(y_test, lr.predict(X_pca_test)))\n",
        "\n",
        "# LDA 데이터\n",
        "X_lda_train, X_lda_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_lda_train, y_train)\n",
        "print(\"LDA 데이터 정확도:\", accuracy_score(y_test, lr.predict(X_lda_test)))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae756d46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요 \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 원본 데이터\n",
        "iris = load_iris()\n",
        "X,y = iris.data, iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e1d94c",
      "metadata": {},
      "source": [
        "### 문제 5. 피처 선택 vs 차원 축소\n",
        "피처 선택과 차원 축소의 차이를 간단히 설명하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "- **피처 선택 (Feature Selection)**: 원래의 특성 중 중요한 일부만 선택 (예: 변수 중요도 기반 선택). 해석이 쉽다.  \n",
        "- **차원 축소 (Dimensionality Reduction)**: 원래 특성을 조합해 새로운 특성(주성분, 판별축)을 생성. 해석은 어렵지만 효율적일 수 있다.  \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77845b35",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- PCA는 데이터의 분산을 최대한 보존하는 축으로 투영한다.  \n",
        "- LDA는 클래스 분리를 최대화하는 축으로 데이터를 투영한다.  \n",
        "- 차원 축소 후에도 모델 성능을 반드시 재평가해야 한다.  \n",
        "- 피처 선택과 차원 축소는 서로 다른 접근 방식이다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49dcff2a",
      "metadata": {},
      "source": [
        "### 미니 프로젝트 : 상수관로 누수 여부 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa70c1df",
      "metadata": {},
      "source": [
        "**목표 :**\n",
        "차원축소를 활용하여 데이터 전처리 후 성능 비교 해보기\n",
        "\n",
        "**주제 :** 상수관로에 설치된 누수 감지 센서 데이터를 활용하여 **누수 여부를** 5가지 클래스로 분류\n",
        "\n",
        "**문제 설명 :**  \n",
        "\n",
        "제공된 데이터를 기반으로 **누수 여부(`leaktype`)를 예측**하는 AI 모델을 개발  \n",
        "**다중 클래스 분류(multi-class classification)** 형태이며, 출력값은 **문자열(string)** 형태의 5개 클래스 중 하나  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e5d4d8",
      "metadata": {},
      "source": [
        "**데이터 설명 :**\n",
        "\n",
        "| 항목 | 설명 | 항목 | 설명 |\n",
        "|------|------|------|------|\n",
        "| `site` | 사이트 번호 | `sid` | 센서 번호 |\n",
        "| `ldate` | 누수 감지 일자 | `lrate` | 누수 확률 |\n",
        "| `llevel` | 누수 레벨 | `leaktype` | 누수 감지 클래스 (**예측 대상**) |\n",
        "| `0Hz ~ 5120Hz` | 주파수별 감지된 누수 진동 크기 | `MAX~` | 감지 횟수 중 최대 주파수 및 최대 누수 크기 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1f9e95",
      "metadata": {},
      "source": [
        "**예측 대상 (Target: `leaktype`)**\n",
        "\n",
        "| 클래스명 | 의미 |\n",
        "|-----------|------|\n",
        "| `out` | 옥외 누수 |\n",
        "| `in` | 옥내 누수 |\n",
        "| `noise` | 기계·전기음 |\n",
        "| `other` | 환경음 |\n",
        "| `normal` | 정상음 |\n",
        "\n",
        "**평가 지표**\n",
        "- **Accuracy (정확도)** : 모델이 전체 데이터 중 정답을 맞춘 비율로 평가합니다.  \n",
        "\n",
        "**활용 가능성**  \n",
        "- 본 데이터는 **상수관로의 누수 및 파손 감지**, **상태 진단**, **스마트 워터 관리 시스템 개발**에 활용될 수 있습니다.\n",
        "\n",
        "**데이터 출처**\n",
        "- AI Hub — [상수관로 누수 감지 데이터 기반 AI 학습용 데이터셋](https://aihub.or.kr/aidata/27709)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617564b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71839955",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0. 데이터 불러오기\n",
        "df = pd.read_csv('data/1.누수감지데이터-통합(leaks-all).csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd496a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 주파수 특성 컬럼만 추출\n",
        "freq_cols = [c for c in df.columns if c.endswith(\"HZ\") or c.startswith(\"MAX\")]\n",
        "df_freq = df[freq_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb3a146",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 타깃 인코딩\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['leaktype_enc'] = le.fit_transform(df['leaktype'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f87e55a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. EDA\n",
        "print(\"데이터 크기:\", df.shape)\n",
        "print(\"타깃 분포:\\n\", df['leaktype'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af21cb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 타깃 분포 시각화\n",
        "sns.countplot(x='leaktype', data=df)\n",
        "plt.title(\"Leaktype Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a74768",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 주파수 특성 기초 통계\n",
        "print(df_freq.describe().T.head(10))\n",
        "\n",
        "# 결측치 확인\n",
        "print(\"결측치 수:\\n\", df.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196f6373",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 차원축소 전 모델 (원본 feature 사용)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_freq, df['leaktype_enc'], test_size=0.2, random_state=42, stratify=df['leaktype_enc']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc70c0ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_orig = XGBClassifier(\n",
        "    max_depth=14, \n",
        "    n_estimators=100, \n",
        "    n_jobs=4, \n",
        "    random_state=42,\n",
        "    use_label_encoder=False, \n",
        "    eval_metric=\"mlogloss\"\n",
        ")\n",
        "model_orig.fit(X_train, y_train)\n",
        "pred_orig = model_orig.predict(X_test)\n",
        "\n",
        "acc_orig = accuracy_score(y_test, pred_orig)\n",
        "f1_orig = f1_score(y_test, pred_orig, average=\"macro\")\n",
        "print(\"차원축소 전 - Accuracy:\", acc_orig, \"F1:\", f1_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647243f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. 차원축소 후 모델 (PCA 적용)\n",
        "n_components = 120\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(df_freq)\n",
        "\n",
        "Xp_train, Xp_test, yp_train, yp_test = train_test_split(\n",
        "    X_pca, df['leaktype_enc'], test_size=0.2, random_state=42, stratify=df['leaktype_enc']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75ff62c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_pca = XGBClassifier(\n",
        "    max_depth=14, \n",
        "    n_estimators=100, \n",
        "    n_jobs=4, \n",
        "    random_state=42,\n",
        "    use_label_encoder=False, \n",
        "    eval_metric=\"mlogloss\"\n",
        ")\n",
        "model_pca.fit(Xp_train, yp_train)\n",
        "pred_pca = model_pca.predict(Xp_test)\n",
        "\n",
        "acc_pca = accuracy_score(yp_test, pred_pca)\n",
        "f1_pca = f1_score(yp_test, pred_pca, average=\"macro\")\n",
        "print(\"차원축소 후(PCA) - Accuracy:\", acc_pca, \"F1:\", f1_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aca098f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix 비교 (차원축소 전 vs 후)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# 차원축소 전\n",
        "cm_orig = confusion_matrix(y_test, pred_orig)\n",
        "sns.heatmap(pd.DataFrame(cm_orig, index=le.classes_, columns=le.classes_),\n",
        "            annot=True, fmt=\"d\", cmap=\"viridis\", ax=axes[0])\n",
        "axes[0].set_title(\"차원축소 전\", fontsize=14)\n",
        "axes[0].set_xlabel(\"Predicted\")\n",
        "axes[0].set_ylabel(\"True\")\n",
        "\n",
        "# 차원축소 후 (PCA)\n",
        "cm_pca = confusion_matrix(yp_test, pred_pca)\n",
        "sns.heatmap(pd.DataFrame(cm_pca, index=le.classes_, columns=le.classes_),\n",
        "            annot=True, fmt=\"d\", cmap=\"viridis\", ax=axes[1])\n",
        "axes[1].set_title(\"차원축소 후 (PCA)\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Predicted\")\n",
        "axes[1].set_ylabel(\"True\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259eb547",
      "metadata": {},
      "outputs": [],
      "source": [
        "차원이 작을수록 계산 효율이 좋아지지만, 너무 줄이면 정보 손실이 커진다\n",
        "\n",
        "그래서 적정 차원 수(k) 는\n",
        "“정보 보존 ↔ 단순화” 사이의 균형점(trade-off)\n",
        "\n",
        "누적 설명된 분산이 90~95% 이상이면 충분히 정보가 보존된다.”\n",
        "는 것이 일반적인 실무 기준입니다.\n",
        "\n",
        "\n",
        "3️⃣ 2차 기준 — 모델 검증 성능(Validation Performance)\n",
        "\n",
        "설명된 분산만으로는 충분치 않습니다.\n",
        "왜냐하면 PCA는 단지 입력 데이터 구조만 보고 차원을 줄이지만,\n",
        "실제 목표(label) 와의 관계는 고려하지 않기 때문입니다.\n",
        "\n",
        "따라서:\n",
        "\n",
        "PCA로 여러 차원 수(k)를 시도해본 뒤,\n",
        "\n",
        "각 k별로 학습 → 검증셋에서 정확도/ROC-AUC/F1 등을 측정 →\n",
        "\n",
        "과적합 없는 구간(성능 안정 구간)을 선택합니다."
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "MS0xMl/ssKjsm5DstpXshowuaXB5bmI=",
    "inserted_date": [
      "2025-09-19"
    ],
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
