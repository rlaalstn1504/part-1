{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35bff4f3",
      "metadata": {},
      "source": [
        "# 차원 축소 (Dimensionality Reduction)\n",
        "\n",
        "\n",
        "\n",
        "## 1. 개요\n",
        "\n",
        "### 배경\n",
        "현실 데이터는 수십, 수백 개의 특성을 가진 **고차원 데이터**인 경우가 많습니다.  \n",
        "차원이 높아지면 → 시각화가 어려워지고, 계산량이 폭발하며, 모델이 과적합되는 문제가 발생합니다.  \n",
        "\n",
        "👉 **차원 축소(Dimensionality Reduction)** 는  \n",
        "- 데이터의 중요한 정보를 유지하면서,  \n",
        "- 불필요하거나 중복된 차원을 줄여  \n",
        "- 시각화, 처리 효율성, 모델 성능 향상에 기여합니다.  \n",
        "\n",
        "## 2. 학습 목표\n",
        "이 토픽을 수강한 뒤, 수강생은 다음을 할 수 있어야 합니다:\n",
        "\n",
        "- 차원 축소의 필요성과 이점을 설명할 수 있다.  \n",
        "- PCA와 LDA의 원리를 이해하고 실제 데이터에 적용할 수 있다.  \n",
        "- 차원 축소를 통해 데이터 시각화 및 모델 성능 평가를 수행할 수 있다.  \n",
        "\n",
        "## 3. 주요 학습 개념\n",
        "\n",
        "- **차원 축소란?**\n",
        "  - 차원 축소의 정의와 필요성  \n",
        "  - 차원 축소의 이점 (시각화, 처리 효율성, 성능 향상 등)  \n",
        "\n",
        "- **PCA (Principal Component Analysis)**  \n",
        "  - PCA의 기본 개념  \n",
        "  - 공분산 행렬과 고유벡터  \n",
        "  - 주성분 선택 원리  \n",
        "  - scikit-learn을 사용한 PCA 구현  \n",
        "\n",
        "- **LDA (Linear Discriminant Analysis)**  \n",
        "  - LDA의 기본 개념  \n",
        "  - 클래스 분리 기준 (between-class variance / within-class variance)  \n",
        "  - scikit-learn을 사용한 LDA 구현  \n",
        "\n",
        "- **차원 축소 활용**  \n",
        "  - 차원 축소를 통한 데이터 시각화 (2D, 3D 표현)  \n",
        "  - 차원 축소 후 모델 성능 비교  \n",
        "  - 차원 축소 vs 피처 선택 차이점  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb6783b",
      "metadata": {},
      "source": [
        "# 0. 들어가기\n",
        "\n",
        "데이터가 가진 **차원(dimension)** 이란, 하나의 데이터가 가지고 있는 **특성(feature)** 의 개수를 의미합니다.  \n",
        "예:  \n",
        "- 키, 몸무게, 나이 → 3차원 데이터  \n",
        "- 픽셀 784개로 이루어진 MNIST 숫자 이미지 → 784차원 데이터  \n",
        "\n",
        "👉 차원이 커질수록 나타나는 문제:  \n",
        "- **시각화 불가능**: 3차원 이상은 인간이 직관적으로 보기 어렵다.  \n",
        "- **계산량 폭발**: 차원이 늘어나면 거리 계산, 행렬 연산이 급격히 늘어난다.  \n",
        "- **차원의 저주(Curse of Dimensionality)**: 차원이 커질수록 데이터가 희소해져 모델 학습이 어려워진다.  \n",
        "\n",
        "따라서 차원을 줄이는 기술, 즉 **차원 축소(Dimensionality Reduction)** 가 필요하다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ddc72f",
      "metadata": {},
      "source": [
        "# 1. 차원 축소 기본 개념\n",
        "\n",
        "## 1.1 차원 축소란?\n",
        "- 원래 데이터의 중요한 정보를 유지하면서 **특성의 개수를 줄이는 과정**  \n",
        "- 불필요한 변수 제거, 데이터 압축, 시각화에 활용  \n",
        "\n",
        "\n",
        "## 1.2 차원 축소의 이점\n",
        "1. **시각화 용이**  \n",
        "   - 2D, 3D로 축소하여 데이터 패턴을 그림으로 확인 가능  \n",
        "2. **처리 효율성 향상**  \n",
        "   - 계산량이 줄어 모델 훈련 속도 증가  \n",
        "3. **잡음 제거**  \n",
        "   - 중요하지 않은 특성을 줄여 모델 성능 향상  \n",
        "4. **과적합 방지**  \n",
        "   - 복잡성을 줄여 일반화 성능 향상"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79dd1e32",
      "metadata": {},
      "source": [
        "\n",
        "## 1.3 차원 축소 방법\n",
        "- **PCA (주성분 분석)**: 분산이 가장 큰 방향을 찾아 차원 축소  \n",
        "- **LDA (선형 판별 분석)**: 클래스 간 분리를 최대화하는 방향으로 축소  \n",
        "- (심화) t-SNE, UMAP: 비선형 데이터 시각화에 강력"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d088f717",
      "metadata": {},
      "source": [
        "## 1.4 간단한 예시 (Iris 데이터셋 시각화 전후)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b373adfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df[\"target\"] = iris.target\n",
        "\n",
        "# 4차원 데이터를 pairplot으로 확인 (복잡함)\n",
        "sns.pairplot(df, hue=\"target\", vars=iris.feature_names)\n",
        "plt.show()\n",
        "\n",
        "# 4차원 데이터는 시각적으로 해석하기 어려움  \n",
        "# 이후 PCA, LDA 등을 통해 2D로 줄이면 훨씬 직관적으로 볼 수 있음 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f849f3a",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 차원 = 데이터의 특성 개수  \n",
        "- 차원이 높으면 계산량과 복잡성이 증가 → 차원의 저주 발생  \n",
        "- 차원 축소는 시각화, 효율성, 성능 향상에 도움을 준다  \n",
        "- 대표 기법: PCA, LDA (→ 다음 장에서 학습)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a654f113",
      "metadata": {},
      "source": [
        "# 2. PCA (Principal Component Analysis)\n",
        "\n",
        "## 2.1 PCA란?\n",
        "- **고차원 데이터를 저차원으로 변환**하는 가장 대표적인 기법  \n",
        "- 데이터의 **분산(variance)**이 가장 큰 방향을 찾아 새로운 축(주성분, Principal Component)을 만든다  \n",
        "- 이렇게 만든 축을 따라 데이터를 투영하여 차원을 줄인다  \n",
        "\n",
        "👉 핵심 아이디어:  \n",
        "\"데이터를 가장 잘 설명할 수 있는 방향(축)을 찾아, 그 축만 남기고 나머지는 버리자\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8776fbed",
      "metadata": {},
      "source": [
        "## 2.2 PCA 절차\n",
        "1. 데이터 표준화 (평균=0, 표준편차=1)  \n",
        "2. 공분산 행렬 계산  \n",
        "3. 공분산 행렬의 고유벡터와 고유값 계산  \n",
        "4. 고유값이 큰 순서대로 주성분 선택  \n",
        "5. 원래 데이터를 주성분 축으로 변환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10aa9ae5",
      "metadata": {},
      "source": [
        "## 2.3 PCA 시각적 직관\n",
        "예를 들어, 2차원 데이터가 대각선 방향으로 퍼져 있다면  \n",
        "👉 원래 축(x, y축) 대신, **대각선 방향 축**이 데이터의 변동을 더 잘 설명한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5643d04",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 2.4 Scikit-learn으로 PCA 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e41e5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 1) 데이터 표준화\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# 2) PCA (2차원으로 축소)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 3) DataFrame 변환\n",
        "df_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
        "df_pca[\"target\"] = y\n",
        "\n",
        "# 4) 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "for target in set(y):\n",
        "    subset = df_pca[df_pca[\"target\"] == target]\n",
        "    plt.scatter(subset[\"PC1\"], subset[\"PC2\"], label=iris.target_names[target])\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.legend()\n",
        "plt.title(\"PCA on Iris Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68417727",
      "metadata": {},
      "source": [
        "## 2.5 주성분의 분산 설명력\n",
        "PCA는 각 주성분이 원래 데이터의 분산을 얼마나 설명하는지도 알려준다.  \n",
        "\n",
        "```python\n",
        "print(\"주성분별 설명된 분산 비율:\", pca.explained_variance_ratio_)\n",
        "print(\"설명된 총 분산 비율:\", sum(pca.explained_variance_ratio_))\n",
        "```\n",
        "\n",
        "👉 일반적으로, **앞 몇 개의 주성분만으로도 대부분의 정보를 유지**할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fe98cd",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- PCA는 데이터의 **분산이 큰 방향**을 찾아 차원을 줄이는 기법이다.  \n",
        "- 주성분은 공분산 행렬의 고유벡터로 정의된다.  \n",
        "- Scikit-learn으로 손쉽게 PCA를 적용할 수 있다.  \n",
        "- `explained_variance_ratio_`를 통해 정보 보존 정도를 확인할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e04f63d",
      "metadata": {},
      "source": [
        "# 3. LDA (Linear Discriminant Analysis)\n",
        "\n",
        "## 3.1 LDA란?\n",
        "- **지도 학습 기반 차원 축소 기법**  \n",
        "- PCA는 클래스(label)를 고려하지 않지만, **LDA는 클래스 정보를 사용**  \n",
        "- 목표: 서로 다른 클래스 간 분리가 잘 되도록 새로운 축을 찾는 것  \n",
        "\n",
        "👉 직관적으로, \"클래스 간의 거리는 최대화, 클래스 내부의 분산은 최소화\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4aea777",
      "metadata": {},
      "source": [
        "## 3.2 LDA 절차\n",
        "1. 클래스별 평균과 전체 평균 계산  \n",
        "2. 클래스 간 분산(between-class scatter)과 클래스 내 분산(within-class scatter) 계산  \n",
        "3. 두 분산 비율을 최대화하는 축 찾기  \n",
        "4. 데이터를 그 축에 투영  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c0a350",
      "metadata": {},
      "source": [
        "## 3.3 LDA와 PCA 차이점\n",
        "- PCA: **데이터의 분산**을 가장 잘 설명하는 축 찾기 (비지도 학습)  \n",
        "- LDA: **클래스 간 분리**를 가장 잘 설명하는 축 찾기 (지도 학습)  \n",
        "\n",
        "\n",
        "## 3.4 Scikit-learn으로 LDA 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769db1dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# LDA (2차원으로 축소)\n",
        "lda = LDA(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "# DataFrame 변환\n",
        "df_lda = pd.DataFrame(X_lda, columns=[\"LD1\", \"LD2\"])\n",
        "df_lda[\"target\"] = y\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "for target in set(y):\n",
        "    subset = df_lda[df_lda[\"target\"] == target]\n",
        "    plt.scatter(subset[\"LD1\"], subset[\"LD2\"], label=iris.target_names[target])\n",
        "plt.xlabel(\"LD1\")\n",
        "plt.ylabel(\"LD2\")\n",
        "plt.legend()\n",
        "plt.title(\"LDA on Iris Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e467faf",
      "metadata": {},
      "source": [
        "## 3.5 LDA의 활용\n",
        "- **시각화**: 고차원 데이터를 2D/3D로 줄여 시각적으로 클래스 분포 확인  \n",
        "- **분류 성능 향상**: 차원을 줄이면서도 클래스 정보를 반영하여 더 잘 분리된 데이터 생성  \n",
        "- **전처리 단계**: 분류 모델 적용 전 데이터 축소"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61d9d94b",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- LDA는 클래스 정보를 활용하는 지도학습 기반 차원 축소 기법이다.  \n",
        "- 클래스 간 분산은 최대화, 클래스 내 분산은 최소화한다.  \n",
        "- PCA와 달리 LDA는 \"클래스 분리\"에 초점이 있다.  \n",
        "- Scikit-learn을 통해 손쉽게 LDA를 적용하고 시각화할 수 있다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5968b0e3",
      "metadata": {},
      "source": [
        "# 4. 차원 축소 활용\n",
        "\n",
        "## 4.1 데이터 시각화\n",
        "차원 축소의 가장 직관적인 효과는 **고차원 데이터를 2D 또는 3D로 시각화**할 수 있다는 점입니다.  \n",
        "\n",
        "예: Iris 데이터셋을 PCA와 LDA로 축소 후 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e635e35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 이미 PCA, LDA로 변환된 데이터 (X_pca, X_lda)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCA 결과 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=\"viridis\", edgecolor=\"k\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Visualization (Iris)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# LDA 결과 시각화\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_lda[:,0], X_lda[:,1], c=y, cmap=\"rainbow\", edgecolor=\"k\")\n",
        "plt.xlabel(\"LD1\")\n",
        "plt.ylabel(\"LD2\")\n",
        "plt.title(\"LDA Visualization (Iris)\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56cf80df",
      "metadata": {},
      "source": [
        "👉 결과:  \n",
        "- PCA는 데이터의 분산을 최대한 보존 → 전체 구조 파악  \n",
        "- LDA는 클래스 분리를 최적화 → 그룹이 더 명확히 구분"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8f62f2",
      "metadata": {},
      "source": [
        "## 4.2 모델 성능 비교\n",
        "차원 축소 후 **모델을 학습**하여 성능 차이를 비교할 수 있습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d979ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터 분할 (원본 데이터 vs PCA 변환 데이터)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 원본 데이터\n",
        "lr = LogisticRegression(max_iter=200)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "print(\"원본 데이터 정확도:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# PCA 데이터\n",
        "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_pca_train, y_train)\n",
        "y_pred_pca = lr.predict(X_pca_test)\n",
        "print(\"PCA 데이터 정확도:\", accuracy_score(y_test, y_pred_pca))\n",
        "\n",
        "# LDA 데이터\n",
        "X_lda_train, X_lda_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_lda_train, y_train)\n",
        "y_pred_lda = lr.predict(X_lda_test)\n",
        "print(\"LDA 데이터 정확도:\", accuracy_score(y_test, y_pred_lda))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7671edd5",
      "metadata": {},
      "source": [
        "👉 보통:  \n",
        "- PCA → 데이터 압축 후에도 성능 유지 또는 향상 가능  \n",
        "- LDA → 클래스 분리도를 높여 분류 성능 개선  \n",
        "\n",
        "\n",
        "\n",
        "## 4.3 피처 선택 vs 차원 축소\n",
        "- **피처 선택 (Feature Selection)**  \n",
        "  - 원래의 변수 중 중요한 것만 고름  \n",
        "  - 해석이 쉽다 (예: “꽃받침 길이와 꽃잎 폭이 중요하다”)  \n",
        "\n",
        "- **차원 축소 (Dimensionality Reduction)**  \n",
        "  - 원래의 변수를 조합해 새로운 변수(주성분, 판별축)를 생성  \n",
        "  - 해석은 어렵지만 더 효율적일 수 있다  \n",
        "\n",
        "👉 즉, 피처 선택은 \"원래 특성 중 일부만 고르기\", 차원 축소는 \"새로운 특성 만들기\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c75059",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 차원 축소는 데이터 시각화, 계산 효율, 성능 향상에 유용하다.  \n",
        "- PCA는 분산 보존, LDA는 클래스 분리에 초점.  \n",
        "- 차원 축소 후에도 모델 성능을 반드시 다시 평가해야 한다.  \n",
        "- 피처 선택과 차원 축소는 다른 접근 방식이므로 구분해야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cbcadf7",
      "metadata": {},
      "source": [
        "### 차원 축소 – 실습 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc90400a",
      "metadata": {},
      "source": [
        "### 문제 1. PCA 변환\n",
        "Iris 데이터셋을 불러와서,  \n",
        "`n_components=2`인 PCA를 적용한 뒤 2차원 산점도로 시각화하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 표준화\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# PCA 적용\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 시각화\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=\"viridis\", edgecolor=\"k\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA on Iris\")\n",
        "plt.show()\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4d5a0afd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요 \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537786a1",
      "metadata": {},
      "source": [
        "### 문제 2. 주성분 분산 설명력\n",
        "문제 1에서 학습한 PCA 객체의  \n",
        "각 주성분별 **설명된 분산 비율**과 **누적 설명된 분산 비율**을 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "print(\"주성분별 설명된 분산 비율:\", pca.explained_variance_ratio_)\n",
        "print(\"누적 설명된 분산 비율:\", sum(pca.explained_variance_ratio_))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5496ea1e",
      "metadata": {},
      "source": [
        "### 문제 3. LDA 변환\n",
        "Iris 데이터셋에 LDA를 적용해 2차원으로 축소하고,  \n",
        "클래스별로 다른 색으로 시각화하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "lda = LDA(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "plt.scatter(X_lda[:,0], X_lda[:,1], c=y, cmap=\"rainbow\", edgecolor=\"k\")\n",
        "plt.xlabel(\"LD1\")\n",
        "plt.ylabel(\"LD2\")\n",
        "plt.title(\"LDA on Iris\")\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c4c6b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요 \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6717697c",
      "metadata": {},
      "source": [
        "### 문제 4. 차원 축소 후 분류 모델 성능 비교\n",
        "Iris 데이터셋에서 **로지스틱 회귀** 모델을 적용했을 때,  \n",
        "- 원본 데이터,  \n",
        "- PCA 변환 데이터,  \n",
        "- LDA 변환 데이터  \n",
        "\n",
        "의 정확도를 비교하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 원본 데이터\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "lr = LogisticRegression(max_iter=200)\n",
        "lr.fit(X_train, y_train)\n",
        "print(\"원본 데이터 정확도:\", accuracy_score(y_test, lr.predict(X_test)))\n",
        "\n",
        "# PCA 데이터\n",
        "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_pca_train, y_train)\n",
        "print(\"PCA 데이터 정확도:\", accuracy_score(y_test, lr.predict(X_pca_test)))\n",
        "\n",
        "# LDA 데이터\n",
        "X_lda_train, X_lda_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
        "lr.fit(X_lda_train, y_train)\n",
        "print(\"LDA 데이터 정확도:\", accuracy_score(y_test, lr.predict(X_lda_test)))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e1d94c",
      "metadata": {},
      "source": [
        "### 문제 5. 피처 선택 vs 차원 축소\n",
        "피처 선택과 차원 축소의 차이를 간단히 설명하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "- **피처 선택 (Feature Selection)**: 원래의 특성 중 중요한 일부만 선택 (예: 변수 중요도 기반 선택). 해석이 쉽다.  \n",
        "- **차원 축소 (Dimensionality Reduction)**: 원래 특성을 조합해 새로운 특성(주성분, 판별축)을 생성. 해석은 어렵지만 효율적일 수 있다.  \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77845b35",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- PCA는 데이터의 분산을 최대한 보존하는 축으로 투영한다.  \n",
        "- LDA는 클래스 분리를 최대화하는 축으로 데이터를 투영한다.  \n",
        "- 차원 축소 후에도 모델 성능을 반드시 재평가해야 한다.  \n",
        "- 피처 선택과 차원 축소는 서로 다른 접근 방식이다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49dcff2a",
      "metadata": {},
      "source": [
        "### 미니 프로젝트 : 차원축소를 활용하여 데이터 전처리 후 성능 비교 해보기\n",
        "\n",
        "데이터 : 상수관로 누수 감지 데이터\n",
        "링크 : https://aihub.or.kr/aihubdata/data/view.do?dataSetSn=138"
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "MS0xMl/ssKjsm5DstpXshowuaXB5bmI=",
    "inserted_date": [
      "2025-09-19"
    ],
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
