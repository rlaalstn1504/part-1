{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b58e625",
      "metadata": {},
      "source": [
        "#  머신러닝 더 빠르고 정확하게\n",
        "\n",
        "머신러닝은 단순히 알고리즘만 아는 것으로 끝나지 않습니다.  \n",
        "실제 현업에서는 **학습 속도가 느리거나**, **예측 성능이 기대보다 낮은 경우**가 자주 발생합니다.  \n",
        "\n",
        "👉 이번 토픽은 모델 성능을 최적화하기 위한 핵심 기법들을 다룹니다:  \n",
        "- 데이터 전처리  \n",
        "- 정규화(Regularization)  \n",
        "- 모델 평가와 하이퍼파라미터 튜닝  \n",
        "\n",
        "## 학습 목표\n",
        "이 토픽을 수강한 뒤, 수강생은 다음을 할 수 있어야 합니다:\n",
        "\n",
        "- 다양한 데이터 전처리 기법을 이해하고 적용할 수 있다.  \n",
        "- 정규화 기법(L1, L2)을 이해하고 활용할 수 있다.  \n",
        "- 교차 검증과 하이퍼파라미터 튜닝을 통해 모델 성능을 평가하고 개선할 수 있다.  \n",
        "\n",
        "\n",
        "## 목차\n",
        "\n",
        "### 0. 들어가기\n",
        "- 왜 \"빠르고 정확하게\"가 중요한가?  \n",
        "- 데이터 전처리 → 정규화 → 모델 평가 & 튜닝으로 이어지는 흐름  \n",
        "\n",
        "\n",
        "### 1. 데이터 전처리\n",
        "- Feature Scaling\n",
        "  - Normalization (0~1 범위)\n",
        "  - Standardization (평균=0, 표준편차=1)\n",
        "  - scikit-learn 실습\n",
        "- One-hot Encoding\n",
        "  - 범주형 데이터를 수치형으로 변환\n",
        "  - pandas `get_dummies()` 실습\n",
        "\n",
        "\n",
        "\n",
        "### 2. 정규화 (Regularization)\n",
        "- Bias(편향) vs Variance(분산)  \n",
        "- Bias-Variance Tradeoff 개념  \n",
        "- 과적합 방지를 위한 정규화 기법\n",
        "  - L1 정규화 (Lasso)\n",
        "  - L2 정규화 (Ridge)\n",
        "- scikit-learn 실습: Lasso, Ridge 회귀 비교\n",
        "\n",
        "\n",
        "\n",
        "### 3. 모델 평가와 하이퍼파라미터 선택\n",
        "- k겹 교차 검증 (k-Fold Cross Validation)  \n",
        "  - scikit-learn `cross_val_score` 실습\n",
        "- 그리드 서치 (Grid Search)  \n",
        "  - scikit-learn `GridSearchCV` 실습\n",
        "- 최적의 하이퍼파라미터 찾기  \n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 전처리와 정규화는 모델 성능에 직접적으로 영향을 미친다.  \n",
        "- Regularization은 과적합을 방지하는 핵심 도구이다.  \n",
        "- 교차 검증과 그리드 서치는 모델 평가와 성능 개선의 표준 절차이다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f13ba0b",
      "metadata": {},
      "source": [
        "# 0. 들어가기\n",
        "\n",
        "머신러닝 모델을 \"더 빠르고 정확하게\" 만들기 위해 가장 먼저 해야 할 일은 **데이터 전처리(Data Preprocessing)** 입니다.  \n",
        "- 현실 데이터는 크기 단위가 제각각 (예: 키=cm, 몸무게=kg, 월수입=만원)  \n",
        "- 숫자 범위가 다르면, 모델이 특정 특성에 **과도하게 영향을 받음**  \n",
        "- 따라서 데이터를 적절히 스케일링(Scaling)하고 변환해야 학습이 잘 이루어집니다.\n",
        "\n",
        "\n",
        "### 예시: 특성 간 범위 차이 문제\n",
        "\n",
        "데이터에 두 개의 특성이 있다고 합시다:\n",
        "\n",
        "- \\(x_1\\): **키 비율** → 값의 범위: 0 ~ 1  \n",
        "- \\(x_2\\): **년 수입** → 값의 범위: 4000 ~ 10000  \n",
        "\n",
        "### 문제점\n",
        "- 두 특성을 그대로 사용하면, 모델은 계산 과정에서 **값의 크기가 큰 $(x_2$) (1000~2000)** 에 더 큰 가중치를 부여하게 됨.  \n",
        "- 실제로는 $(x_1$) (키 비율)도 중요한데, **숫자 스케일 차이 때문에 모델이 무시**할 수 있음.  \n",
        "\n",
        "\n",
        "### 직관적 비유\n",
        "- 어떤 학생의 성적을 예로 들어봅시다:\n",
        "  - **과목 A (출석점수)**: 0~1점  \n",
        "  - **과목 B (시험점수)**: 1000~2000점  \n",
        "\n",
        "총점을 단순 합으로 계산하면?  \n",
        "- 과목 A 점수는 아무리 변해도 1점 차이  \n",
        "- 과목 B 점수는 최소 1000점 차이  \n",
        "\n",
        "👉 당연히 **시험점수(과목 B)** 가 모든 결과를 좌우하게 됨 → 출석점수는 사실상 무시됨.  \n",
        "\n",
        "\n",
        "### 해결 방법\n",
        "- 데이터를 **스케일링**하여 두 특성이 비슷한 범위를 갖도록 변환해야 함.\n",
        "- 예:\n",
        "  - Min-Max Scaling → 모든 값을 0~1 사이로 맞춤  \n",
        "  - Standardization → 평균=0, 표준편차=1로 변환  \n",
        "\n",
        "이렇게 하면 모델이 **특성의 실제 중요도**를 제대로 반영할 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93ca85a2",
      "metadata": {},
      "source": [
        "# 1. 데이터 전처리\n",
        "\n",
        "## 1.1 Feature Scaling (특성 스케일링)\n",
        "\n",
        "### (1) Normalization (정규화)\n",
        "- 데이터 값을 **0~1 사이로 압축**  \n",
        "- 공식:  \n",
        "  $$\n",
        "  x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
        "  $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edd787f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = np.array([[50], [200], [500]])\n",
        "scaler = MinMaxScaler()\n",
        "normalized = scaler.fit_transform(data)\n",
        "\n",
        "print(\"원본 데이터:\\n\", data)\n",
        "print(\"정규화 데이터:\\n\", normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf2e430",
      "metadata": {},
      "source": [
        "### (2) Standardization (표준화)\n",
        "- 데이터의 평균=0, 표준편차=1로 맞춤  \n",
        "- 공식:  \n",
        "  \n",
        "  $z = \\frac{x - \\mu}{\\sigma}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29723c54",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = np.array([[50], [200], [500]])\n",
        "scaler = StandardScaler()\n",
        "standardized = scaler.fit_transform(data)\n",
        "\n",
        "print(\"표준화 데이터:\\n\", standardized)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7539b19f",
      "metadata": {},
      "source": [
        "👉 경사 하강법(Gradient Descent) 기반 모델은 **스케일링이 필수적**입니다.  \n",
        "특성 범위가 다르면, 어떤 방향으로 먼저 학습할지 혼란이 생기기 때문입니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a9f1f9",
      "metadata": {},
      "source": [
        "## 1.2 One-hot Encoding (범주형 데이터 변환)\n",
        "\n",
        "머신러닝 모델은 숫자만 처리할 수 있습니다.  \n",
        "따라서 범주형 데이터(예: 성별=남/여, 지역=서울/부산/대구)는 **숫자 벡터**로 변환해야 합니다.  \n",
        "\n",
        "- **문제점**: 단순히 \"남=0, 여=1\"처럼 하면, **순서/크기 관계가 생겨버림**  \n",
        "- **해결책**: One-hot Encoding → 각 범주를 별도의 열로 분리, 해당 범주에만 1, 나머지는 0  \n",
        "\n",
        "### 예시 (Gender 컬럼 변환 전/후)\n",
        "\n",
        "| Index | Gender |\n",
        "|-------|--------|\n",
        "| 0     | Male   |\n",
        "| 1     | Female |\n",
        "| 2     | Female |\n",
        "| 3     | Male   |\n",
        "\n",
        "👇 One-hot Encoding 적용 후\n",
        "\n",
        "| Index | Gender_Female | Gender_Male |\n",
        "|-------|---------------|-------------|\n",
        "| 0     | 0             | 1           |\n",
        "| 1     | 1             | 0           |\n",
        "| 2     | 1             | 0           |\n",
        "| 3     | 0             | 1           |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712a8fdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"Gender\": [\"Male\", \"Female\", \"Female\", \"Male\"]})\n",
        "encoded = pd.get_dummies(df, columns=[\"Gender\"])\n",
        "\n",
        "print(df)\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e6d322",
      "metadata": {},
      "source": [
        "👉 결과:  \n",
        "- \"Male\" → [1, 0]  \n",
        "- \"Female\" → [0, 1]  \n",
        "\n",
        "### ✅ 체크포인트\n",
        "- Normalization: 데이터 범위를 0~1 사이로 맞춤  \n",
        "- Standardization: 평균=0, 표준편차=1로 맞춤  \n",
        "- One-hot Encoding: 범주형 데이터를 숫자 벡터로 변환  \n",
        "- Feature Scaling은 경사 하강법의 효율을 높이고, One-hot Encoding은 범주형 데이터 처리를 가능하게 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2d3467",
      "metadata": {},
      "source": [
        "# 2. 정규화 (Regularization)\n",
        "\n",
        "## 2.1 왜 정규화가 필요한가?\n",
        "머신러닝 모델은 훈련 데이터에 너무 **과적합(overfitting)** 되거나,  \n",
        "너무 단순해서 **과소적합(underfitting)** 되는 경우가 많습니다.  \n",
        "\n",
        "- **과소적합(Underfitting)**: 모델이 단순 → 데이터 패턴을 잘 못 잡음  \n",
        "- **과적합(Overfitting)**: 모델이 복잡 → 훈련 데이터에는 잘 맞지만 새로운 데이터에서는 성능이 떨어짐  \n",
        "\n",
        "  <img src=\"image/overfitting.png\" width=\"500\">\n",
        "\n",
        "이미지 출처 : https://www.geeksforgeeks.org/machine-learning/underfitting-and-overfitting-in-machine-learning/\n",
        "\n",
        "👉 정규화(Regularization)는 **모델이 과적합되는 것을 막고, 일반화 성능을 높이는 방법**입니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2649aa6",
      "metadata": {},
      "source": [
        "## 2.2 정규화 개념\n",
        "정규화는 모델이 **불필요하게 큰 가중치**를 가지지 않도록 제약을 주어,  \n",
        "과적합을 방지하고 일반화 성능을 높이는 방법입니다.  \n",
        "\n",
        "- **L1 정규화 (Lasso Regression)**  \n",
        "  - 가중치의 절댓값 합을 패널티로 부여  \n",
        "  - 일부 가중치를 0으로 만들어 **특성 선택(feature selection)** 효과  \n",
        "\n",
        "- **L2 정규화 (Ridge Regression)**  \n",
        "  - 가중치의 제곱합을 패널티로 부여\n",
        "  - 모든 가중치를 조금씩 줄여 안정적인 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cca3678",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2.3 scikit-learn으로 과적합 문제 해결\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# 샘플 데이터 생성\n",
        "X = np.random.rand(100, 5) * 10\n",
        "y = 3*X[:,0] + 2*X[:,1] - X[:,2] + np.random.randn(100)*2\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 선형 회귀\n",
        "lr = LinearRegression().fit(X_train, y_train)\n",
        "print(\"LinearRegression MSE:\", mean_squared_error(y_test, lr.predict(X_test)))\n",
        "\n",
        "# Ridge 회귀 (L2)\n",
        "ridge = Ridge(alpha=1.0).fit(X_train, y_train)\n",
        "print(\"Ridge MSE:\", mean_squared_error(y_test, ridge.predict(X_test)))\n",
        "\n",
        "# Lasso 회귀 (L1)\n",
        "lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n",
        "print(\"Lasso MSE:\", mean_squared_error(y_test, lasso.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "835e2b8c",
      "metadata": {},
      "source": [
        "## 2.4 L1, L2 직접 비교\n",
        "- **L1 (Lasso)**: 일부 계수=0 → 불필요한 특성 제거 가능  \n",
        "- **L2 (Ridge)**: 모든 계수를 작게 만들어 안정적인 모델  \n",
        "- 실제로는 L1+L2 혼합한 **Elastic Net**도 많이 사용  \n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 정규화는 과적합 방지와 일반화 성능 향상에 핵심적이다.  \n",
        "- L1(Lasso): 가중치 절댓값 합 → 특성 선택 효과  \n",
        "- L2(Ridge): 가중치 제곱합 → 안정적인 모델  \n",
        "- `alpha` 값이 클수록 정규화 강도가 세지며, 너무 크면 과소적합 위험이 있다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080481f5",
      "metadata": {},
      "source": [
        "# 3. 모델 평가와 하이퍼파라미터 선택\n",
        "\n",
        "> 목적: **훈련 데이터에서만 잘 맞는 모델**을 피하고, **새로운 데이터에서도 일관되게 잘 작동(일반화)** 하도록 평가·튜닝한다.\n",
        "\n",
        "\n",
        "## 3.1 왜 모델 평가가 중요한가?\n",
        "- **훈련 성능 = 실제 성능 아님**: 훈련 데이터에 맞춘 점수는 낙관적일 수 있음(과적합).\n",
        "- **일반화 확인**: 보지 못한 데이터(검증/테스트)에서 성능을 확인해야 함.\n",
        "- **신뢰성**: 평가 절차가 재현 가능해야 하며, 데이터 누수(leakage)를 방지해야 함."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa420c1",
      "metadata": {},
      "source": [
        "## 3.2 데이터 분할 전략\n",
        "\n",
        "### (1) Hold-out 분할(예: **8:1:1 = train:valid:test**)\n",
        "- **train**: 학습\n",
        "- **valid**: 하이퍼파라미터 선택/모델 비교\n",
        "- **test**: 최종 성능 보고(딱 1번만 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8249b2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 적재\n",
        "# ------------------------------------------\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# ------------------------------------------\n",
        "# 8:1:1 분할 (계층화 분할: 각 클래스 비율 유지)\n",
        "# ------------------------------------------\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "print(\"train/valid/test:\", X_train.shape, X_valid.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3f4d51e",
      "metadata": {},
      "source": [
        "#### 변형 예시: **6:2:2**, **7:1.5:1.5** 등. 데이터가 적으면 교차 검증을 권장."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b76c831",
      "metadata": {},
      "source": [
        "### (2) 시계열(순서형) 데이터\n",
        "#### 1. 특징\n",
        "- 데이터는 시간 순서대로 기록됨 (예: 주식 가격, 날씨 데이터, 센서 데이터).\n",
        "- **순서가 중요**하므로 무작위 섞기(shuffle) 금지.  \n",
        "- 항상 **과거 → 미래** 순서를 지켜서 학습해야 함.  \n",
        "\n",
        "\n",
        "#### 2. 올바른 분할 방식 (Sliding Window)\n",
        "- 일반 데이터 분할(`train_test_split`)은 무작위 추출이 가능하지만, 시계열은 순서를 보존해야 함.  \n",
        "- **Sliding Window**: 일정 길이의 과거 데이터를 묶어서(train window) 그 직후 미래 구간을 예측(valid window).  \n",
        "  - 예: \"과거 3일 → 다음 1일 예측\"  \n",
        "- 장점: 입력 시퀀스 길이가 일정 → 딥러닝/머신러닝 모델 학습에 바로 활용 가능.\n",
        "\n",
        "#### 3. 코드 예시: Sliding Window 데이터셋 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad241b17",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 예제 시계열 데이터 (0 ~ 19)\n",
        "series = np.arange(20)\n",
        "\n",
        "def make_window_data(series, window=5, horizon=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(series) - window - horizon + 1):\n",
        "        X.append(series[i:i+window])            # 과거 구간\n",
        "        y.append(series[i+window:i+window+horizon])  # 예측 구간\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 과거 5일 데이터를 사용해 다음 1일을 예측\n",
        "X, y = make_window_data(series, window=5, horizon=1)\n",
        "\n",
        "print(\"X shape:\", X.shape)  # (샘플 수, window 크기)\n",
        "print(\"y shape:\", y.shape)  # (샘플 수, horizon 크기)\n",
        "print(\"첫 번째 샘플 X:\", X[0], \"-> y:\", y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "851fe0ef",
      "metadata": {},
      "source": [
        "## 3.3 교차 검증(Cross Validation) 종류\n",
        "- **KFold**: 무작위로 K분할 → 학습/검증 K회 반복 후 평균.\n",
        "- **StratifiedKFold**: 분류에서 **클래스 비율 유지**.\n",
        "    - **예시**: 암 환자 데이터(환자 10%, 정상인 90%) → 일반 KFold는 어떤 fold엔 환자가 아예 없을 수도 있음.  \n",
        "- **GroupKFold**: 동일 그룹은 같은 폴드에만 배치.\n",
        "    - **예시**: 남/녀 를 맞춰야 할때 동일한 사람의 사진이 2장이상 존재할때 같은 그룹(train/val/test)에 배치\n",
        "- **TimeSeriesSplit**: 시계열 전용(시간 순서 유지).\n",
        "- **RepeatedKFold/RepeatedStratifiedKFold**: KFold 교차검증을 여러 번 반복해 분산 감소."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed627848",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "# 입력 특성 행렬(X)과 정답 벡터(y) 분리\n",
        "# X : 꽃받침 길이, 꽃받침 폭, 꽃잎 길이, 꽃잎 폭 (4개의 특성)\n",
        "# y : 붓꽃 품종 (0=setosa, 1=versicolor, 2=virginica)\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 로지스틱 회귀 모델 생성\n",
        "# 분류(classification) 문제에 사용되는 선형 모델\n",
        "# max_iter=200 : 학습 반복 횟수 제한 (기본값은 100, 수렴 문제 방지를 위해 늘림)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# StratifiedKFold : 각 클래스 비율(레이블 분포)을 유지하면서 데이터셋을 여러 조각으로 나누는 K-겹 교차검증 방법\n",
        "# n_splits=5 → 데이터를 5개의 폴드(fold)로 나눔 (즉, 5번의 학습/평가 수행)\n",
        "# shuffle=True → 데이터를 무작위로 섞은 후 분할 (데이터 순서에 의한 편향 방지)\n",
        "# random_state=42 → 난수 시드를 고정하여 재현성 확보\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
        "\n",
        "# cross_val_score() : 교차 검증을 자동으로 수행해주는 함수\n",
        "# 인자 설명:\n",
        "#   model   : 사용할 학습 모델 (여기서는 로지스틱 회귀)\n",
        "#   X, y    : 입력 데이터와 정답 레이블\n",
        "#   cv      : 교차검증 분할 전략 (StratifiedKFold 객체)\n",
        "#   scoring : 성능 평가 지표. 여기서는 'accuracy' (정확도)\n",
        "#\n",
        "# 실행 과정:\n",
        "#   1. 데이터를 StratifiedKFold에 따라 5개의 폴드로 나눔\n",
        "#   2. 각 폴드에 대해 1개는 테스트, 나머지 4개는 학습용으로 사용\n",
        "#   3. 5번 반복하여 모델의 정확도를 계산\n",
        "#   4. 각 반복에서의 정확도 점수를 배열 형태로 반환\n",
        "print(\"교차 검증 점수:\", scores)\n",
        "print(\"평균 정확도:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d3db86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 각 폴드의 인덱스 확인\n",
        "for fold_idx, (train_index, test_index) in enumerate(cv.split(X, y), start=1):\n",
        "    print(f\"\\n📂 Fold {fold_idx}\")\n",
        "    print(f\"학습 데이터 인덱스 ({len(train_index)}개): {train_index[:10]} ...\")  # 앞부분만 표시\n",
        "    print(f\"테스트 데이터 인덱스 ({len(test_index)}개): {test_index[:10]} ...\")\n",
        "    print(f\"→ 테스트 데이터의 클래스 분포: {np.bincount(y[test_index])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe75dfc",
      "metadata": {},
      "source": [
        "#### **TIP**: 데이터 전처리(스케일링)는 반드시 **교차 검증 폴드 안에서** `fit`되어야 함 → `Pipeline` 사용!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3c0679",
      "metadata": {},
      "source": [
        "## 3.4 평가 지표 선택 가이드"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cef0a01b",
      "metadata": {},
      "source": [
        "### 1) 분류(Classification)\n",
        "\n",
        "#### 혼동행렬(Confusion Matrix) 표\n",
        "\n",
        "| 실제 \\ 예측 | 0 (Negative)         | 1 (Positive)         |\n",
        "|-------------|-----------------------|-----------------------|\n",
        "| **0 (Negative)** | **TN**: 진짜 음성 (정상 정답) | **FP**: 거짓 양성 (거짓 경보) |\n",
        "| **1 (Positive)** | **FN**: 거짓 음성 (놓침)     | **TP**: 진짜 양성 (정상 검출) |\n",
        "\n",
        "#### 혼동행렬(Confusion Matrix) 용어 정리\n",
        "- **TP (True Positive)**: 실제 1이고, 예측도 1  \n",
        "- **FP (False Positive)**: 실제 0인데, 예측이 1 (거짓 경보)  \n",
        "- **TN (True Negative)**: 실제 0이고, 예측도 0  \n",
        "- **FN (False Negative)**: 실제 1인데, 예측이 0 (놓침)\n",
        "\n",
        "#### **지표 공식**  \n",
        " - Accuracy = (TP + TN) / (TP + FP + TN + FN)  \n",
        " - Precision = TP / (TP + FP)  \n",
        " - Recall = TP / (TP + FN)  \n",
        " - F1 = 2 · (Precision · Recall) / (Precision + Recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d13e74",
      "metadata": {},
      "source": [
        "| 지표 | 정의/설명 | 장점 | 주의할 점 / 활용 상황 |\n",
        "|------|-----------|------|------------------------|\n",
        "| **Accuracy** | 전체 샘플 중 정답 비율 | 직관적, 해석 쉬움 | 클래스 불균형(예: 정상 99%, 이상 1%)에 매우 취약 |\n",
        "| **Precision (정밀도)** | `예측=양성` 중 실제 양성 비율 | 잘못된 경보(오탐) 줄이는 데 중요 | 양성 놓침(미탐)에는 둔감 |\n",
        "| **Recall (재현율)** | 실제 양성 중 예측=양성 비율 | 양성 놓치지 않는 게 중요할 때 유리 | 오탐(거짓 양성)이 많아질 수 있음 |\n",
        "| **F1 Score** | Precision·Recall의 조화 평균 | Precision·Recall 균형 평가 | 해석은 다소 어렵지만 불균형 데이터에 자주 사용 |\n",
        "| **ROC-AUC** | 모든 임계값에서 TPR vs FPR 곡선 아래 면적 | 임계값에 독립적, 전반적 성능 평가 | 클래스 극심 불균형일 때 과대평가될 수 있음 |\n",
        "| **PR-AUC** | Precision-Recall 곡선 아래 면적 | 양성 클래스 희소할 때 유리 | ROC-AUC보다 해석 어렵지만 불균형 심할 때 필수 |\n",
        "| **Top-k Accuracy** | 다중 클래스에서 상위 k개 예측 안에 정답 있는지 | 이미지 분류(예: Top-5 Accuracy)에서 유리 | k 설정 필요 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d448fcdd",
      "metadata": {},
      "source": [
        "### 2) 회귀(Regression)\n",
        "\n",
        "| 지표 | 정의/설명 | 장점 | 주의할 점 / 활용 상황 |\n",
        "|------|-----------|------|------------------------|\n",
        "| **MAE (Mean Absolute Error)** | 절댓값 오차 평균 | 해석 직관적, 이상치 영향 적음 | 큰 오차에 둔감 |\n",
        "| **MSE (Mean Squared Error)** | 제곱 오차 평균 | 미분/최적화에 유리 | 큰 오차에 과도한 패널티 |\n",
        "| **RMSE (Root Mean Squared Error)** | 제곱 오차의 제곱근 | 원 단위 복원, 큰 오차 강조 | MAE보다 이상치 민감 |\n",
        "| **R² (결정계수)** | 모델이 데이터 분산을 얼마나 설명하는지 (1=완벽) | 상대적 성능 비교에 유용 | 데이터 분포에 따라 음수가 될 수 있음 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea203bd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# scoring 예시: \"accuracy\", \"f1_macro\", \"roc_auc_ovr\", \"neg_mean_absolute_error\" 등"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f931a8cc",
      "metadata": {},
      "source": [
        "## 3.5 하이퍼파라미터란?\n",
        "- **파라미터**: 모델이 **데이터로부터 학습**하는 값 (예: 선형회귀의 가중치, NN의 W, b)\n",
        "- **하이퍼파라미터**: 사람이 **사전에 설정**하는 값 (예: 정규화 강도 C/alpha, 트리 깊이, 러닝레이트)\n",
        "\n",
        "| 알고리즘 | 대표 하이퍼파라미터 | 의미/영향 |\n",
        "|---|---|---|\n",
        "| LogisticRegression | C, penalty, solver | 정규화 강도, 규제 형태 |\n",
        "| SVM | C, kernel, gamma | 마진/복잡도 제어, 커널 폭 |\n",
        "| RandomForest | n_estimators, max_depth, min_samples_split | 앙상블 크기/복잡도 |\n",
        "| Ridge/Lasso | alpha | L2/L1 정규화 강도 |\n",
        "| XGBoost/LightGBM | n_estimators, learning_rate, max_depth, subsample | 부스팅 단계/학습률/복잡도 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc82f642",
      "metadata": {},
      "source": [
        "## 3.6 그리드 서치(Grid Search) + 파이프라인(Pipeline) + 누수 방지\n",
        "\n",
        "### 1) Grid Search란?\n",
        "- 모델의 **하이퍼파라미터 후보 집합**을 정의하고,  \n",
        "- 교차검증(Cross Validation)으로 모든 조합을 평가해 **최적 조합을 찾는 방법**.  \n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52920de9",
      "metadata": {},
      "source": [
        "### 2) Pipeline이 필요한 이유\n",
        "\n",
        "머신러닝 모델 학습에는 보통 이런 단계가 필요합니다:\n",
        "```\n",
        "[데이터] → [전처리기(예: 스케일러)] → [모델 학습]\n",
        "```\n",
        "\n",
        "\n",
        "#### ❌ 잘못된 방식 (누수 발생)\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)  # ⚠️ 전체 데이터로 평균/분산 학습 (X_train+X_test 모두 포함)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(model.score(X_test_scaled, y_test))\n",
        "```\n",
        "\n",
        "- 여기서는 **테스트 데이터 정보까지 평균/분산 학습에 들어감**  \n",
        "- 즉, 미래(테스트)의 정보를 미리 들여다본 셈 → 평가 점수가 실제보다 높게 나옴 (데이터 누수)\n",
        "\n",
        "\n",
        "#### ✅ 올바른 방식 (Pipeline 사용)\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # step 1: 전처리\n",
        "    ('svc', SVC())                 # step 2: 모델\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)     # 내부적으로:\n",
        "# scaler.fit(X_train) + scaler.transform(X_train)\n",
        "# svc.fit(X_train_scaled, y_train)\n",
        "\n",
        "pipe.predict(X_test)           # 내부적으로:\n",
        "# scaler.transform(X_test) (train에서 학습한 평균/분산만 사용)\n",
        "# svc.predict(X_test_scaled)\n",
        "```\n",
        "\n",
        "- `fit`을 호출하면 → train 데이터에서만 **전처리 fit**  \n",
        "- `predict`를 호출하면 → train에서 학습한 기준(평균/분산)으로 test 데이터 변환 후 예측  \n",
        "- 따라서 **test 데이터는 오직 평가용으로만 사용** → 누수 자동 방지\n",
        "\n",
        "\n",
        "### ✅ 왜 Pipeline이 중요한가?\n",
        "- 전처리와 모델을 따로 두면, 사용자가 실수로 `fit(X 전체)` 같은 코드를 짤 수 있음 → 누수 발생  \n",
        "- **Pipeline은 전처리와 모델을 한 덩어리로 묶어서**,  \n",
        "  - 학습(train 단계)에서는 train 데이터로만 전처리 fit  \n",
        "  - 검증/테스트 단계에서는 transform만 적용  \n",
        "- 즉, **사용자가 따로 구분해줄 필요 없이** 자동으로 안전하게 동작  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494be65c",
      "metadata": {},
      "source": [
        "### 3) GridSearchCV + Pipeline 사용법\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 1. Pipeline 구성 (스텝에 이름 부여)\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),   #  step 1 전처리\n",
        "    ('svc', SVC())                  #  step 2 모델\n",
        "])\n",
        "\n",
        "# 2. 탐색할 파라미터 (스텝이름__파라미터)\n",
        "param_grid = {\n",
        "    'svc__C': [0.1, 1, 10], # 'svc' 스텝의 C 파라미터\n",
        "    'svc__kernel': ['linear', 'rbf'], # 'svc' 스텝의 kernel 파라미터\n",
        "    'svc__gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# 3. GridSearchCV 실행\n",
        "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy') # cv=5 : 5-fold 교차검증\n",
        "grid.fit(*load_iris(return_X_y=True))\n",
        "\n",
        "print(\"최적 파라미터:\", grid.best_params_)\n",
        "print(\"최적 성능:\", grid.best_score_)\n",
        "```\n",
        "\n",
        "\n",
        "### 4) 핵심 정리\n",
        "- **Pipeline**: 전처리 + 모델을 한 덩어리로 묶음.  \n",
        "- **GridSearchCV**: 하이퍼파라미터를 전수조사해 최적값 선택.  \n",
        "- **누수 방지**: 전처리는 fold별 train에서만 `fit`, valid/test는 `transform`만 적용.  \n",
        "- **param_grid 키**: `스텝이름__파라미터` 형식으로 지정해야 함.  \n",
        "\n",
        "\n",
        "### ✅ 요약 그림\n",
        "\n",
        "```\n",
        "[Raw Data] → [Scaler (fit on train only)] → [Model] → [Predict]\n",
        "                ↑\n",
        "                └─ Pipeline이 자동으로 누수 방지 보장\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee120190",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ------------------------------------------\n",
        "# 파이프라인: [스케일링 -> 로지스틱회귀]\n",
        "# ------------------------------------------\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# 하이퍼파라미터 그리드\n",
        "#   - 파이프라인 스텝 이름 'clf__' 접두사로 접근\n",
        "# ------------------------------------------\n",
        "param_grid = {\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10, 100],\n",
        "    \"clf__penalty\": [\"l2\"],            # liblinear/lbfgs 기준\n",
        "    \"clf__solver\": [\"liblinear\", \"lbfgs\"]\n",
        "}\n",
        "\n",
        "# ------------------------------------------\n",
        "# 교차 검증 설정(계층화 K-겹)\n",
        "# ------------------------------------------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ------------------------------------------\n",
        "# GridSearchCV: scoring 변경 가능(불균형시 f1_macro 등 권장)\n",
        "# ------------------------------------------\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"최적 하이퍼파라미터:\", grid.best_params_)\n",
        "print(\"검증 평균 점수(교차검증):\", grid.best_score_)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 선택된 최적 모델로 검증/테스트 평가\n",
        "# ------------------------------------------\n",
        "best_model = grid.best_estimator_\n",
        "valid_pred = best_model.predict(X_valid)\n",
        "test_pred  = best_model.predict(X_test)\n",
        "\n",
        "print(\"VALID accuracy:\", accuracy_score(y_valid, valid_pred))\n",
        "print(\"TEST  accuracy:\", accuracy_score(y_test,  test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f3c56a",
      "metadata": {},
      "source": [
        "> **왜 Pipeline인가?**  \n",
        "> 스케일러를 train+valid 전체에 미리 `fit`해버리면 **검증 정보가 훈련에 새는 데이터 누수** 발생.  \n",
        "> Pipeline은 각 폴드마다 **훈련 파트에만 fit → 검증 파트에 transform**을 자동 적용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb79fac",
      "metadata": {},
      "source": [
        "## 3.7 중첩 교차 검증(Nested CV): 과적합된 튜닝을 방지한 **공정한** 성능 추정\n",
        "- **바깥(outer) CV**: 일반화 성능 추정\n",
        "- **안쪽(inner) CV**: 하이퍼파라미터 선택(GridSearch)\n",
        "- 장점: **튜닝 편향**을 줄인 “공정한” 최종 점수 획득"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5867a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "param_grid = {\"clf__C\": [0.01, 0.1, 1, 10, 100]}\n",
        "inner_search = GridSearchCV(pipe, param_grid, cv=inner_cv, scoring=\"f1_macro\", n_jobs=-1)\n",
        "\n",
        "# 바깥쪽에서 inner_search 자체를 평가(각 폴드마다 안쪽에서 튜닝 수행)\n",
        "nested_scores = cross_val_score(inner_search, X, y, cv=outer_cv, scoring=\"f1_macro\", n_jobs=-1)\n",
        "\n",
        "print(\"Nested CV f1_macro:\", nested_scores)\n",
        "print(\"Nested CV 평균:\", nested_scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6368cee2",
      "metadata": {},
      "source": [
        "## 3.8 추가 팁 & 흔한 함정\n",
        "- **Seed 고정**: `random_state` 고정 → 재현 가능성↑\n",
        "- **클래스 불균형**: `stratify`, `class_weight='balanced'`, 적절 지표(F1, PR-AUC) 사용\n",
        "- **베이스라인**: 간단 모델(로지스틱/리니어)로 기준 점수 만든 뒤 복잡도 증가\n",
        "- **RandomizedSearchCV**: 큰 공간은 **랜덤 탐색**이 효율적\n",
        "- **Bayesian Optimization**: 고성능 탐색 도구(Optuna, skopt 등)도 고려\n",
        "- **특성 스케일링**: 거리/정규화 기반 모델(SVM, KNN, 로지스틱 등)엔 필수\n",
        "- **데이터 누수**: 통합 전처리(스케일링/인코딩/선택)는 **반드시 Pipeline 안에서**!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e42142f",
      "metadata": {},
      "source": [
        "## 3.9 요약 (Cheat Sheet)\n",
        "- **분할**: 8:1:1(또는 교차 검증), 시계열/그룹은 전용 CV 사용\n",
        "- **지표**: 문제 특성에 맞게 선택(불균형은 F1·PR-AUC)\n",
        "- **튜닝**: Pipeline + (Grid/Randomized)Search, 검증 데이터로 비교\n",
        "- **검증 강화**: Nested CV로 공정한 최종 성능 추정\n",
        "- **누수 금지**: 전처리는 항상 폴드 내부에서 `fit`!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "034d694f",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- [ ] 8:1:1 분할 또는 교차 검증으로 **일반화 성능**을 확인했는가?  \n",
        "- [ ] 전처리·모델을 **Pipeline**으로 묶어 **누수**를 방지했는가?  \n",
        "- [ ] 문제 특성에 맞는 **평가지표**를 선택했는가?  \n",
        "- [ ] 합리적인 **하이퍼파라미터 공간**을 정의했는가?  \n",
        "- [ ] (선택) **Nested CV**로 튜닝 편향을 줄였는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73e0f3c",
      "metadata": {},
      "source": [
        "## 머신러닝 더 빠르고 정확하게 – 실습 문제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d977f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "titanic = pd.read_csv(\"data/titanic.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333b0249",
      "metadata": {},
      "source": [
        "### 문제 1. Feature Scaling\n",
        "Titanic 데이터셋에서 `Age` 컬럼을 불러와서 **정규화(Normalization)** 와 **표준화(Standardization)** 를 적용해보세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# 데이터 불러오기\n",
        "titanic = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "age = titanic[[\"Age\"]].dropna()# 결측치 제거\n",
        "\n",
        "# 정규화\n",
        "minmax = MinMaxScaler().fit_transform(age)\n",
        "print(\"정규화 결과:\\n\", minmax[:5])\n",
        "\n",
        "# 표준화\n",
        "standard = StandardScaler().fit_transform(age)\n",
        "print(\"표준화 결과:\\n\", standard[:5])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd0617f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e312907f",
      "metadata": {},
      "source": [
        "### 문제 2. One-hot Encoding\n",
        "Titanic 데이터셋의 `Sex` 컬럼을 One-hot Encoding하여 새로운 DataFrame을 만들어보세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "encoded = pd.get_dummies(titanic[[\"Sex\"]], prefix=\"Sex\")\n",
        "print(encoded.head())\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f372c3a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65918235",
      "metadata": {},
      "source": [
        "### 문제 3. L1 / L2 정규화\n",
        "Titanic 데이터셋에서 `Pclass`, `Age`, `Fare`를 입력 변수로, `Survived`를 타깃 변수로 하여  \n",
        "Lasso(L1)와 Ridge(L2) 회귀를 각각 적용해보고 성능(결정계수)을 비교하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# 간단한 전처리\n",
        "df = titanic[[\"Survived\", \"Pclass\",  \"Survived\", \"Age\", \"Fare\"]].dropna()\n",
        "X = df[[\"Pclass\", \"Survived\", \"Fare\"]]\n",
        "y = df[\"Age\"]\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ridge (L2)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(\"Ridge score:\", ridge.score(X_test, y_test)) # .score : R² (결정계수) \n",
        "\n",
        "# R² = 1 → 완벽하게 예측\n",
        "# R² = 0 → 평균값으로 예측하는 것과 동일한 수준\n",
        "# R² < 0 → 모델이 평균으로 예측하는 것보다 오히려 못함\n",
        "\n",
        "# Lasso (L1)\n",
        "lasso = Lasso(alpha=0.01)\n",
        "lasso.fit(X_train, y_train)\n",
        "print(\"Lasso score:\", lasso.score(X_test, y_test))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e520f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943b8cee",
      "metadata": {},
      "source": [
        "### 문제 4. k겹 교차 검증\n",
        "Titanic 데이터셋에서 `Pclass`, `Sex`, `Age`를 사용해 로지스틱 회귀를 학습하고, 5겹 교차 검증으로 평균 정확도를 구하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# 간단한 전처리\n",
        "df = titanic[[\"Survived\", \"Pclass\", \"Sex\", \"Age\"]].dropna()\n",
        "df = pd.get_dummies(df, columns=[\"Sex\"])\n",
        "X = df.drop(\"Survived\", axis=1)\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "\n",
        "print(\"교차 검증 점수:\", scores)\n",
        "print(\"평균 정확도:\", scores.mean())\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82bab533",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a58a48",
      "metadata": {},
      "source": [
        "### 문제 5. 그리드 서치\n",
        "Titanic 데이터셋에서 로지스틱 회귀 모델을 사용하고, `C` 값 후보 [0.01, 0.1, 1, 10]에 대해 GridSearchCV로 최적 하이퍼파라미터를 찾아보세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\"C\": [0.01, 0.1, 1, 10]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"최적 파라미터:\", grid.best_params_)\n",
        "print(\"최적 성능:\", grid.best_score_)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebf6c83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce670c1c",
      "metadata": {},
      "source": [
        "### 문제 6. 데이터 분할 (Train/Validation/Test)\n",
        "Titanic 데이터셋을 **8:1:1 비율**로 학습, 검증, 테스트 세트로 나누어보세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = titanic[[\"Survived\", \"Pclass\", \"Age\", \"Fare\"]].dropna()\n",
        "X = df.drop(\"Survived\", axis=1)\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "# 8:2 분할 → 0.2 중 절반은 검증, 절반은 테스트\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(X_train.shape, X_valid.shape, X_test.shape)\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9081c2",
      "metadata": {},
      "source": [
        "\n",
        "### 문제 7. 표준화 + 로지스틱 회귀\n",
        "Titanic 데이터셋에서 `Age`, `Fare`를 입력 변수로 사용하고, **표준화(StandardScaler)**를 적용한 후 로지스틱 회귀를 학습하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "df = titanic[[\"Survived\", \"Age\", \"Fare\"]].dropna()\n",
        "X = df[[\"Age\", \"Fare\"]]\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "print(\"모델 정확도:\", model.score(X_scaled, y))\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d488df",
      "metadata": {},
      "source": [
        "### 문제 8. ROC-AUC 평가\n",
        "Titanic 데이터셋에서 `Pclass`, `Sex`, `Age`를 사용해 로지스틱 회귀를 학습하고, **ROC-AUC 점수**를 계산하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "df = titanic[[\"Survived\", \"Pclass\", \"Sex\", \"Age\"]].dropna()\n",
        "df = pd.get_dummies(df, columns=[\"Sex\"])\n",
        "X = df.drop(\"Survived\", axis=1)\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X, y)\n",
        "probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "print(\"ROC-AUC:\", roc_auc_score(y, probs))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2415924f",
      "metadata": {},
      "source": [
        "### 문제 9. 교차 검증 지표 변경\n",
        "Titanic 데이터셋에서 로지스틱 회귀 모델을 학습하고, **교차 검증 시 Accuracy가 아닌 F1-score**를 사용해 평균 성능을 구하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=5, scoring=\"f1\")\n",
        "print(\"평균 F1-score:\", scores.mean())\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5870092",
      "metadata": {},
      "source": [
        "### 문제 10. 파이프라인(Pipeline) 활용\n",
        "Titanic 데이터셋에서 `Age`, `Fare`를 입력 변수로 사용하여,  \n",
        "**[표준화 → 로지스틱 회귀]** 단계를 Pipeline으로 구성하고 정확도를 계산하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "pipe.fit(X, y)\n",
        "print(\"정확도:\", pipe.score(X, y))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8399a7a",
      "metadata": {},
      "source": [
        "### 문제 11. Randomized Search\n",
        "Titanic 데이터셋에서 로지스틱 회귀의 `C` 값을 [0.001, 0.01, 0.1, 1, 10, 100] 중에서,  \n",
        "**RandomizedSearchCV**를 사용해 최적 하이퍼파라미터를 찾아보세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "search = RandomizedSearchCV(LogisticRegression(max_iter=200), param_dist, cv=5, n_iter=3, random_state=42)\n",
        "search.fit(X, y)\n",
        "\n",
        "print(\"최적 파라미터:\", search.best_params_)\n",
        "print(\"최적 점수:\", search.best_score_)\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26fc612",
      "metadata": {},
      "source": [
        "### 문제 12. 특성 중요도 확인 (Lasso)\n",
        "Titanic 데이터셋에서 `Pclass`, `Age`, `Fare`를 입력 변수로 하고,  \n",
        "Lasso(L1) 회귀를 학습한 후 **계수(coefficient)** 값을 출력해보세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "lasso = Lasso(alpha=0.01)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "print(\"계수:\", lasso.coef_)\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "933e4471",
      "metadata": {},
      "source": [
        "### 문제 13. Confusion Matrix\n",
        "Titanic 데이터셋에서 로지스틱 회귀 모델을 학습하고,  \n",
        "**혼동 행렬(confusion matrix)**을 출력해보세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(confusion_matrix(y, y_pred))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e47836a0",
      "metadata": {},
      "source": [
        "### 문제 14. Classification Report\n",
        "Titanic 데이터셋에서 로지스틱 회귀 모델을 학습하고,  \n",
        "**Precision, Recall, F1-score**를 포함한 Classification Report를 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python \n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y, y_pred))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9be32f7",
      "metadata": {},
      "source": [
        "### 문제 15. Nested Cross Validation\n",
        "Titanic 데이터셋에서 로지스틱 회귀 모델을 사용하고,  \n",
        "안쪽(inner) 교차 검증으로 하이퍼파라미터를 튜닝한 뒤,  \n",
        "바깥쪽(outer) 교차 검증으로 최종 성능을 평가하는 **Nested CV**를 구현하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
        "\n",
        "param_grid = {\"C\": [0.01, 0.1, 1, 10]}\n",
        "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=inner_cv)\n",
        "nested_scores = cross_val_score(grid, X, y, cv=outer_cv)\n",
        "\n",
        "print(\"Nested CV 점수:\", nested_scores)\n",
        "print(\"평균 점수:\", nested_scores.mean())\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c172adfd",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 전처리(스케일링, 원-핫 인코딩)는 모델 학습에 필수적이다.  \n",
        "- 정규화(L1, L2)는 과적합을 방지하고 일반화 성능을 높인다.  \n",
        "- k겹 교차 검증은 모델 안정성을 평가하는 좋은 방법이다.  \n",
        "- Grid Search는 최적 하이퍼파라미터를 자동으로 찾아준다.  "
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "MS0xMF/rqLjsi6Drn6zri51f642UX+u5oOultOqzoF/soJXtmZXtlZjqsowuaXB5bmI=",
    "inserted_date": [
      "2025-09-19"
    ],
    "kernelspec": {
      "display_name": "py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
