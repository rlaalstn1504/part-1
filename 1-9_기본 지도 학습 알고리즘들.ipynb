{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "851b6401",
      "metadata": {},
      "source": [
        "#  기본 지도 학습 알고리즘들\n",
        "\n",
        "### 기획 배경\n",
        "- **기본 지도 학습 알고리즘들**은 머신러닝의 핵심인 **지도 학습(Supervised Learning)** 을 배우는 단계입니다.  \n",
        "- 머신러닝 기본기를 다진 후, 실제 데이터에 적용할 수 있는 **회귀와 분류 알고리즘**을 익혀야 합니다.  \n",
        "- 단순하고 직관적인 알고리즘을 먼저 배우면, 이후 복잡한 모델(딥러닝 포함)에도 같은 원리를 적용할 수 있습니다.  \n",
        "\n",
        "\n",
        "### 학습 목표\n",
        "이 토픽을 수강한 뒤, 수강생은 다음을 할 수 있어야 합니다:\n",
        "\n",
        "- 지도 학습의 대표 알고리즘인 **선형 회귀, 다중 선형 회귀, 로지스틱 회귀**의 원리를 이해한다.  \n",
        "- 손실 함수와 경사 하강법을 통해 모델을 학습하는 과정을 설명할 수 있다.  \n",
        "- `scikit-learn` 라이브러리를 활용해 실제 데이터를 학습/예측/평가할 수 있다.  \n",
        "- 모델 성능을 평가하고 개선할 수 있다.  \n",
        "\n",
        "\n",
        "## 목차\n",
        "\n",
        "### 0. 들어가기\n",
        "- 지도 학습(Supervised Learning)의 개념 복습  \n",
        "- 회귀(Regression)와 분류(Classification)의 차이"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e6256f6",
      "metadata": {},
      "source": [
        "### 1. 선형 회귀 (Linear Regression)\n",
        "- 개념: 입력(x)과 출력(y)의 **직선적 관계**를 모델링  \n",
        "- 수학적 표현:  \n",
        "  $y = w x + b$\n",
        "- 손실 함수: 평균제곱오차(MSE)  \n",
        "- 경사 하강법 개념 & 학습률의 역할  \n",
        "- Numpy로 직접 구현하기  \n",
        "- `scikit-learn` 예제: 단순 선형 회귀\n",
        "\n",
        "\n",
        "<img src=\"image/linear_regression.png\" width=\"500\">  \n",
        "\n",
        "이미지 출처 : [https://www.lennysnewsletter.com/p/linear-regression-and-correlation-analysis](https://www.lennysnewsletter.com/p/linear-regression-and-correlation-analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29d523c",
      "metadata": {},
      "source": [
        "### 2. 다중 선형 회귀 (Multiple Linear Regression)\n",
        "- 개념: 입력 변수가 여러 개인 경우  \n",
        "- 수학적 표현:  \n",
        "  $\n",
        "  y = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b\n",
        "  $\n",
        "- 경사 하강법 적용  \n",
        "- 정규방정식(Closed-form solution)  \n",
        "- 경사 하강법 vs 정규방정식 비교  \n",
        "- `scikit-learn` 예제: 다중 선형 회귀  \n",
        "\n",
        "<img src=\"image/Simple_multiple_comparison.jpg\" width=\"500\">  \n",
        "\n",
        "이미지 출처 : [https://www.shiksha.com/online-courses/articles/linear-and-multiple-regression/](https://www.shiksha.com/online-courses/articles/linear-and-multiple-regression/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a225da2",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 3. 로지스틱 회귀 (Logistic Regression)\n",
        "- 개념: **분류(Classification)** 문제에 사용  \n",
        "- 시그모이드 함수:  \n",
        "  $\n",
        "  \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
        "  $\n",
        "- 결정 경계와 확률적 해석  \n",
        "- 손실 함수: 로지스틱 손실(Log loss)  \n",
        "- Numpy로 로지스틱 회귀 구현  \n",
        "- `scikit-learn` 예제: 이진 분류, 다중 분류  \n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 지도 학습 = 입력과 정답(Label)으로 학습하는 방식  \n",
        "- 회귀(연속값 예측) vs 분류(범주 예측) 구분 가능  \n",
        "- 선형 회귀 → 다중 선형 회귀 → 로지스틱 회귀는 지도 학습의 기본기  \n",
        "- 손실 함수와 경사 하강법은 모든 머신러닝 모델 학습의 핵심 개념  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03eb2050",
      "metadata": {},
      "source": [
        "## 0. 들어가기\n",
        "\n",
        "지도 학습(Supervised Learning)은 **입력(x)** 과 **정답(y)** 을 함께 제공하여,  \n",
        "모델이 x→y 규칙을 학습하게 만드는 방식입니다.  \n",
        "\n",
        "- **회귀(Regression)**: 연속적인 숫자를 예측 (예: 집값, 키, 몸무게)  \n",
        "- **분류(Classification)**: 범주를 예측 (예: 스팸/햄, 합격/불합격)  \n",
        "\n",
        "\n",
        "<img src=\"image/supervised.png\" width=\"400\">\n",
        "\n",
        "> 비유: **회귀는 주관식(연속 값 범위에서 답)**, **분류는 객관식(라벨 중 하나 선택)**. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14dbac35",
      "metadata": {},
      "source": [
        "### 0.1 예측 모델의 기본 구조 및 학습 흐름\n",
        "\n",
        "지도학습 모델은 입력 x를 받아 함수 f(x; θ)를 통해 예측값 ŷ를 생성합니다.  \n",
        "이때 θ(세타)는 모델의 파라미터(가중치, 절편 등)로, 학습의 대상입니다.\n",
        "\n",
        "$\n",
        "\\hat{y} = f(x; \\theta)\n",
        "$\n",
        "\n",
        "모델은 학습 과정에서 **‘예측값과 실제값의 차이’를 줄이도록 θ를 조정**합니다.  \n",
        "이 차이를 정량화한 것이 바로 **손실함수(Loss Function)** 입니다.\n",
        "\n",
        "#### 학습 흐름\n",
        "\n",
        "1️⃣ **데이터 입력**: (x, y) 쌍이 모델에 주어짐  \n",
        "2️⃣ **예측 수행**: 모델이 현재 파라미터로 ŷ = f(x; θ) 계산  \n",
        "3️⃣ **손실 계산**: 예측값 ŷ와 실제값 y의 차이를 수치로 표현  \n",
        "4️⃣ **파라미터 조정**: 경사하강법으로 손실이 줄어드는 방향으로 θ 업데이트  \n",
        "\n",
        "> 즉, 지도학습의 핵심은  \n",
        "> **“얼마나 잘 예측했는가(손실)”를 정의하고,  \n",
        "> 그 손실을 줄이기 위해 파라미터를 업데이트하는 과정**입니다.\n",
        "\n",
        "#### 회귀와 분류의 출력 차이\n",
        "\n",
        "| 구분 | 출력 형태 | 예시 | 대표 모델 |\n",
        "|------|-------------|--------|------------|\n",
        "| **회귀** | 실수(연속 값) | 키, 온도, 매출액 | Linear Regression, SVR |\n",
        "| **분류** | 범주(이산 값) | 고양이/개, 합격/불합격 | Logistic Regression, SVM, CNN |\n",
        "\n",
        "회귀는 \"수치를 맞추는 문제\",  \n",
        "분류는 \"라벨을 구분하는 문제\"이므로  \n",
        "손실함수의 형태도 서로 달라집니다.\n",
        "\n",
        "> 두 과업은 **본질이 다르므로 손실함수와 평가지표가 다릅니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c58092",
      "metadata": {},
      "source": [
        "### 0.2 회귀 vs 분류: 왜 손실 함수·평가 지표가 다른가?\n",
        "- **회귀**는 예측값과 실제값의 **수치적 거리**를 줄이는 문제가 핵심 → “오차의 크기”를 직접 최소화하는 손실이 필요.  \n",
        "- **분류**는 **확률적 의사결정**(정답 라벨에 높은 확률을 부여)과 **의사결정 임계값**이 핵심 → 확률분포와 분리도를 반영하는 손실·지표가 필요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a37650",
      "metadata": {},
      "source": [
        "### 0.3 손실함수(회귀)\n",
        "- **MSE(Mean Squared Error)**: $(\\frac{1}{N}\\sum (y-\\hat y)^2$)  \n",
        "  - 큰 오차에 가중(제곱) → 이상치에 민감, 평균적으로 매끈하게 맞춤.\n",
        "- **MAE(Mean Absolute Error)**: $(\\frac{1}{N}\\sum |y-\\hat y|$)  \n",
        "  - 이상치에 덜 민감, 중앙값 적합에 가까움.\n",
        "- **Huber/Smooth L1**: 작은 오차는 제곱, 큰 오차는 절대값 → **MSE·MAE 절충**(이상치 완화).\n",
        "- **Quantile Loss**: 특정 분위수(예: P90) 예측 → **수요예측, 리스크 상한/하한 추정**에 유용.\n",
        "\n",
        "    <img src=\"image/linear regression-error.png\" width=\"500\">  \n",
        "\n",
        "    이미지 출처 : [https://community.cloudera.com/t5/Community-Articles/Understanding-Linear-Regression/ta-p/281391](https://community.cloudera.com/t5/Community-Articles/Understanding-Linear-Regression/ta-p/281391)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359c5d98",
      "metadata": {},
      "source": [
        "### 0.4 손실함수(분류)\n",
        "- **Binary Cross-Entropy(Log Loss)**: 이진 분류  \n",
        "  - 정답 라벨에 확률을 최대화(잘못 확신하면 큰 패널티).\n",
        "- **Categorical Cross-Entropy**: 다중 분류(softmax)  \n",
        "  - 정답 클래스 확률을 키우고, 오답 확률을 낮춤.\n",
        "\n",
        "    <img src=\"image/softmax.webp\" width=\"500\">  \n",
        "\n",
        "    이미지 출처 : [https://www.singlestore.com/blog/a-guide-to-softmax-activation-function/](https://www.singlestore.com/blog/a-guide-to-softmax-activation-function/)\n",
        "    softmax.webp\n",
        "\n",
        "- **Focal Loss**: 불균형 데이터에서 어려운 샘플에 더 큰 가중 → **소수 클래스 학습 강화**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f6f8cd",
      "metadata": {},
      "source": [
        "### 0.4 평가지표(회귀)\n",
        "\n",
        "모델이 학습을 마친 뒤에는 **얼마나 정확히 예측했는가**를 평가해야 합니다.  \n",
        "하지만 회귀와 분류는 **출력값의 형태가 다르기 때문에**,  평가 기준 역시 서로 다른 관점으로 정의됩니다.\n",
        "\n",
        "- **회귀**는 **연속적인 값**을 예측하므로, “얼마나 가까운가(오차의 크기)”가 핵심입니다.  \n",
        "\n",
        "- **RMSE**: $\\sqrt{\\text{MSE}}$, 단위가 원래 타깃과 같아 해석 용이.  \n",
        "- **MAE**: 평균 절대 오차, 이상치 강건성.  \n",
        "- **MAPE**: 평균 상대 오차(%) — **0 근처 타깃, 음수, 단위 문제에 주의**.  \n",
        "- **\\(R^2\\)**: 설명력(0~1 이상), 기준모델 대비 성능 — **불균형 범위·비선형일 때 오해 소지**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b92458",
      "metadata": {},
      "source": [
        "### 0.5 평가지표(분류)\n",
        "\n",
        "**분류**는 **라벨(범주)** 를 맞히는 것이므로, “얼마나 잘 구분했는가(정답 비율·확률 분리도)”가 핵심입니다.  \n",
        "불균형 데이터에서는 단순 정확도보다 **정밀도·재현율·AUC** 등의 세부 지표가 중요합니다.\n",
        "\n",
        "- **Accuracy**: 전체 정답 비율 — **클래스 불균형에 취약**.  \n",
        "\n",
        "- **Precision / Recall / F1 Score**:  \n",
        "  - Precision: 예측한 양성 중 진짜 양성 비율  \n",
        "  - Recall: 실제 양성 중 잡아낸 비율  \n",
        "  - F1: 정밀·재현 조화 평균(임계값 민감)  \n",
        "  - **마이크로/매크로 평균** 선택에 유의(다중 클래스/불균형).\n",
        "- **ROC-AUC**: 임계값 전 범위에서 분리도(특이도 vs 민감도).  \n",
        "- **PR-AUC**: 양성 희소 시 **더 정보량 있는 지표**.  \n",
        "- **Log Loss**: 확률 보정(calibration)까지 평가."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb3eed1d",
      "metadata": {},
      "source": [
        "#### 0.7-1 손실함수 요약 표\n",
        "\n",
        "회귀 알고리즘 손실 함수 \n",
        "대표 손실함수 | 핵심 특징 | 비고/주의 |\n",
        "|---|---|---|\n",
        "|**MSE** | 오차 제곱 평균. 큰 오차에 더 큰 패널티 → 매끈한 적합 | 이상치에 민감 |\n",
        "|**MAE** | 절대오차 평균. 이상치에 강건 | 미분 불연속(0 근처) – 구현은 가능 |\n",
        "|**Huber / Smooth L1** | 작은 오차는 제곱, 큰 오차는 절대값 → MSE·MAE 절충 | 이상치 완화 + 안정적 학습 |\n",
        "|**Quantile Loss** | 분위수(예: P90) 예측 | 수요/리스크 상하한 예측에 유용 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48ddebad",
      "metadata": {},
      "source": [
        "분류 알고리즘 손실 함수\n",
        "\n",
        "| 과업 | 대표 손실함수 | 핵심 특징 | 비고/주의 |\n",
        "|---|---|---|---|\n",
        "| **분류(이진)** | **Binary Cross-Entropy (Log Loss)** | 정답 라벨 확률 최대화(잘못된 확신에 큰 패널티) | 시그모이드/로지스틱과 함께 사용 |\n",
        "| **분류(다중)** | **Categorical Cross-Entropy** | 정답 클래스 확률 최대화 | 소프트맥스와 함께 사용 |\n",
        "| **불균형/난샘플 가중** | **Focal Loss** | 어려운 샘플에 가중, 쉬운 샘플 페널티 감소 | 클래스 불균형 완화 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d958fe",
      "metadata": {},
      "source": [
        "#### 0.7-2 평가지표 요약 표\n",
        "\n",
        "회귀 알고리즘 평가지표\n",
        "\n",
        "대표 평가지표 | 정의(요약) | 비고/주의 |\n",
        "|---|---|---|\n",
        "| **RMSE** | MSE의 제곱근(타깃 단위와 동일) | 큰 오차에 민감 |\n",
        "| **MAE** | 절대오차 평균 | 이상치 강건 |\n",
        "| **MAPE** | 상대오차 평균(%) | 0/음수 취급 주의, 스케일 영향 적음 |\n",
        "| **R²** | 설명력(기준모델 대비 개선 비율) | 비선형/분산 큰 데이터에서 오해 소지 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4536d006",
      "metadata": {},
      "source": [
        "#### 혼동행렬(Confusion Matrix) 용어 정리\n",
        "- **TP (True Positive)**: 실제 1이고, 예측도 1  \n",
        "- **FP (False Positive)**: 실제 0인데, 예측이 1 (거짓 경보)  \n",
        "- **TN (True Negative)**: 실제 0이고, 예측도 0  \n",
        "- **FN (False Negative)**: 실제 1인데, 예측이 0 (놓침)\n",
        "\n",
        "#### 혼동행렬(Confusion Matrix) 표\n",
        "\n",
        "| 실제 \\ 예측 | 0 (Negative)         | 1 (Positive)         |\n",
        "|-------------|-----------------------|-----------------------|\n",
        "| **0 (Negative)** | **TN**: 진짜 음성 (정상 정답) | **FP**: 거짓 양성 (거짓 경보) |\n",
        "| **1 (Positive)** | **FN**: 거짓 음성 (놓침)     | **TP**: 진짜 양성 (정상 검출) |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25fc8686",
      "metadata": {},
      "source": [
        "| 과업 | 대표 평가지표 | 정의(요약) | 비고/주의 |\n",
        "|---|---|---|---|\n",
        "| **분류** | **Accuracy** | (TP+TN)/전체 | 불균형 시 왜곡 위험 큼 |\n",
        "|  | **Precision** | TP/(TP+FP) | 양성 예측의 “정확함” |\n",
        "|  | **Recall** | TP/(TP+FN) | 실제 양성 포착률(민감도) |\n",
        "|  | **F1-score** | 2·(P·R)/(P+R) | Precision–Recall 균형 지표 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8491cf9",
      "metadata": {},
      "source": [
        "#### 왜 Accuracy만 보면 위험한가? (왜곡 사례)\n",
        "\n",
        "- **클래스 불균형**: 양성이 1%인 데이터에서 전부 0으로 예측하면 **정확도 99%** 지만,  \n",
        "  - Recall = 0 (양성 전부 놓침), Precision 정의 불가(양성 예측 없음) → **실전 성능 형편없음**\n",
        "- **임계값(Threshold) 민감성**: 같은 모델도 임계값 선택에 따라 Accuracy는 비슷해 보여도 **Precision/Recall 트레이드오프**가 크게 달라짐\n",
        "- **비대칭 비용**: FP와 FN의 비용이 다르면 Accuracy만 높여도 **운영 비용**이 커질 수 있음  \n",
        "  (예: 불량 검출에서 FN은 치명적 손실)\n",
        "\n",
        "> 실전 권장: **Accuracy + (Precision/Recall/F1)** 를 함께 보고, 업무 비용/목표에 맞춰 **임계값을 최적화**하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e368d4",
      "metadata": {},
      "source": [
        "### 연습 문제) 암 진단 모델 평가\n",
        "\n",
        "한 병원에서 **암(양성, Positive)** / **정상(음성, Negative)** 예측 모델을 만들었습니다.  \n",
        "테스트 데이터 100명에 대한 실제값과 모델 예측 결과는 아래와 같습니다.\n",
        "\n",
        "|              | 실제 암 (Positive) | 실제 정상 (Negative) |\n",
        "|--------------|--------------------|-----------------------|\n",
        "| **예측 암**      | 5                  | 5                     |\n",
        "| **예측 정상**    | 5                  | 85                    |\n",
        "\n",
        "\n",
        "위 표는 **혼동행렬(Confusion Matrix)** 입니다.  \n",
        "- TP (참양성) = 5  \n",
        "- FP (거짓양성) = 5  \n",
        "- FN (거짓음성) = 5  \n",
        "- TN (참음성) = 85  \n",
        "\n",
        "\n",
        "#### 문제\n",
        "1. Accuracy, Precision, Recall, F1 Score를 각각 계산하시오.  \n",
        "2. 왜 Accuracy만 보면 모델 성능이 좋아 보일 수 있는지 설명하시오.  \n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>정답</summary>\n",
        "\n",
        "```python\n",
        "TP, FP, FN, TN = 5, 5, 5, 85\n",
        "\n",
        "# 1. Accuracy\n",
        "accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "# (5 + 85) / 100 = 0.90\n",
        "\n",
        "# 2. Precision (양성 예측의 정확도)\n",
        "precision = TP / (TP + FP)\n",
        "# 5 / (5 + 5) = 0.50\n",
        "\n",
        "# 3. Recall (양성 중 놓치지 않은 비율)\n",
        "recall = TP / (TP + FN)\n",
        "# 5 / (5 + 5) = 0.50\n",
        "\n",
        "# 4. F1 Score\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "# 2 * (0.5 * 0.5) / (0.5 + 0.5) = 0.50\n",
        "\n",
        "print(f\"Accuracy={accuracy:.2f}, Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}\")\n",
        "```\n",
        "\n",
        "출력:\n",
        "```\n",
        "Accuracy=0.90, Precision=0.50, Recall=0.50, F1=0.50\n",
        "```\n",
        "\n",
        "#### 해설\n",
        "- Accuracy만 보면 **90%**로 성능이 좋아 보임.  \n",
        "- 하지만 실제 암 환자(양성)는 절반이나 놓쳤음(Recall=0.5).  \n",
        "- Precision도 0.5 → 암이라고 예측한 절반은 정상인 사람.  \n",
        "- **결론**: 클래스 불균형(암=10명, 정상=90명) 상황에서 Accuracy는 과대평가를 일으킴.  \n",
        "  → 이런 경우 **Precision, Recall, F1** 지표로 성능을 반드시 함께 평가해야 함.  \n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b47760",
      "metadata": {},
      "source": [
        "## 1. 선형 회귀 (Linear Regression)\n",
        "\n",
        "<img src=\"image/linear_regression.png\" width=\"500\">  \n",
        "\n",
        "이미지 출처 : [https://www.lennysnewsletter.com/p/linear-regression-and-correlation-analysis](https://www.lennysnewsletter.com/p/linear-regression-and-correlation-analysis)\n",
        "\n",
        "### 1.1 개념\n",
        "- 입력 변수와 출력 변수 사이의 관계를 **직선(Linear)** 으로 모델링  \n",
        "- 단순 선형 회귀:  \n",
        "  $$\n",
        "  y = w x + b\n",
        "  $$  \n",
        "  - w = 기울기(가중치, weight)  \n",
        "  - b = 절편(bias)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df1ef1ca",
      "metadata": {},
      "source": [
        "### 1.2 손실 함수 (Loss Function)\n",
        "모델의 성능을 평가하기 위해 **예측값과 실제값의 차이**를 계산해야 함.  \n",
        "대표적으로 **평균제곱오차(MSE, Mean Squared Error)** 사용.  \n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
        "$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765b4635",
      "metadata": {},
      "source": [
        "### 1.3 경사 하강법 (Gradient Descent)\n",
        "- 손실 함수를 최소화하기 위한 방법  \n",
        "- 파라미터 w, b를 조금씩 조정하여 MSE를 줄여나감  \n",
        "- **학습률(learning rate, η)**: 얼마나 크게 이동할지 조절  \n",
        "\n",
        "업데이트 식:  \n",
        "$$\n",
        "w := w - \\eta \\frac{\\partial L}{\\partial w}, \\quad b := b - \\eta \\frac{\\partial L}{\\partial b}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb81d5af",
      "metadata": {},
      "source": [
        "### 1.4 Numpy로 직접 구현하기\n",
        "```python\n",
        "import numpy as np\n",
        "# 선형 회귀(Linear Regression)를 경사하강법(Gradient Descent)으로 학습하는 최소 예제\n",
        "# y ≈ w*x + b 형태의 1차 모델을 가정하고, MSE(평균제곱오차)를 최소화하는 w, b를 찾습니다.\n",
        "# - X: 입력(특징) 벡터, shape: (N,)\n",
        "# - y: 정답(타깃) 벡터, shape: (N,)\n",
        "# - w, b: 학습할 파라미터(스칼라)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# 데이터 (공부 시간 -> 점수)\n",
        "# 예시로 완벽한 선형관계 y = 2x 를 사용합니다.\n",
        "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
        "y = np.array([2, 4, 6, 8, 10], dtype=float)  # y = 2x 관계\n",
        "\n",
        "# 파라미터 초기화\n",
        "# w: 기울기(스칼라), b: 절편(스칼라)\n",
        "w = 0.0\n",
        "b = 0.0\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "# lr(learning rate): 한 번의 업데이트에서 이동하는 크기\n",
        "# epochs: 전체 데이터셋을 몇 번 반복하여 학습할지\n",
        "lr = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# 샘플 수 (편미분에서 평균을 내기 위해 사용)\n",
        "N = len(X)\n",
        "\n",
        "# 경사 하강법 루프\n",
        "for _ in range(epochs):\n",
        "    # 1) 순전파(Forward): 현재 파라미터로 예측값 계산\n",
        "    # y_pred_i = w*X_i + b\n",
        "    y_pred = w * X + b  # shape: (N,)\n",
        "\n",
        "    # 2) 오차(Error) 계산: 예측 - 정답\n",
        "    # error_i = y_pred_i - y_i\n",
        "    error = y_pred - y  # shape: (N,)\n",
        "\n",
        "    # (참고) 손실 함수를 MSE로 두면:\n",
        "    # L = (1/N) * Σ (y_pred_i - y_i)^2\n",
        "    # 여기서는 매 스텝마다 L을 굳이 출력하진 않지만, 모니터링하려면 아래 주석을 해제하세요.\n",
        "    # L = (1.0 / N) * np.sum(error ** 2)\n",
        "\n",
        "    # 3) 역전파(Gradient) 계산\n",
        "    # MSE에 대한 w, b의 편미분(벡터/행렬 미분 결과)을 사용합니다.\n",
        "    # dL/dw = (2/N) * Σ (y_pred_i - y_i) * X_i  = (2/N) * (error · X)\n",
        "    # dL/db = (2/N) * Σ (y_pred_i - y_i)        = (2/N) * sum(error)\n",
        "    dw = (2.0 / N) * np.dot(error, X)   # 스칼라\n",
        "    db = (2.0 / N) * np.sum(error)      # 스칼라\n",
        "\n",
        "    # 4) 파라미터 업데이트(경사 하강)\n",
        "    # w := w - lr * dL/dw\n",
        "    # b := b - lr * dL/db\n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "\n",
        "    # (선택) 학습 과정을 보고 싶다면 주기적으로 손실을 출력하세요.\n",
        "    # if _ % 100 == 0:\n",
        "    #     print(f\"epoch={_:4d}  L={L:.6f}  w={w:.4f}  b={b:.4f}\")\n",
        "\n",
        "# 학습된 파라미터와 최종 예측 출력\n",
        "print(\"학습된 w:\", w)         # 이론적 정답은 2.0에 수렴\n",
        "print(\"학습된 b:\", b)         # 이론적 정답은 0.0에 수렴\n",
        "print(\"예측 y:\", w * X + b)   # 각 X에 대한 최종 예측값\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0aa2bd9",
      "metadata": {},
      "source": [
        "### 1.5 scikit-learn으로 선형 회귀 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cb2d48",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 입력을 2D로 변환\n",
        "X = X.reshape(-1, 1)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"회귀 계수 w:\", model.coef_)\n",
        "print(\"절편 b:\", model.intercept_)\n",
        "print(\"예측:\", model.predict([[6]]))  # 공부시간 6시간 → 점수 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6695e173",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 선형 회귀는 입력과 출력의 관계를 직선으로 모델링한다.  \n",
        "- 손실 함수(MSE)를 최소화하는 방향으로 학습한다.  \n",
        "- 경사 하강법은 w, b를 조정하며 오차를 줄이는 과정이다.  \n",
        "- `scikit-learn`을 사용하면 쉽게 선형 회귀를 적용할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0b3cbc",
      "metadata": {},
      "source": [
        "## 2. 다중 선형 회귀 (Multiple Linear Regression)\n",
        "\n",
        "### 2.1 개념\n",
        "- 단순 선형 회귀는 입력이 하나일 때 \\( y = wx + b \\)  \n",
        "- 다중 선형 회귀는 입력이 여러 개일 때 사용  \n",
        "\n",
        "$$\n",
        "y = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b\n",
        "$$\n",
        "\n",
        "예: 집값 예측  \n",
        "- $( x_1 $): 방 개수  \n",
        "- $( x_2 $): 평수  \n",
        "- $( x_3 $): 위치 지수  \n",
        "\n",
        "\n",
        "\n",
        "### 2.2 손실 함수\n",
        "- 여전히 평균제곱오차(MSE)를 사용  \n",
        "- 여러 개의 w(가중치)와 b(절편)를 동시에 학습  \n",
        "\n",
        "\n",
        "\n",
        "### 2.3 경사 하강법\n",
        "- 파라미터 벡터 $( \\vec{w} $)를 학습  \n",
        "- 업데이트 식:  \n",
        "\n",
        "$$\n",
        "\\vec{w} := \\vec{w} - \\eta \\nabla_w L, \\quad b := b - \\eta \\frac{\\partial L}{\\partial b}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205d9095",
      "metadata": {
        "vscode": {
          "languageId": "ini"
        }
      },
      "source": [
        "### 2.4 Numpy로 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d6a498",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 데이터 (방 개수, 평수) → 집값\n",
        "X = np.array([[1, 30],\n",
        "              [2, 50],\n",
        "              [3, 70],\n",
        "              [4, 100]])   # 입력 특성 2개\n",
        "y = np.array([200, 400, 600, 1000])   # 출력\n",
        "\n",
        "# 파라미터 초기화\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0.0\n",
        "lr = 0.0001\n",
        "epochs = 1000\n",
        "\n",
        "# 경사 하강법\n",
        "for _ in range(epochs):\n",
        "    y_pred = np.dot(X, w) + b\n",
        "    error = y_pred - y\n",
        "    \n",
        "    dw = (2/len(X)) * np.dot(X.T, error)\n",
        "    db = (2/len(X)) * np.sum(error)\n",
        "    \n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "\n",
        "print(\"학습된 w:\", w)\n",
        "print(\"학습된 b:\", b)\n",
        "print(\"예측:\", np.dot(X, w) + b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48221a66",
      "metadata": {},
      "source": [
        "### 2.5 scikit-learn으로 다중 회귀 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f0f0f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"회귀 계수 w:\", model.coef_)\n",
        "print(\"절편 b:\", model.intercept_)\n",
        "print(\"예측:\", model.predict([[3, 80]]))  # 방 3개, 80평 → 집값 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8d51fd",
      "metadata": {
        "vscode": {
          "languageId": "ini"
        }
      },
      "source": [
        "### 2.6 경사 하강법 vs 정규방정식\n",
        "- **경사 하강법**  \n",
        "  - 반복적으로 최적해를 근사  \n",
        "  - 대용량 데이터셋에 적합 (빅데이터)  \n",
        "- **정규방정식**  \n",
        "  - 한 번의 계산으로 해 구함  \n",
        "  - 데이터 크기가 작을 때 효율적  \n",
        "  - 하지만 $( (X^TX)^{-1} $) 계산에 비용이 큼 (O(n³))  \n",
        "\n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 다중 선형 회귀는 여러 입력 변수를 동시에 고려할 수 있다.  \n",
        "- 손실 함수는 MSE, 학습은 경사 하강법 또는 정규방정식으로 가능하다.  \n",
        "- `scikit-learn`을 사용하면 매우 간단하게 구현할 수 있다.  \n",
        "- 경사 하강법은 대규모 데이터에, 정규방정식은 소규모 데이터에 적합하다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06cfe4a4",
      "metadata": {},
      "source": [
        "## 3. 로지스틱 회귀 (Logistic Regression)\n",
        "\n",
        "### 3.1 개념\n",
        "- **선형 회귀**는 연속값 예측 → 회귀 문제에 적합  \n",
        "- **로지스틱 회귀**는 범주형 값 예측 → 분류 문제에 적합  \n",
        "- 예: 스팸메일(스팸/정상), 시험 결과(합격/불합격)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f2de6e",
      "metadata": {},
      "source": [
        "### 3.2 시그모이드 함수 (Sigmoid Function)\n",
        "- 로지스틱 회귀의 핵심: 입력값을 **0~1 사이 확률**로 변환  \n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "- z = \\( w x + b \\)  \n",
        "\n",
        "\n",
        "<img src=\"image/sigmoid.jpg\" alt=\"sigmoid\" width=\"400\">\n",
        "\n",
        "이미지 출처 : https://en.wikipedia.org/wiki/Sigmoid_function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1dbd3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Sigmoid Function\")\n",
        "plt.xlabel(\"z\")\n",
        "plt.ylabel(\"σ(z)\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40fa0a01",
      "metadata": {},
      "source": [
        "### 3.3 결정 경계 (Decision Boundary)\n",
        "- 시그모이드 함수 결과 ≥ 0.5 → 1 (True)  \n",
        "- 시그모이드 함수 결과 < 0.5 → 0 (False)  \n",
        "\n",
        "즉, 확률을 기준으로 분류하는 것."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7829a75",
      "metadata": {},
      "source": [
        "### 3.4 손실 함수 (Log Loss)\n",
        "- 회귀에서는 MSE 사용  \n",
        "- 분류에서는 **Log Loss (Cross-Entropy Loss)** 사용  \n",
        "\n",
        "$$\n",
        "L = -\\frac{1}{n} \\sum_{i=1}^n \\Big[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\Big]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d16b6a33",
      "metadata": {},
      "source": [
        "### 3.5 Numpy로 구현하기 (이진 분류)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "683a74d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터: 공부 시간(x) → 합격 여부(y: 0 or 1)\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([0, 0, 0, 1, 1])  \n",
        "\n",
        "# 파라미터 초기화\n",
        "w = 0.0\n",
        "b = 0.0\n",
        "lr = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# 경사 하강법\n",
        "for _ in range(epochs):\n",
        "    z = w*X + b\n",
        "    y_pred = sigmoid(z)\n",
        "    \n",
        "    # 오차\n",
        "    error = y_pred - y\n",
        "    \n",
        "    # 파라미터 업데이트\n",
        "    dw = np.dot(error, X) / len(X)\n",
        "    db = np.sum(error) / len(X)\n",
        "    \n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "\n",
        "print(\"학습된 w:\", w)\n",
        "print(\"학습된 b:\", b)\n",
        "\n",
        "# 예측\n",
        "test = np.array([2.5, 3.5, 5])\n",
        "pred = sigmoid(w*test + b)\n",
        "print(\"예측 확률:\", pred)\n",
        "print(\"분류 결과:\", (pred >= 0.5).astype(int))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eda7945",
      "metadata": {},
      "source": [
        "### 3.6 scikit-learn으로 로지스틱 회귀 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7caf422f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = X.reshape(-1, 1)  # 입력을 2D로 변환\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"예측 확률:\", model.predict_proba([[3], [4], [5]]))\n",
        "print(\"분류 결과:\", model.predict([[3], [4], [5]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c00c52",
      "metadata": {},
      "source": [
        "### 3.7 다중 분류 (Multiclass Classification)\n",
        "- 로지스틱 회귀는 기본적으로 이진 분류에 사용  \n",
        "- **OvR(One vs Rest)** 방식으로 다중 분류 확장 가능  \n",
        "  - 예: 숫자(0~9) 손글씨 분류  \n",
        "\n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 로지스틱 회귀는 분류 문제에서 사용되는 지도학습 알고리즘이다.  \n",
        "- 시그모이드 함수를 사용해 확률(0~1)로 해석할 수 있다.  \n",
        "- 손실 함수는 Log Loss (교차 엔트로피)를 사용한다.  \n",
        "- `scikit-learn`으로 손쉽게 이진/다중 분류 문제를 해결할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bec50a",
      "metadata": {},
      "source": [
        "### 4. Scikit-learn 모델 사용법 요약\n",
        "\n",
        "#### 1) 분류(Classification) 예제 — 로지스틱 회귀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "612556c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 분류(Classification) 기본 흐름:\n",
        "# 1) 더미 데이터 생성 (make_classification)\n",
        "# 2) 학습/검증 데이터 분리 (train_test_split)\n",
        "# 3) 모델 생성 및 학습 (LogisticRegression.fit)\n",
        "# 4) 예측 (predict, predict_proba)\n",
        "# 5) 성능 평가 (정확도, F1, ROC-AUC, 혼동행렬)\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cf9061dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Classification] Logistic Regression\n",
            "Accuracy  : 0.8550\n",
            "Precision : 0.8913\n",
            "Recall    : 0.8119\n",
            "F1        : 0.8497\n",
            "ROC-AUC   : 0.9248\n",
            "Confusion Matrix:\n",
            " [[89 10]\n",
            " [19 82]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86        99\n",
            "           1       0.89      0.81      0.85       101\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.86      0.86      0.85       200\n",
            "weighted avg       0.86      0.85      0.85       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) 더미 데이터 생성\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, # - n_samples: 샘플 수\n",
        "    n_features=10, # - n_features: 특징 수\n",
        "    n_informative=5, # - n_informative: 실제로 유용한 특징 수\n",
        "    n_redundant=2, # - n_redundant: 중복 특징 수\n",
        "    random_state=42 # - random_state: 재현성\n",
        ")\n",
        "\n",
        "# 2) 학습/검증 데이터 분리\n",
        "# - stratify=y: 분류에서는 클래스 비율을 유지하도록 권장\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2,\n",
        "    random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3) 모델 생성 및 학습\n",
        "# - max_iter: 수렴 보장을 위해 여유 있게 설정\n",
        "# - n_jobs: 가능한 경우 병렬 처리 (LogisticRegression 일부 solver에서만 사용)\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4) 예측\n",
        "y_pred = clf.predict(X_test)                    # 라벨 예측(0/1)\n",
        "y_prob = clf.predict_proba(X_test)[:, 1]        # 양성(1) 클래스 확률\n",
        "\n",
        "# 5) 성능 평가\n",
        "acc  = accuracy_score(y_test, y_pred)           # 전체 정확도\n",
        "prec = precision_score(y_test, y_pred)          # 정밀도(양성 예측의 정확성)\n",
        "rec  = recall_score(y_test, y_pred)             # 재현율(양성 포착률)\n",
        "f1   = f1_score(y_test, y_pred)                 # F1(정밀/재현 조화평균)\n",
        "auc  = roc_auc_score(y_test, y_prob)            # ROC-AUC(임계값 독립 분리도)\n",
        "\n",
        "cm   = confusion_matrix(y_test, y_pred)         # 혼동행렬\n",
        "rep  = classification_report(y_test, y_pred)    # 클래스별 정밀/재현/F1 상세\n",
        "\n",
        "print(\"[Classification] Logistic Regression\")\n",
        "print(f\"Accuracy  : {acc:.4f}\")\n",
        "print(f\"Precision : {prec:.4f}\")\n",
        "print(f\"Recall    : {rec:.4f}\")\n",
        "print(f\"F1        : {f1:.4f}\")\n",
        "print(f\"ROC-AUC   : {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Classification Report:\\n\", rep)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a638bb27",
      "metadata": {},
      "source": [
        "### 기본 지도 학습 알고리즘들 – 실습 문제\n",
        "\n",
        "### 문제 1. 단순 선형 회귀 (Numpy 구현)\n",
        "X = [1, 2, 3, 4, 5], y = [2, 4, 6, 8, 10] 데이터를 이용해  \n",
        "경사 하강법으로 선형 회귀를 학습하고, w와 b를 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "w, b = 0.0, 0.0\n",
        "lr = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "for _ in range(epochs):\n",
        "    y_pred = w*X + b\n",
        "    error = y_pred - y\n",
        "    \n",
        "    dw = (2/len(X)) * np.dot(error, X)\n",
        "    db = (2/len(X)) * np.sum(error)\n",
        "    \n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "\n",
        "print(\"w:\", w, \"b:\", b)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08aa7183",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 작성하세요"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0567f348",
      "metadata": {},
      "source": [
        "### 문제 3. 선형 회귀 (scikit-learn)\n",
        "사이킷런의 `LinearRegression`을 사용해, 공부 시간 X=[1,2,3,4,5]과 점수 y=[2,4,6,8,10] 데이터를 학습하고, 6시간 공부했을 때 점수를 예측하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"회귀 계수:\", model.coef_)\n",
        "print(\"절편:\", model.intercept_)\n",
        "print(\"6시간 예측:\", model.predict([[6]]))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0e00ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 작성하세요\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 6, 8, 10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e14aea7",
      "metadata": {},
      "source": [
        "### 문제 4. 로지스틱 회귀 (Numpy 구현)\n",
        "X = [1, 2, 3, 4, 5], y = [0, 0, 0, 1, 1] 데이터에서  \n",
        "로지스틱 회귀를 경사 하강법으로 학습한 후, X=3, 4, 5에 대한 확률과 분류 결과를 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([0, 0, 0, 1, 1])\n",
        "\n",
        "w, b = 0.0, 0.0\n",
        "lr = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "for _ in range(epochs):\n",
        "    z = w*X + b\n",
        "    y_pred = sigmoid(z)\n",
        "    error = y_pred - y\n",
        "    \n",
        "    dw = np.dot(error, X) / len(X)\n",
        "    db = np.sum(error) / len(X)\n",
        "    \n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "\n",
        "test = np.array([3, 4, 5])\n",
        "probs = sigmoid(w*test + b)\n",
        "preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"확률:\", probs)\n",
        "print(\"분류:\", preds)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87f7bfb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 작성하세요"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe783fe",
      "metadata": {},
      "source": [
        "\n",
        "### 문제 5. 로지스틱 회귀 (scikit-learn)\n",
        "사이킷런의 `LogisticRegression`을 사용해,  \n",
        "X=[1,2,3,4,5], y=[0,0,0,1,1] 데이터를 학습하고,  \n",
        "X=3, 4, 5의 예측 확률과 분류 결과를 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([0, 0, 0, 1, 1])\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"예측 확률:\", model.predict_proba([[3], [4], [5]]))\n",
        "print(\"분류 결과:\", model.predict([[3], [4], [5]]))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d6d21e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 작성하세요"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e36b07",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 선형 회귀는 연속적인 값을 예측, 로지스틱 회귀는 분류 문제에 사용된다.  \n",
        "- 경사 하강법은 손실 함수를 줄이면서 파라미터를 학습하는 기본 알고리즘이다.  \n",
        "- 정규방정식은 소규모 데이터에서 해를 빠르게 구할 수 있다.  \n",
        "- `scikit-learn`을 사용하면 복잡한 수식을 직접 구현하지 않아도 쉽게 모델을 적용할 수 있다.  \n"
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "MS05X+q4sOuzuCDsp4Drj4Qg7ZWZ7Iq1IOyVjOqzoOumrOymmOuTpC5pcHluYg==",
    "inserted_date": [
      "2025-09-19"
    ],
    "kernelspec": {
      "display_name": "py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
