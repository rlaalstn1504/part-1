{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7effb130",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 아래 명령어를 통해 필요한 라이브러리를 미리 설치해주세요\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748783fe",
      "metadata": {},
      "source": [
        "# 결정 트리와 앙상블 기법\n",
        "\n",
        "**결정 트리와 앙상블 기법**은 머신러닝에서 가장 널리 쓰이는 알고리즘 중 하나입니다.  \n",
        "- **결정 트리(Decision Tree)**: 구조가 직관적이고 시각화가 쉬워 해석 가능성이 높음. 그러나 단독으로는 과적합 위험이 크고 성능이 제한적일 수 있음.  \n",
        "- **앙상블 기법(Ensemble Methods)**: 여러 개의 모델을 결합하여 단일 모델보다 더 좋은 성능을 발휘. 특히 배깅(Bagging)과 부스팅(Boosting)이 대표적.  \n",
        "\n",
        "👉 이 토픽에서는 **결정 트리 기본기**부터 **랜덤 포레스트, 부스팅(XGBoost 포함)** 까지 학습하여, 복잡한 데이터셋에서도 효과적인 모델을 구축할 수 있도록 합니다.  \n",
        "\n",
        "학습 목표\n",
        "\n",
        "- 결정 트리의 기본 개념과 작동 방식을 이해하고 설명할 수 있다.  \n",
        "- 앙상블 기법의 원리를 이해하고 실제 데이터에 적용할 수 있다.  \n",
        "- 결정 트리 및 앙상블 모델을 학습·평가하고, 성능을 비교할 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d64a00",
      "metadata": {},
      "source": [
        "## 0. 들어가기\n",
        "\n",
        "머신러닝 모델 중에는 복잡하지만 해석하기 어려운 모델(예: 신경망)과  \n",
        "단순하지만 이해하기 쉬운 모델(예: 선형 회귀)이 있습니다.  \n",
        "\n",
        "👉 **결정 트리(Decision Tree)** 는 그 중간에 위치합니다.  \n",
        "- 데이터의 규칙을 \"나무(tree)\" 구조로 표현  \n",
        "- 사람이 이해하기 쉽고, 시각화 가능  \n",
        "- 그러나 깊게 만들면 과적합이 쉽게 발생  \n",
        "\n",
        "  <img src=\"image/Decision Tree.webp\" width=\"500\">\n",
        "\n",
        "이미지 출처 : https://www.displayr.com/what-is-a-decision-tree/\n",
        "\n",
        "이번 장에서는 **결정 트리의 기본 개념**부터 실습까지 다룹니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ecb3fb",
      "metadata": {},
      "source": [
        "## 1. 결정 트리 (Decision Tree)\n",
        "\n",
        "### 1.1 기본 개념\n",
        "- 데이터를 분할하여 예측하는 알고리즘  \n",
        "- 트리 구조:  \n",
        "  - **루트 노드(root)**: 시작점  \n",
        "  - **분기 노드(split node)**: 질문을 던지는 부분 (예: 나이 > 30?)  \n",
        "  - **리프 노드(leaf node)**: 최종 예측 결과 (예: 생존 / 사망)  \n",
        "\n",
        "  <img src=\"image/tree_arch.webp\">  \n",
        "  \n",
        "  이미지 출처 : https://medium.com/@ryassminh/math-for-ml-understand-regression-decision-trees-with-simple-examples-9474fceb6802"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430d22b3",
      "metadata": {},
      "source": [
        "### 1.2 작동 방식\n",
        "1. 데이터를 가장 잘 나누는 기준(feature, 조건)을 선택  \n",
        "2. 해당 조건으로 분할  \n",
        "3. 각 분할된 그룹에 대해 다시 같은 과정을 반복  \n",
        "4. 더 이상 나눌 수 없을 때 예측 결과 결정  \n",
        "\n",
        "👉 즉, \"질문을 반복하면서 최종 답을 찾는 과정\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b065571b",
      "metadata": {},
      "source": [
        "### 1.3 분할 기준\n",
        "\n",
        "결정 트리(Decision Tree)는 데이터를 나눌 때 **불순도(impurity)** 를 줄이는 방향으로 작동합니다.  \n",
        "즉, 노드를 분할했을 때 데이터가 더 “순수(pure)”해지도록 기준을 정하는 것입니다.  \n",
        "\n",
        "\n",
        "####  불순도(Impurity)란?\n",
        "- **불순도 = 섞여 있는 정도**\n",
        "- 한 노드에 클래스가 여러 개 뒤섞여 있으면 **불순도↑**\n",
        "- 한 노드에 모두 같은 클래스만 있으면 **불순도=0 (완전 순수)**\n",
        "\n",
        "  $$\n",
        "  G(p) = 1 - \\sum_{i=1}^k p_i^2\n",
        "  $$\n",
        "  \n",
        "#### 예시: 남/여 구분\n",
        "\n",
        "어떤 데이터셋이 있고, 트리가 사람의 성별(남자/여자)을 맞추는 문제라고 해봅시다.\n",
        "\n",
        "####  루트 노드 (처음 상태)\n",
        "- 데이터: 총 10명  \n",
        "  - 남자(Male) = 5명  \n",
        "  - 여자(Female) = 5명  \n",
        "\n",
        "- 지니 불순도 계산:\n",
        "  $$\n",
        "  G = 1 - (p_{male}^2 + p_{female}^2) \n",
        "  = 1 - (0.5^2 + 0.5^2) \n",
        "  = 1 - (0.25+0.25) = 0.5\n",
        "  $$\n",
        "  → 완전히 섞여 있음 (불순도 최대)\n",
        "\n",
        "\n",
        "####  분할 1: “머리 길이” 기준으로 나누기\n",
        "- **긴 머리 그룹**: 여자 4명, 남자 1명  \n",
        "  $$\n",
        "  G = 1 - (0.8^2 + 0.2^2) \n",
        "  = 1 - (0.64+0.04) = 0.32\n",
        "  $$\n",
        "\n",
        "- **짧은 머리 그룹**: 남자 4명, 여자 1명  \n",
        "  $$\n",
        "  G = 1 - (0.8^2 + 0.2^2) = 0.32\n",
        "  $$\n",
        "\n",
        "👉 분할 후 평균 지니 불순도:\n",
        "$$\n",
        "G_{avg} = \\frac{5}{10}\\times 0.32 + \\frac{5}{10}\\times 0.32 = 0.32\n",
        "$$\n",
        "\n",
        "→ 루트에서 0.5였던 불순도가 0.32로 **줄어듦** → 좋은 분할!\n",
        "\n",
        "\n",
        "####  분할 2: “키” 기준으로 나누기\n",
        "- **키 ≥ 170cm 그룹**: 남자 3명, 여자 2명  \n",
        "  $$\n",
        "  G = 1 - (0.6^2 + 0.4^2) = 1 - (0.36+0.16) = 0.48\n",
        "  $$\n",
        "- **키 < 170cm 그룹**: 남자 2명, 여자 3명  \n",
        "  $$\n",
        "  G = 1 - (0.4^2 + 0.6^2) = 0.48\n",
        "  $$\n",
        "\n",
        "👉 분할 후 평균 지니 불순도:\n",
        "$$\n",
        "G_{avg} = 0.48\n",
        "$$\n",
        "\n",
        "→ 루트에서 0.5였던 불순도가 0.48로 **거의 줄지 않음** → 나쁜 분할."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0900006b",
      "metadata": {},
      "source": [
        "#### 🔹 자동으로 분할 기준 선택\n",
        "- 사람이 `170cm일까, 180cm일까?`라고 임의로 정하는 게 아니라,  \n",
        "- **트리 알고리즘이 모든 feature와 가능한 분할 기준(threshold)** 을 시도해봅니다.  \n",
        "  - 연속형 변수(키, 나이, 수입): 가능한 분할점 후보를 전부 계산 → 불순도 감소량이 최대인 기준 선택  \n",
        "  - 범주형 변수(성별, 지역): 가능한 조합을 모두 시도 → 불순도 감소량이 최대인 조합 선택  \n",
        "- 결과적으로 “불순도를 가장 크게 줄여주는 기준”이 자동으로 선택됩니다.\n",
        "\n",
        "\n",
        "### ✅ 정리\n",
        "- 트리는 **여러 후보 특성(feature)** 중에서  \n",
        "  **불순도를 가장 크게 줄이는 기준**을 스스로 선택해 데이터를 나눕니다.\n",
        "- 예제에서는 “머리 길이”가 “키”보다 훨씬 좋은 분할 기준.\n",
        "\n",
        "\n",
        "### 💡 직관적 비유\n",
        "- 반 친구들을 남/여로 정확히 나누고 싶다.  \n",
        "- **키 기준**으로 나누면 섞임이 여전히 많음 → 불순도 높음.  \n",
        "- **머리 길이 기준**으로 나누면 각 그룹이 한쪽 성별로 치우침 → 불순도 낮음.  \n",
        "- 따라서 트리는 “머리 길이”를 자동으로 최적 기준으로 선택.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44278b8c",
      "metadata": {},
      "source": [
        "### 1) 엔트로피(Entropy)\n",
        "- 데이터의 **무질서 정도**를 측정\n",
        "- 공식:\n",
        "  $$\n",
        "  H(p) = - \\sum p \\log_2 p\n",
        "  $$\n",
        "  여기서 \\(p\\)는 특정 클래스에 속할 확률\n",
        "\n",
        "- 👉 값 해석:\n",
        "  - 클래스가 섞여 있을수록 엔트로피 ↑ (최대 = 1)\n",
        "  - 한쪽 클래스만 있으면 엔트로피 = 0\n",
        "\n",
        "| 예시 (두 클래스 비율) | 엔트로피 값 |\n",
        "|-------------------|-----------|\n",
        "| (0, 1) → 한쪽만 존재 | 0 |\n",
        "| (0.5, 0.5) → 반반   | 1 (최대 무질서) |\n",
        "| (0.8, 0.2)         | 0.72 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe4e6e1",
      "metadata": {},
      "source": [
        "### 2) 지니 불순도(Gini Impurity)\n",
        "- 무작위로 두 샘플을 뽑았을 때 **서로 다른 클래스일 확률**\n",
        "- 공식:\n",
        "  $$\n",
        "  G(p) = 1 - \\sum_{i=1}^k p_i^2\n",
        "  $$\n",
        "\n",
        "- $(k$): 클래스의 개수  \n",
        "- $(p_i$): 현재 노드에서 클래스 $(i$)에 속할 확률 (샘플 비율)\n",
        "\n",
        "### 🔹 유도 과정 (직관)\n",
        "1. 어떤 노드에 클래스 A, B가 있다고 가정\n",
        "   - 클래스 A 비율 = $(p_A$)  \n",
        "   - 클래스 B 비율 = $(p_B$)  \n",
        "   - (조건: $(p_A + p_B = 1$))\n",
        "\n",
        "2. 무작위로 두 샘플을 뽑았을 때 **같은 클래스일 확률**은?  \n",
        "   - 둘 다 A일 확률: $(p_A \\times p_A = p_A^2$)  \n",
        "   - 둘 다 B일 확률: $(p_B \\times p_B = p_B^2$)  \n",
        "   - 따라서 \"같은 클래스일 확률\" = $(p_A^2 + p_B^2$)\n",
        "\n",
        "3. \"서로 다른 클래스일 확률\" = $(1 - (p_A^2 + p_B^2)$)  \n",
        "   → 이것이 바로 **지니 불순도** 공식!\n",
        "\n",
        "\n",
        "### 🔹 예시 (2클래스)\n",
        "- (0.5, 0.5):  \n",
        "  $(G = 1 - (0.5^2 + 0.5^2) = 1 - (0.25+0.25) = 0.5$)  \n",
        "  → 가장 섞여 있음 (불순도 최대)\n",
        "\n",
        "- (0.8, 0.2):  \n",
        "  $(G = 1 - (0.64 + 0.04) = 0.32$)  \n",
        "  → 다소 순수해짐\n",
        "\n",
        "- (1.0, 0.0):  \n",
        "  $(G = 1 - (1^2 + 0^2) = 0$)  \n",
        "  → 완전히 순수\n",
        "\n",
        "\n",
        "### 🔹 특징\n",
        "- 범위: 0 ~ 0.5 (2클래스일 때)  \n",
        "- 클래스가 많아질수록 최대값은 1에 가까워짐 (더 섞일 수 있으므로)  \n",
        "- **작을수록 순수** → 결정 트리는 Gini가 줄어드는 방향으로 분할을 선택\n",
        "\n",
        "- 👉 값 해석:\n",
        "  - 클래스가 섞일수록 지니 ↑ (최대 = 0.5 for 2클래스)\n",
        "  - 한쪽 클래스만 있으면 지니 = 0\n",
        "\n",
        "| 예시 (두 클래스 비율) | 지니 값 |\n",
        "|-------------------|--------|\n",
        "| (0, 1)            | 0 |\n",
        "| (0.5, 0.5)        | 0.5 (최대) |\n",
        "| (0.8, 0.2)        | 0.32 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf04b0f",
      "metadata": {},
      "source": [
        "### 엔트로피 vs 지니 비교\n",
        "- 둘 다 **데이터 분할의 불순도**를 측정\n",
        "- **공통점**: 한쪽으로 치우칠수록 값 ↓, 섞일수록 값 ↑\n",
        "- **차이점**:\n",
        "  - 엔트로피: 로그 계산 → 이론적 의미(정보량) 강조\n",
        "  - 지니: 계산 단순, 조금 더 빠름\n",
        "- 실제 성능 차이는 크지 않음 (둘 다 많이 사용)\n",
        "\n",
        "\n",
        "### 직관적 비유\n",
        "- 반에서 학생들이 \"사과팀\"과 \"바나나팀\"으로 나뉜다고 하자.\n",
        "- **한 반에 모두 사과팀만 있다** → 완전히 순수, 불순도=0\n",
        "- **사과팀/바나나팀이 반반** → 가장 섞여 있음, 불순도 최대\n",
        "- 결정 트리는 이런 불순도를 줄이는 쪽으로 반을 나누어감."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74526063",
      "metadata": {},
      "source": [
        "### 1.4 Scikit-learn으로 결정 트리 분류기 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467223ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 불러오기\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 결정 트리 모델 학습\n",
        "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 트리 시각화\n",
        "plt.figure(figsize=(12, 6))\n",
        "plot_tree(model, feature_names=data.feature_names, class_names=data.target_names, filled=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a1c75ad",
      "metadata": {},
      "source": [
        "### 1.5 가지치기 (Pruning)\n",
        "- 트리가 너무 깊어지면 훈련 데이터에 과적합됨  \n",
        "- 해결 방법:  \n",
        "  - `max_depth`: 트리의 최대 깊이 제한  \n",
        "  - `min_samples_split`: 분할하기 위한 최소 샘플 수  \n",
        "  - `min_samples_leaf`: 리프 노드에 필요한 최소 샘플 수  \n",
        "  \n",
        "  <img src=\"image/Prune.png\">  \n",
        "\n",
        "  이미지 출처 : https://developers.google.com/machine-learning/decision-forests/overfitting-and-pruning?hl=ko\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 결정 트리는 데이터를 조건으로 분할하는 알고리즘이다.  \n",
        "- 분할 기준으로는 엔트로피와 지니 불순도가 사용된다.  \n",
        "- 트리가 너무 깊으면 과적합되므로 가지치기로 제어해야 한다.  \n",
        "- Scikit-learn을 사용하면 결정 트리 모델을 손쉽게 구현할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f18b185",
      "metadata": {},
      "source": [
        "## 2. 앙상블 기법 (Ensemble Methods)\n",
        "\n",
        "### 2.1 앙상블 기법이란?\n",
        "- 단일 모델 하나보다, 여러 개의 모델을 결합하면 더 좋은 성능을 낼 수 있습니다.  \n",
        "- 이를 **앙상블(ensemble)** 기법이라고 합니다.  \n",
        "- 아이디어: \"여러 사람이 투표하면 더 정확한 답을 얻을 수 있다.\"  \n",
        "\n",
        "\n",
        "### 2.2 배깅 (Bagging: Bootstrap Aggregating)\n",
        "- 데이터 샘플을 여러 번 복원추출(bootstrap) 각각의 모델을 학습시킴  \n",
        "- 모든 모델의 예측을 `평균(회귀)` 또는 `다수결(분류)`로 결합  \n",
        "- 분산(Variance)을 줄이고, 과적합 위험을 완화  \n",
        "\n",
        "👉 대표적인 배깅 기법: **랜덤 포레스트(Random Forest)**  \n",
        "\n",
        "\n",
        "### 2.3 랜덤 포레스트 (Random Forest)\n",
        "- 여러 개의 **결정 트리(Decision Tree)** 를 학습시킨 뒤,  \n",
        "  예측을 모아 최종 결과를 결정하는 알고리즘  \n",
        "- 각 트리는 데이터와 특성을 랜덤하게 선택 → 다양성을 확보  \n",
        "- 장점:  \n",
        "  - 단일 결정 트리보다 과적합 위험이 낮음  \n",
        "  - 속성 중요도(Feature Importance)를 제공"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c86553",
      "metadata": {},
      "source": [
        "### 2.4 Scikit-learn으로 랜덤 포레스트 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9393ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터 불러오기\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 학습/테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 랜덤 포레스트 모델 학습\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 및 평가\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 속성 중요도 출력\n",
        "import pandas as pd\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "print(feature_importances.sort_values(ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae991f14",
      "metadata": {},
      "source": [
        "### 2.5 랜덤 포레스트의 장점과 한계\n",
        "- **장점**  \n",
        "  - 단일 결정 트리보다 일반화 성능이 높음  \n",
        "  - 다양한 데이터셋에서 안정적인 성능  \n",
        "  - 속성 중요도를 통해 해석 가능  \n",
        "\n",
        "- **한계**  \n",
        "  - 단일 트리에 비해 학습 속도가 느릴 수 있음  \n",
        "  - 매우 많은 트리를 사용할 경우 계산량 증가  \n",
        "  - 최신 알고리즘에 비해 예측 성능이 떨어짐\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 배깅은 데이터를 여러 번 샘플링하여 여러 모델을 학습시키는 방법이다.  \n",
        "- 랜덤 포레스트는 여러 결정 트리를 조합한 대표적인 배깅 기법이다.  \n",
        "- 랜덤 포레스트는 과적합 위험을 줄이고, 안정적이고 강력한 성능을 제공한다.  \n",
        "- 속성 중요도를 통해 어떤 변수가 중요한지 확인할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7cf9be",
      "metadata": {},
      "source": [
        "## 3. 부스팅 (Boosting)\n",
        "\n",
        "### 3.1 부스팅 개념\n",
        "- 배깅과 달리, **순차적으로 모델을 학습**시키는 방법  \n",
        "- 각 단계에서 **이전 모델이 틀린 데이터를 더 잘 맞추도록** 다음 모델이 학습  \n",
        "- 즉, **이전 모델의 오차(error)** 를 점점 줄여가며 전체 성능을 향상  \n",
        "- 약한 학습기(Weak Learner) 여러 개를 **가중합(weighted sum)** 형태로 결합해  \n",
        "  강한 학습기(Strong Learner)를 만든다  \n",
        "- 대표 알고리즘: **Gradient Boosting, AdaBoost, XGBoost**\n",
        "\n",
        "**데이터는 복원추출하지 않음**  \n",
        "→ 모든 학습 단계에서 전체 데이터를 사용하지만, 각 샘플에 **가중치(weight)** 를 달리 부여하여  \n",
        "**이전 모델이 잘못 예측한 샘플에 더 집중**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d0e3fd2",
      "metadata": {},
      "source": [
        "\n",
        "### 3.2 Gradient Boosting\n",
        "- 기본 아이디어:  \n",
        "  1. 초기 예측값(단순 평균)으로 시작  \n",
        "  2. 모델이 틀린 부분(잔차, residual)을 계산 \n",
        "  3. 이 잔차를 줄이는 방향으로 **새로운 트리(약한 모델)** 추가 \n",
        "  4. 각 단계에서 **작은 학습률(learning rate)** 을 곱해 조금씩 보정  \n",
        "\n",
        "👉 장점: 높은 정확도  \n",
        "👉 단점: 순차 학습이라 학습 속도가 느림\n",
        "\n",
        "\n",
        "$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$ \n",
        "\n",
        "- $F_m(x)$: m번째까지의 전체 모델  \n",
        "- $h_m(x)$: 새로 추가되는 약한 모델  \n",
        "- $\\eta$: 학습률(learning rate) → 보정 강도를 조절"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21bce7c6",
      "metadata": {},
      "source": [
        "### 정리  \n",
        "\n",
        "배깅(Bagging)은 여러 모델을 **독립적으로 병렬 학습**시켜  \n",
        "결과를 평균(또는 다수결)하는 방식으로 **분산(Variance)** 을 줄이는 기법이고,  \n",
        " \n",
        "부스팅(Boosting)은 모델을 **순차적으로 학습**시켜 **앞선 모델의 오차를 다음 모델이 보정**하도록  \n",
        "만들어 **편향(Bias)** 을 줄이는 기법입니다.  \n",
        "\n",
        "👉 배깅은 “다양성으로 안정화”,  \n",
        "👉 부스팅은 “오차 보정으로 정밀화”를 노립니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962746d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Gradient Boosting 실습 (scikit-learn)\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 데이터\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 학습\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# 예측 및 평가\n",
        "y_pred = gb.predict(X_test)\n",
        "print(\"Gradient Boosting 정확도:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4526a253",
      "metadata": {},
      "source": [
        "### 3.3 XGBoost\n",
        "- Gradient Boosting을 최적화한 라이브러리  \n",
        "- 장점:  \n",
        "  - 더 빠른 학습 속도  \n",
        "  - 정규화(L1, L2) 지원 → 과적합 방지  \n",
        "  - 대규모 데이터에서도 효율적  \n",
        "- Kaggle 대회에서 자주 사용되는 강력한 알고리즘 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03829777",
      "metadata": {},
      "outputs": [],
      "source": [
        "### XGBoost 실습\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# 모델 학습\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# 예측 및 평가\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "print(\"XGBoost 정확도:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07703f38",
      "metadata": {},
      "source": [
        "### 3.4 Gradient Boosting vs XGBoost\n",
        "- **Gradient Boosting**: 구현이 단순, 기본적인 부스팅 개념 학습에 적합  \n",
        "- **XGBoost**: 속도와 성능이 뛰어나 실무 및 대규모 데이터셋에 적합  \n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 부스팅은 모델을 순차적으로 학습시켜 오차를 줄이는 방식이다.  \n",
        "- Gradient Boosting은 기본적인 부스팅 알고리즘이다.  \n",
        "- XGBoost는 Gradient Boosting을 개선하여 속도와 성능을 강화한 알고리즘이다.  \n",
        "- 학습률과 트리 개수는 부스팅 알고리즘에서 중요한 하이퍼파라미터이다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf5c0f7d",
      "metadata": {},
      "source": [
        "## 4. 모델 평가와 비교\n",
        "\n",
        "\n",
        "\n",
        "### 4.1 왜 모델 평가가 중요한가?\n",
        "- 단일 성능 지표만 보면 과적합을 놓치기 쉽습니다.  \n",
        "- 훈련 데이터뿐 아니라 **검증 데이터, 교차 검증 결과**로 모델을 평가해야 합니다.  \n",
        "- 특히 **결정 트리 vs 랜덤 포레스트 vs 부스팅(XGBoost)** 모델의 차이를 비교할 수 있어야 합니다.  \n",
        "\n",
        "\n",
        "\n",
        "### 4.2 교차 검증 (Cross Validation)\n",
        "- 데이터를 여러 폴드로 나눠 학습과 평가를 반복 → 평균 성능 확인  \n",
        "- 안정적이고 일반화된 성능 평가 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5867fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# 모델 정의\n",
        "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "\n",
        "models = {\"Decision Tree\": dt, \"Random Forest\": rf, \"Gradient Boosting\": gb, \"XGBoost\": xgb_model}\n",
        "\n",
        "# 교차 검증\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"{name} 평균 정확도: {np.mean(scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5d160c",
      "metadata": {},
      "source": [
        "### 4.3 과적합 확인\n",
        "- **결정 트리**: 깊이가 깊어질수록 과적합 위험 ↑  \n",
        "- **랜덤 포레스트**: 과적합 방지 효과 ↑  \n",
        "- **부스팅(XGBoost)**: 높은 성능, 그러나 과적합 가능 → 학습률과 트리 수 조정 필요  \n",
        "\n",
        "👉 해결책:  \n",
        "- 교차 검증으로 안정적 평가  \n",
        "- `max_depth`, `n_estimators`, `learning_rate` 등 하이퍼파라미터 조정  \n",
        "\n",
        "\n",
        "\n",
        "### 4.4 성능 비교 예시 출력\n",
        "- 보통 결과는 다음과 같이 나올 수 있음 (데이터셋에 따라 다름)  \n",
        "\n",
        "| 모델               | 평균 정확도 |\n",
        "|--|-|\n",
        "| Decision Tree      | 0.90        |\n",
        "| Random Forest      | 0.95        |\n",
        "| Gradient Boosting  | 0.96        |\n",
        "| XGBoost            | 0.97        |\n",
        "\n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 모델은 반드시 교차 검증으로 평가해야 한다.  \n",
        "- 결정 트리는 과적합되기 쉬운 반면, 랜덤 포레스트와 부스팅은 일반화 성능이 더 높다.  \n",
        "- XGBoost는 대규모 데이터셋에서 뛰어난 성능을 보인다.  \n",
        "- 하이퍼파라미터 튜닝을 통해 성능을 추가로 개선할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16dc280c",
      "metadata": {},
      "source": [
        "### 결정 트리와 앙상블 기법 – 실습 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00829d1a",
      "metadata": {},
      "source": [
        "### 문제 1. 결정 트리 학습 및 시각화\n",
        "Breast Cancer 데이터셋을 불러와서,  \n",
        "`max_depth=3`인 결정 트리를 학습하고 시각화하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plot_tree(model, feature_names=data.feature_names, class_names=data.target_names, filled=True)\n",
        "plt.show()\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a639080",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e07d5e5",
      "metadata": {},
      "source": [
        "### 문제 2. 랜덤 포레스트 학습 및 정확도 평가\n",
        "Breast Cancer 데이터셋을 사용하여  \n",
        "`n_estimators=100`인 랜덤 포레스트 분류기를 학습하고 테스트 정확도를 구하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e66a411",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8213a1b",
      "metadata": {},
      "source": [
        "### 문제 3. 속성 중요도 확인\n",
        "문제 2에서 학습한 랜덤 포레스트 모델의 **상위 5개 속성 중요도**를 출력하세요.  \n",
        "\n",
        "속성 중요도란?\n",
        "- **속성(feature) 중요도**는 모델이 **예측을 할 때 각 입력 변수가 얼마나 중요한 역할을 했는지**를 나타내는 값입니다.  \n",
        "- 값이 높을수록 그 특성이 **결과에 더 큰 영향을 미쳤다**는 의미입니다.  \n",
        "- 트리 기반 모델(의사결정나무, 랜덤포레스트, XGBoost 등)은 학습 과정에서  \n",
        "  데이터를 분할할 때 **불순도(impurity)** 를 얼마나 줄였는지를 기준으로  \n",
        "  자동으로 각 특성의 중요도를 계산합니다.  \n",
        "\n",
        "확인 방법\n",
        "- 학습된 모델 객체의 속성 `feature_importances_` 를 확인하면 됩니다.  \n",
        "- 이 값은 각 특성의 상대적 중요도를 나타내며,  \n",
        "  합계가 1(=100%)이 되도록 정규화되어 있습니다.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "print(feature_importances.sort_values(ascending=False).head(5))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30811844",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed4fad08",
      "metadata": {},
      "source": [
        "### 문제 4. Gradient Boosting 적용\n",
        "Breast Cancer 데이터셋에서 `learning_rate=0.1`, `n_estimators=100`인  \n",
        "GradientBoostingClassifier를 학습하고 정확도를 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb.predict(X_test)\n",
        "print(\"Gradient Boosting 정확도:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f59a898",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fc1371",
      "metadata": {},
      "source": [
        "### 문제 5. XGBoost 적용 및 비교\n",
        "Breast Cancer 데이터셋에서 XGBoost 모델을 학습하고,  \n",
        "Gradient Boosting과 정확도를 비교하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3,\n",
        "                          random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "print(\"XGBoost 정확도:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de0a3d0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74345546",
      "metadata": {},
      "source": [
        "### 문제 6. 교차 검증으로 모델 비교\n",
        "결정 트리, 랜덤 포레스트, Gradient Boosting, XGBoost 모델을  \n",
        "5겹 교차 검증하여 평균 정확도를 각각 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(max_depth=3, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42, \n",
        "                              use_label_encoder=False, eval_metric=\"logloss\")\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"{name} 평균 정확도: {np.mean(scores):.4f}\")\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86cb2f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d0e6a5",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 결정 트리는 이해하기 쉽지만 과적합 위험이 크다.  \n",
        "- 랜덤 포레스트는 배깅 기법으로 안정적 성능을 낸다.  \n",
        "- Gradient Boosting은 잔차를 줄이며 학습하는 강력한 기법이다.  \n",
        "- XGBoost는 최적화된 부스팅 알고리즘으로, 속도와 성능이 뛰어나다.  \n",
        "- 교차 검증을 통해 모델의 일반화 성능을 안정적으로 비교할 수 있다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669893e6",
      "metadata": {},
      "source": [
        "### Kaggle 경진대회 도전"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e283f7",
      "metadata": {},
      "source": [
        "**대회 주소 :** https://www.kaggle.com/competitions/titanic/overview\n",
        "\n",
        "**목표 :** 전처리 방법 변경 및 모델을 최적화하여 제출 후 스코어 0.82 이상 도달\n",
        "\n",
        "**주의사항 :** 파일 제출 횟수가 하루 10번으로 제한\n",
        "\n",
        "**참고 :**   아래 Baseline 코드는 참고만 하시고 되도록 직접 모든 과정을 해보시길 권장드립니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705bdb64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검증 데이터 정확도: 0.8212290502793296\n",
            "\n",
            "분류 보고서:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       105\n",
            "           1       0.80      0.76      0.78        74\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.82      0.81      0.81       179\n",
            "weighted avg       0.82      0.82      0.82       179\n",
            "\n",
            "\n",
            "제출 파일이 생성되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 데이터 불러오기\n",
        "train = pd.read_csv('data/titanic.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "# 데이터 전처리\n",
        "def preprocess_data(df):\n",
        "    # 결측치 처리\n",
        "    df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
        "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
        "    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n",
        "    \n",
        "    # 범주형 변수 처리\n",
        "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "    df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "    \n",
        "    # 필요한 특성 선택\n",
        "    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "    return df[features]\n",
        "\n",
        "# 학습 데이터 전처리\n",
        "X = preprocess_data(train)\n",
        "y = train['Survived']\n",
        "\n",
        "# 학습 데이터와 검증 데이터 분리\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# RandomForest 모델 생성 및 학습\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# 검증 데이터로 예측\n",
        "val_pred = rf_model.predict(X_val)\n",
        "\n",
        "# 모델 성능 평가\n",
        "print('검증 데이터 정확도:', accuracy_score(y_val, val_pred))\n",
        "print('\\n분류 보고서:')\n",
        "print(classification_report(y_val, val_pred))\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "test_processed = preprocess_data(test)\n",
        "test_pred = rf_model.predict(test_processed)\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test['PassengerId'],\n",
        "    'Survived': test_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('\\n제출 파일이 생성되었습니다.')\n"
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "MS0xMV/qsrDsoJXtirjrpqzsmYAg7JWZ7IOB67iUIOq4sOuylS5pcHluYg==",
    "inserted_date": [
      "2025-09-19"
    ],
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
