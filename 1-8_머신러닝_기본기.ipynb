{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "591dd570",
      "metadata": {},
      "source": [
        "#  머신러닝 기본기\n",
        "\n",
        "###  학습 목표\n",
        "이 토픽을 수강한 뒤, 수강생은 다음을 할 수 있어야 합니다:\n",
        "\n",
        "- 머신러닝의 기본 개념을 이해하고 설명할 수 있다.  \n",
        "- 머신러닝에 필요한 기초 수학 개념(선형대수학, 확률·통계, 미적분학)을 이해하고 적용할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc93bf41",
      "metadata": {},
      "source": [
        "###  목차\n",
        "\n",
        "#### 0. 들어가기\n",
        "- 머신러닝이란 무엇인가?  \n",
        "- AI → 머신러닝 → 딥러닝 관계 정리  \n",
        "- “데이터에서 패턴을 학습해 미래를 예측하는 것”으로 요약  \n",
        "\n",
        "\n",
        "\n",
        "#### 1. 머신러닝 개요\n",
        "- **정의와 역사**\n",
        "  - 규칙 기반 프로그래밍 vs 데이터 기반 학습\n",
        "  - 1950년대 퍼셉트론 → 2000년대 SVM → 2010년대 딥러닝  \n",
        "- **주요 응용 분야**\n",
        "  - 이미지 인식, 음성 인식, 자연어 처리, 추천 시스템, 자율주행\n",
        "- **학습 패러다임**\n",
        "  - 지도학습 (Supervised Learning): 입력 + 정답 → 모델 학습\n",
        "  - 비지도학습 (Unsupervised Learning): 정답 없는 데이터 패턴 찾기\n",
        "  - 강화학습 (Reinforcement Learning): 보상을 극대화하는 행동 학습\n",
        "\n",
        "\n",
        "\n",
        "#### 2. 머신러닝을 위한 기초 수학\n",
        "\n",
        "##### 2.1 선형대수학 복습\n",
        "- **벡터와 행렬의 개념**\n",
        "  - 데이터 = 벡터/행렬로 표현\n",
        "- **행렬 연산**\n",
        "  - 행렬곱 = 데이터 변환의 핵심\n",
        "- **벡터의 거리와 내적**\n",
        "  - 유클리드 거리: 데이터 간 유사도\n",
        "  - 내적: 두 벡터의 각도 → 코사인 유사도\n",
        "- **기하학적 해석**\n",
        "  - 고차원 공간에서 데이터의 위치와 관계 설명\n",
        "\n",
        "👉 머신러닝 활용 예: KNN(거리 기반 분류), PCA(차원 축소)  \n",
        "\n",
        "\n",
        "\n",
        "##### 2.2 확률과 통계 복습\n",
        "- **확률의 기본 개념**\n",
        "  - 사건, 표본 공간, 확률값 (0~1)\n",
        "- **확률변수**\n",
        "  - 이산형 (주사위, 동전 던지기), 연속형 (키, 몸무게)\n",
        "- **확률분포와 기대값**\n",
        "  - 정규분포, 베르누이 분포, 이항분포\n",
        "  - 기대값과 분산\n",
        "- **조건부 확률과 베이즈 정리**\n",
        "  - P(A|B) = P(A∩B)/P(B)\n",
        "  - 베이즈 정리: 사전확률 → 사후확률로 갱신\n",
        "- **머신러닝에서의 필요성**\n",
        "  - 데이터의 불확실성을 다루고, 예측에 신뢰도를 부여\n",
        "\n",
        "👉 머신러닝 활용 예: 스팸메일 필터(나이브 베이즈 분류기)  \n",
        "\n",
        "\n",
        "\n",
        "##### 2.3 미적분학 기초\n",
        "- **함수와 그래프**\n",
        "  - 입력 x → 출력 y\n",
        "- **미분**\n",
        "  - 변화율, 기울기  \n",
        "  - 극소점·극대점 찾기\n",
        "- **편미분과 기울기**\n",
        "  - 고차원 함수에서 변수별 기울기 계산\n",
        "- **적분**\n",
        "  - 면적과 누적값 해석\n",
        "- **머신러닝에서의 필요성**\n",
        "  - 손실 함수 최소화 (경사하강법)\n",
        "  - 뉴럴 네트워크 학습 = 미분 + 최적화\n",
        "\n",
        "👉 머신러닝 활용 예: 회귀분석(최소제곱법), 신경망 학습(역전파)  \n",
        "\n",
        "\n",
        "#### 3. 종합 연결\n",
        "- 머신러닝의 본질 = **수학적 개념을 도구 삼아 데이터에서 규칙을 학습하는 것**  \n",
        "- 선형대수학 → 데이터 구조 & 변환  \n",
        "- 확률·통계 → 불확실성 모델링  \n",
        "- 미적분학 → 최적화와 학습  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f64d2271",
      "metadata": {},
      "source": [
        "## 0. 들어가기\n",
        "\n",
        "머신러닝은 요즘 가장 많이 쓰이는 인공지능 기술 중 하나입니다.  \n",
        "- **AI (인공지능)**: 인간의 지능을 모방하는 모든 기술  \n",
        "- **머신러닝**: 데이터를 통해 스스로 규칙을 학습하는 AI의 한 분야  \n",
        "- **딥러닝**: 머신러닝의 한 갈래, 신경망(Neural Network)을 활용하는 방식  \n",
        "\n",
        "즉,  \n",
        "- AI ⊃ 머신러닝 ⊃ 딥러닝  \n",
        "\n",
        "- <img src=\"image/ai_ml_dl.png\" width=\"500\">  \n",
        "이미지 출처 : https://www.coursera.org/articles/ai-vs-deep-learning-vs-machine-learning-beginners-guide  \n",
        "👉 머신러닝은 “데이터에서 패턴을 학습해 미래를 예측하는 기술”이라고 요약할 수 있습니다.\n",
        "\n",
        "### 머신러닝의 대표적인 예시\n",
        "- **스팸 메일 분류**  \n",
        "  - **데이터**: 과거 이메일(스팸/정상 라벨 포함)  \n",
        "  - **학습 방식**: 이메일의 단어 등장 빈도나 발신자 정보 등을 숫자로 변환한 뒤,  \n",
        "    로지스틱 회귀(Logistic Regression) 같은 분류 알고리즘을 사용해  \n",
        "    “스팸일 확률”을 계산하도록 학습  \n",
        "  - **결과**: 새로운 메일이 들어오면 스팸 확률을 계산해 자동으로 분류  \n",
        "\n",
        "- **주택 가격 예측**  \n",
        "  - **데이터**: 과거 거래된 주택의 면적, 위치, 방 개수, 가격 정보  \n",
        "  - **학습 방식**: 선형회귀(Linear Regression)를 활용해,  \n",
        "    주택 가격이 면적·위치 등과 어떻게 선형적인 관계를 가지는지 규칙을 찾음  \n",
        "  - **결과**: 새로운 주택의 조건을 입력하면 예상 가격을 수치로 예측  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf97126",
      "metadata": {},
      "source": [
        "## 1. 머신러닝 개요\n",
        "\n",
        "## 1.1 머신러닝의 정의\n",
        "- **기존 프로그래밍**: 사람이 직접 규칙(조건문, 수식)을 코드로 작성  \n",
        "  - 예시:  \n",
        "    - 스팸 메일 분류 → 특정 단어(“광고”, “당첨”)가 있으면 스팸으로 처리  \n",
        "    - 주택 가격 예측 → 면적 × 일정 금액 + 방 개수 × 일정 금액 으로 계산식 직접 지정  \n",
        "- **머신러닝**: 데이터와 정답(레이블)을 주면, 컴퓨터가 스스로 규칙을 찾아냄  \n",
        "  - 예시:  \n",
        "    - 스팸 메일 분류 → 수천 개의 메일 데이터를 보고, 어떤 단어 조합이 스팸에 자주 나타나는지 학습  \n",
        "    - 주택 가격 예측 → 실제 거래 데이터를 보고, 면적·위치·방 개수와 가격의 관계를 자동으로 계산\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93c9287",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 규칙 기반 프로그래밍 예시\n",
        "def spam_filter(email):\n",
        "    if \"free\" in email.lower():\n",
        "        return \"spam\"\n",
        "    else:\n",
        "        return \"ham\"\n",
        "\n",
        "print(spam_filter(\"FREE money!!!\"))  # spam\n",
        "print(spam_filter(\"Hello friend\"))   # ham"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747bd198",
      "metadata": {},
      "source": [
        "위 규칙 기반 방식은 단순하지만, 복잡한 데이터에는 적용하기 어렵습니다.  \n",
        "머신러닝은 데이터를 많이 주어 **스스로 규칙을 찾도록** 하는 방식입니다.  \n",
        "\n",
        "## 1.2 머신러닝의 역사 (간단 개요)\n",
        "\n",
        "- **퍼셉트론(Perceptron, 1957·Rosenblatt)**  \n",
        "  - 퍼셉트론(Perceptron)은 1957년에 고안된 인공 신경망의 가장 기본적인 알고리즘.  \n",
        "  여러 개의 입력 신호를 받아 하나의 출력 신호를 내보내는 구조로, 인간의 신경 세포(뉴런)가 작동하는 방식을 모방해 만듦\n",
        "  - 선형 결정경계를 학습하는 **이진 분류기**. 입력 $(x$)에 대해 가중치 $(w$)와 편향 $(b$)를 적용해 $(y = \\text{step}(w^\\top x + b)$)로 예측.  \n",
        "  - 학습은 오분류된 샘플에 대해 $(w \\leftarrow w + \\eta (y - \\hat{y})x$) 형태의 **가중치 업데이트 규칙**으로 수행(온라인 학습).  \n",
        "  - **장점**: 단순하고 빠르다, 선형적으로 분리 가능한 데이터에 수렴 보장.  \n",
        "  - **한계**: XOR처럼 **비선형적으로 분리 불가능한 문제**는 해결 불가(Minsky & Papert, 1969).  \n",
        "  - **극복**: 은닉층을 쌓은 **다층 퍼셉트론(MLP)** 과 **역전파(Backpropagation, 1986)** 의 재발견으로 비선형 문제 학습 가능.\n",
        "  \n",
        "  <img src=\"image/neuron.png\" width =500>\n",
        "\n",
        "  <img src=\"image/perceptron.png\" width =500>  <img src=\"image/XOR.png\" width =342>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e52d4dc",
      "metadata": {},
      "source": [
        "- **1980~90년대: 고전 ML 전성기**  \n",
        "  - **SVM**(마진 극대화, 커널 트릭), **결정트리/랜덤포레스트**, **나이브 베이즈**, **KNN** 등 범용 성능이 높은 전통 알고리즘이 산업 전반에 확산.  \n",
        "  - 특징 공학(feature engineering)과 커널 선택이 성능의 핵심."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f1e0ae",
      "metadata": {},
      "source": [
        "- **2010년대 이후: 딥러닝 르네상스(Deep Learning Boom)**  \n",
        "  - **데이터**: 웹/모바일/IoT 보급으로 대규모 라벨링 데이터셋(예: ImageNet, 수백만 장) 가용.  \n",
        "  - **연산자원**: **GPU**(대규모 행렬·합성곱을 병렬 계산) 보편화, CUDA/라이브러리 최적화, 멀티-GPU·분산학습 도입.  \n",
        "  - **알고리즘 개선**: ReLU 활성화, 드롭아웃, 배치정규화, 잔차연결(ResNet), 최적화 기법(Adam 등).  \n",
        "  - **결정적 사건**: 2012년 **AlexNet**이 ImageNet에서 오류율을 크게 낮추며 컴퓨터비전 판도를 변경.  \n",
        "  - **생태계**: Theano·Caffe에서 **TensorFlow, PyTorch**로 이어지는 프레임워크의 성숙, 대규모 학습을 가능케 하는 **클라우드/TPU** 등 하드웨어 가속.  \n",
        "  - 결과적으로 영상·음성·자연어 처리 전 영역에서 **규모의 경제(데이터×모델×연산)** 가 성능을 주도."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c705cd9a",
      "metadata": {},
      "source": [
        "## 1.3 머신러닝의 주요 응용 분야\n",
        "- 이미지 인식 (얼굴 인식, 자율주행)  \n",
        "- 음성 인식 (스마트 스피커)  \n",
        "- 자연어 처리 (번역, 챗봇)  \n",
        "- 추천 시스템 (넷플릭스, 유튜브, 쇼핑몰)  \n",
        "- 금융 (사기 탐지, 신용평가)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e73850",
      "metadata": {},
      "source": [
        "## 1.4 머신러닝의 학습 패러다임\n",
        "\n",
        "### (1) 지도학습 (Supervised Learning)\n",
        "\n",
        "- **정의**: 입력 $x$와 정답 라벨 $y$가 `쌍으로 주어졌을 때`, $(f_\\theta: x \\mapsto y$)를 학습하여  \n",
        "**보지 못한 데이터**의 $y$를 정확히 예측하도록 하는 학습.  \n",
        "- **목표**: 손실함수 $( \\mathcal{L}(y, \\hat{y}) $)를 최소화하도록 파라미터 $\\theta$를 최적화(보통 SGD/Adam 등).  \n",
        "- **주요 과업**  \n",
        "  - **분류(Classification)**: 이산 라벨 예측(이메일 스팸/정상, 질병 유무).  \n",
        "  손실: 크로스엔트로피.  \n",
        "  지표: 정확도, 정밀도/재현율/F1, ROC-AUC.  \n",
        "\n",
        "  - **회귀(Regression)**: 연속 값 예측(주택가격, 수요량).  \n",
        "  손실: MSE/MAE.  \n",
        "  지표: RMSE, $(R^2$).  \n",
        "  - (필요 시) **랭킹/확률 추정/다중라벨** 등으로 확장 가능.\n",
        "\n",
        "- **일반 파이프라인**  \n",
        "  1) **데이터 분할**: Train/Validation/Test(예: 6/2/2 또는 K-겹 교차검증).  \n",
        "  2) **전처리/특징화**: 결측치 처리, 스케일링, 인코딩, 데이터 증강(이미지/텍스트).  \n",
        "  3) **모델 선택**: 선형모델, 트리계열, 커널 SVM, 신경망 등 과업/데이터 규모/해석가능성에 따라 선택.  \n",
        "  4) **학습/튜닝**: 하이퍼파라미터 탐색(그리드/베이지안), 조기종료, 정규화(L1/L2/드롭아웃).  \n",
        "  5) **평가/검증**: 유효성 검증 지표와 에러분석(혼동행렬, 잔차진단).  \n",
        "  6) **배포/모니터링**: 데이터 분포 변화(컨셉 드리프트) 감시, 재학습 전략 설계.\n",
        "\n",
        "- **현업 고려사항(실전 포인트. 아직은 참고만 하세요 ㅎㅎ)**  \n",
        "  - **과적합**: 파라미터가 데이터보다 강할 때 발생 → 정규화, 데이터 증강, 교차검증, 단순화.  \n",
        "  - **데이터 누수(Data Leakage)**: 학습 시점에 테스트 정보가 유입되는 문제 → 파이프라인 순서 엄수(스케일링은 분할 후 학습셋 기준).  \n",
        "  - **클래스 불균형**: 가중 손실, 리샘플링(SMOTE/언더샘플링), 적절한 지표(F1/PR-AUC) 채택.  \n",
        "  - **라벨 품질**: 라벨링 기준서, 다중 어노테이터 합의  \n",
        "  - **재현성**: 시드 고정, 데이터/코드/환경 버저닝.\n",
        "\n",
        "- **예(분류)**: *나이·소득·과거 구매 횟수 → “구매할지/안할지” 예측*  \n",
        "  - 전처리: 결측 보간 → 표준화/원-핫 인코딩.  \n",
        "  - 모델: 로지스틱 회귀(해석용) 또는 트리/부스팅(XGBoost/LightGBM) → 성능 비교.  \n",
        "  - 지표: 정확도와 함께 **정밀도·재현율·F1**로 불균형 대응."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fcc17b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "#  지도학습 전체 파이프라인 예시 (Linear Regression)\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import joblib  # 모델 저장용\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1) 데이터 분할 (Train / Validation / Test)\n",
        "# ------------------------------------------------\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8]])\n",
        "y = np.array([2, 4.1, 6.2, 8.1, 10, 12.1, 13.9, 16])\n",
        "\n",
        "# Train(60%) / Validation(20%) / Test(20%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "# (0.25 of 0.8 = 0.2 → 총 6:2:2 비율)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2) 전처리: 스케일링 (평균=0, 표준편차=1)\n",
        "# ------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3) 모델 선택: Ridge 회귀 (L2 정규화 포함)\n",
        "# ------------------------------------------------\n",
        "model = LinearRegression()()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4) 하이퍼파라미터 탐색 및 학습\n",
        "# ------------------------------------------------\n",
        "param_grid = {'alpha': [0.01, 0.1, 1, 10]}\n",
        "grid = GridSearchCV(model, param_grid, cv=3, scoring='r2')\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(f\"최적 하이퍼파라미터: {grid.best_params_}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5) 평가 및 검증\n",
        "# ------------------------------------------------\n",
        "# Validation\n",
        "y_val_pred = best_model.predict(X_val_scaled)\n",
        "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "print(f\"[Validation] RMSE={val_rmse:.3f}, R²={val_r2:.3f}\")\n",
        "\n",
        "# Test\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "print(f\"[Test] RMSE={test_rmse:.3f}, R²={test_r2:.3f}\")\n",
        "\n",
        "# 예측 예시\n",
        "new_X = np.array([[9]])\n",
        "new_X_scaled = scaler.transform(new_X)\n",
        "pred = best_model.predict(new_X_scaled)\n",
        "print(f\"공부시간 9시간 → 예측 점수 {pred[0]:.2f}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6) 배포/모니터링 대비: 모델 및 전처리 저장\n",
        "# ------------------------------------------------\n",
        "joblib.dump(best_model, 'ridge_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"모델과 스케일러 저장 완료 (ridge_model.pkl, scaler.pkl)\")\n",
        "\n",
        "# 추후 재로딩 예시:\n",
        "# loaded_model = joblib.load('ridge_model.pkl')\n",
        "# loaded_scaler = joblib.load('scaler.pkl')\n",
        "# 예측 시 loaded_scaler.transform() 후 loaded_model.predict() 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fcd36d",
      "metadata": {},
      "source": [
        "### (2) 비지도학습 (Unsupervised Learning)\n",
        "- 정답(레이블) 없이 데이터 패턴을 학습  \n",
        "- 군집화(Clustering), 차원축소(Dimensionality Reduction)  \n",
        "\n",
        "- <img src=\"image/unsupervised.jpg\">  \n",
        "이미지 출처 : https://commons.wikimedia.org/wiki/File:Img_agrupament.jpg\n",
        "\n",
        "예: 고객을 데이터 패턴에 따라 그룹으로 나누기, 식물의 품종 분류  \n",
        "\n",
        "\n",
        "\n",
        "### (3) 강화학습 (Reinforcement Learning)\n",
        "- 에이전트(Agent)가 환경(Environment)과 상호작용하면서 **보상(Reward)** 을 극대화하도록 학습  \n",
        "- 체스, 바둑, 게임, 로보틱스 등에 활용  \n",
        "\n",
        "![Atari Breakout](image/atari.png)  \n",
        "*Atari Breakout – 강화학습 적용하여 벽돌깨기와 같은 게임을 하는 모델을 만들 수 있음*\n",
        "### ✅ 체크포인트\n",
        "- 머신러닝은 데이터에서 규칙을 학습하는 AI 기술이다.  \n",
        "- 지도학습, 비지도학습, 강화학습으로 나뉜다.  \n",
        "- 지도학습은 정답이 있는 데이터, 비지도학습은 정답이 없는 데이터, 강화학습은 보상을 통한 학습이다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e07f89",
      "metadata": {},
      "source": [
        "## 2. 머신러닝을 위한 기초 수학\n",
        "\n",
        "머신러닝의 많은 알고리즘은 **선형대수학, 확률·통계, 미적분학** 개념 위에 세워져 있습니다.  \n",
        "이번 섹션에서는 먼저 **선형대수학 복습**을 진행합니다.  \n",
        "\n",
        "\n",
        "\n",
        "- **스칼라(Scalar)**: 단일 숫자 (0차원 데이터)  \n",
        "  - 예: `3`, `π`, `-1.5`\n",
        "\n",
        "- **벡터(Vector)**: 숫자를 한 줄로 나열한 것 (1차원 데이터)  \n",
        "  - 예: \\([1, 2, 3]\\)  \n",
        "  - 수학적으로는 n개의 원소를 가진 벡터 = **n차원 벡터**  \n",
        "  - 넘파이에서 `shape = (n,)`\n",
        "\n",
        "- **행렬(Matrix)**: 숫자가 직사각형 형태로 배열된 것 (2차원 데이터)  \n",
        "  - 예:  \n",
        "   $$\n",
        "    \\begin{bmatrix}\n",
        "    1 & 2 \\\\\n",
        "    3 & 4 \\\\\n",
        "    \\end{bmatrix}\n",
        "    $$  \n",
        "  - 넘파이에서 `shape = (m, n)`\n",
        "\n",
        "- **텐서(Tensor)**: 3차원 이상의 다차원 배열 (3차원 데이터 이상)  \n",
        "  - 예: 28×28 픽셀 흑백 이미지 100장 → `shape = (100, 28, 28)`  \n",
        "  - 딥러닝에서 주로 사용되며, 색상 채널(RGB)까지 포함하면 `shape = (100, 28, 28, 3)`\n",
        "\n",
        "  <img src=\"image/vector.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0bf232f",
      "metadata": {},
      "source": [
        "### 혼동하기 쉬운 개념: \"배열 차원(ndim)\" vs \"벡터공간 차원\"\n",
        "\n",
        "- **배열 차원(ndim, 프로그래밍 관점)**  \n",
        "  - 배열의 축(axis) 개수를 뜻함  \n",
        "  - 예: `np.array([1,2,3])` → `ndim=1`, `shape=(3,)`\n",
        "\n",
        "- **벡터공간 차원(수학 관점)**  \n",
        "  - 벡터가 속한 공간의 좌표축 개수  \n",
        "  - 예: `[1,2,3]`은 $(\\mathbb{R}^3$)에 속하는 3차원 벡터  \n",
        "\n",
        "👉 따라서 `np.array([1,2,3])`는 **배열 차원=1**이면서 **벡터공간 차원=3**으로 해석할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7965fbbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 벡터 예시\n",
        "v = np.array([1, 2, 3])\n",
        "print(\"벡터:\", v)\n",
        "\n",
        "# 행렬 예시\n",
        "M = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "print(\"행렬:\\n\", M)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4081a9",
      "metadata": {},
      "source": [
        "### 행렬 연산\n",
        "\n",
        "- **덧셈/뺄셈**: 같은 크기의 행렬끼리 원소별 연산  \n",
        "- **스칼라 곱**: 행렬의 모든 원소에 같은 수를 곱함  \n",
        "- **행렬 곱**: (m×n) 행렬 × (n×p) 행렬 = (m×p) 행렬  \n",
        "\n",
        "### 내적 (Dot Product)\n",
        "\n",
        "- 정의: 두 벡터를 곱해 스칼라를 얻는 연산  \n",
        "- 수식:  \n",
        "  $$\n",
        "  \\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i\n",
        "  $$\n",
        "\n",
        "- 예시:  \n",
        "  $$\n",
        "  \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}\n",
        "  \\cdot\n",
        "  \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix}\n",
        "  =\n",
        "  1\\cdot 4 + 2\\cdot 5 + 3\\cdot 6 = 32\n",
        "  $$\n",
        "\n",
        "\n",
        "### 행렬 곱 (Matrix Multiplication)\n",
        "\n",
        "- 정의:  \n",
        "  $$\n",
        "  C = A \\times B, \\quad C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
        "  $$\n",
        "\n",
        "- 예시:  \n",
        "  $$\n",
        "  A =\n",
        "  \\begin{bmatrix}\n",
        "  1 & 2 \\\\\n",
        "  3 & 4\n",
        "  \\end{bmatrix},\n",
        "  \\quad\n",
        "  B =\n",
        "  \\begin{bmatrix}\n",
        "  5 & 6 \\\\\n",
        "  7 & 8\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  A \\times B =\n",
        "  \\begin{bmatrix}\n",
        "  1\\cdot 5 + 2\\cdot 7 & 1\\cdot 6 + 2\\cdot 8 \\\\\n",
        "  3\\cdot 5 + 4\\cdot 7 & 3\\cdot 6 + 4\\cdot 8\n",
        "  \\end{bmatrix}\n",
        "  =\n",
        "  \\begin{bmatrix}\n",
        "  19 & 22 \\\\\n",
        "  43 & 50\n",
        "  \\end{bmatrix}\n",
        "  $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac80a4b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 머신러닝에서 행렬곱은 입력 데이터를 가중치(Weight) 행렬에 곱해 예측값을 만드는 데 사용됩니다.  \n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "print(\"A + B:\\n\", A + B)\n",
        "print(\"A * 2:\\n\", A * 2)\n",
        "print(\"A × B (행렬곱):\\n\", np.dot(A, B))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42061446",
      "metadata": {},
      "source": [
        "### 벡터의 거리와 내적\n",
        "\n",
        "벡터 간의 관계를 이해하는 것은 머신러닝, 정보검색, 자연어처리 등에서 매우 중요합니다.  \n",
        "- **유클리드 거리**는 두 점 사이의 “얼마나 떨어져 있는지”를 재는 척도입니다.  \n",
        "- **내적**은 두 벡터가 “얼마나 같은 방향을 향하고 있는지”를 나타냅니다.  \n",
        "- **코사인 유사도**는 내적을 정규화하여 “방향 유사성만”을 비교할 수 있게 해줍니다.  \n",
        "  내적은 벡터의 크기와 방향 모두에 의존하기 때문에, 같은 방향이라도 크기가 다르면 값이 달라집니다.  \n",
        "  코사인 유사도는 두 벡터의 길이로 나누어 크기의 영향을 제거하고, 오직 방향(각도)만 비교합니다.  \n",
        "  문서나 문장의 의미적 유사도 계산에 자주 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065730a3",
      "metadata": {},
      "source": [
        "#### **유클리드 거리 (Euclidean distance)**: 두 점(벡터) 사이의 직선 거리  \n",
        "\n",
        "$$\n",
        "d(\\vec{x}, \\vec{y}) = \\sqrt{\\sum_i (x_i - y_i)^2}\n",
        "$$\n",
        "\n",
        "#### 참고: np.linalg.norm(x)\n",
        "NumPy의 np.linalg.norm은 유클리드 거리(벡터의 길이) 를 쉽게 구할 수 있는 함수입니다.  \n",
        "일반적으로, 노름(norm) 은 벡터의 “크기” 또는 “길이”를 수학적으로 정의한 개념입니다.   \n",
        "\n",
        "예시코드 : \n",
        "\n",
        "```python\n",
        "# 벡터의 유클리드 거리 (기본값 ℓ2 노름)\n",
        "v = np.array([3, 4])\n",
        "print(np.linalg.norm(v))  # 5.0  → √(3² + 4²)\n",
        "\n",
        "# 두 점 사이 거리\n",
        "p1 = np.array([1, 2])\n",
        "p2 = np.array([4, 6])\n",
        "print(np.linalg.norm(p1 - p2))  # 5.0  → √((4-1)² + (6-2)²)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c3eb32",
      "metadata": {},
      "source": [
        "#### **내적 (Dot Product)**: 두 벡터의 크기와 방향을 동시에 반영하는 값  \n",
        "\n",
        "$$\n",
        "\\vec{x} \\cdot \\vec{y} = \\sum_i x_i y_i\n",
        "$$\n",
        "\n",
        "  - 양수 → 비슷한 방향  \n",
        "  - 음수 → 반대 방향  \n",
        "  - 0 → 직교 (서로 독립)  \n",
        "\n",
        "- **코사인 유사도 (Cosine Similarity)**: 두 벡터의 **방향만** 비교하는 척도  \n",
        "\n",
        "$$\n",
        "\\cos(\\theta) = \\frac{\\vec{x} \\cdot \\vec{y}}{\\|\\vec{x}\\|\\|\\vec{y}\\|}\n",
        "$$\n",
        "\n",
        "  - 1 → 같은 방향  \n",
        "  - 0 → 직교  \n",
        "  - -1 → 정반대 방향  \n",
        "  - 문서 유사도 계산 등에서 자주 활용"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a66d325",
      "metadata": {},
      "source": [
        "### 연습 문제. 코사인 유사도 계산  \n",
        "벡터 `a = [1, 2, 2]`, `b = [2, 1, 2]`의 내적과 코사인 유사도를 계산하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "1. 내적 계산  \n",
        "$$\n",
        "a \\cdot b = (1 \\times 2) + (2 \\times 1) + (2 \\times 2) = 2 + 2 + 4 = 8\n",
        "$$\n",
        "\n",
        "2. 벡터의 크기  \n",
        "$$\n",
        "\\|a\\| = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{1 + 4 + 4} = \\sqrt{9} = 3\n",
        "$$  \n",
        "$$\n",
        "\\|b\\| = \\sqrt{2^2 + 1^2 + 2^2} = \\sqrt{4 + 1 + 4} = \\sqrt{9} = 3\n",
        "$$\n",
        "\n",
        "3. 코사인 유사도  \n",
        "$$\n",
        "\\cos(\\theta) = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{8}{3 \\times 3} = \\frac{8}{9} \\approx 0.889\n",
        "$$\n",
        "\n",
        "**정답:**  \n",
        "- 내적 = 8  \n",
        "- 코사인 유사도 ≈ 0.889  \n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f2395f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 파이썬으로 유클리드 거리, 내적, 코사인 유사도 계산 예시\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2])\n",
        "y = np.array([3, 4])\n",
        "\n",
        "# 유클리드 거리\n",
        "dist = np.linalg.norm(x - y)\n",
        "print(\"유클리드 거리:\", dist)\n",
        "\n",
        "# 내적\n",
        "dot = np.dot(x, y)\n",
        "print(\"내적:\", dot)\n",
        "\n",
        "# 코사인 유사도\n",
        "cos_sim = dot / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "print(\"코사인 유사도:\", cos_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8323ec72",
      "metadata": {},
      "source": [
        "### 기하학적 해석\n",
        "- 벡터는 공간에서의 “방향과 크기”를 나타냄  \n",
        "- 행렬 × 벡터 곱은 벡터를 **회전·확대·축소·투영**하는 선형변환으로 이해 가능  \n",
        "\n",
        "예시:  \n",
        "$A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}, \\quad x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$   \n",
        "\n",
        "$y = A x = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$ \n",
        "\n",
        "→ \\(x\\) 벡터가 x축 방향으로 2배, y축 방향으로 3배 늘어남.  \n",
        "\n",
        "👉 예: PCA(주성분분석)는 고차원 데이터를 새로운 축으로 투영하여 차원을 줄임  \n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 머신러닝의 데이터는 벡터와 행렬로 표현된다.  \n",
        "- 행렬 연산(특히 곱셈)은 입력 데이터 × 가중치 계산의 핵심이다.  \n",
        "- 유클리드 거리와 내적은 데이터 간 유사도를 계산하는 기본 도구다.  \n",
        "- 선형대수학은 머신러닝에서 “데이터를 수학적으로 다루는 언어” 역할을 한다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e1aa29",
      "metadata": {},
      "source": [
        "## 2.2 확률과 통계 복습\n",
        "\n",
        "머신러닝은 데이터의 **불확실성**을 다루는 학문입니다.  \n",
        "따라서 확률과 통계는 머신러닝을 이해하는 데 필수적인 배경지식입니다.  \n",
        "\n",
        "### 확률의 기본 개념\n",
        "- **사건(Event)**: 주사위를 던져 3이 나오는 것  \n",
        "- **표본 공간(Sample space)**: 가능한 모든 결과 (예: {1,2,3,4,5,6})  \n",
        "- **확률**: 사건이 일어날 가능성 (0 ≤ P(A) ≤ 1)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd32ba0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 동전 10번 던졌을 때 앞면이 나오는 횟수 시뮬레이션\n",
        "import numpy as np\n",
        "\n",
        "trials = np.random.choice([\"앞\", \"뒤\"], size=10, p=[0.5, 0.5])\n",
        "print(trials)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d156f2",
      "metadata": {},
      "source": [
        "### 확률변수\n",
        "- **정의**: 확률변수(Random Variable)는 **확률실험의 결과를 실수(real number)로 대응시키는 함수**  \n",
        "  - 즉, \"집합\"이 아니라 **함수형 객체**이며, 값의 자료형은 실수(ℝ)로 정의됨  \n",
        "  - 다만, 그 확률변수가 실제로 가질 수 있는 값들의 집합(지원, support)은 문제에 따라 정수 집합일 수도, 연속 구간일 수도 있음  \n",
        "\n",
        "- **이산확률변수 (Discrete Random Variable)** \n",
        "  - 유한개의 값 혹은 셀 수 있는 개수의 값으로 구성되어 있는 확률 변수\n",
        "  - 가질 수 있는 값이 **유한하거나 셀 수 있는 무한 집합**에 속함  \n",
        "  - 자료형: 정수형(int) 값으로 취급되는 경우가 많음  \n",
        "  - 예: 주사위 눈금 $X \\in \\{1,2,3,4,5,6\\}$, 동전 앞/뒤 $Y \\in \\{0,1\\}$  \n",
        "\n",
        "- **연속확률변수 (Continuous Random Variable)**  \n",
        "  - 가질 수 있는 값이 **연속 구간(실수 전체 구간의 부분집합)**  \n",
        "  - 자료형: 실수형(float, real) 값으로 취급  \n",
        "  - 예: 사람의 키 $Z \\in [140,200]$ cm, 시험 점수 $W \\in [0,100]$ "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6f6a8a",
      "metadata": {},
      "source": [
        "### 확률분포와 기대값\n",
        "- **확률분포**: 확률변수가 각 값(이산/연속)에 대해 나타날 가능성을 설명하는 규칙  \n",
        "  - 예: 베르누이분포(동전), 이항분포(반복된 동전 던지기), 정규분포(자연 현상 데이터)  \n",
        "\n",
        "- **기대값(평균)**: 확률변수가 장기적으로 가질 값의 “중심”  \n",
        "  - 이산형 예: 주사위 $X$의 기대값  \n",
        "    $E[X] = \\sum_{k=1}^6 k \\cdot \\frac{1}{6} = 3.5$  \n",
        "  - 연속형 예: $X \\sim U(0,1)$ (0~1 균등분포)라면  \n",
        "    $E[X] = \\int_0^1 x \\, dx = 0.5$  \n",
        "\n",
        "- **분산**: 기대값을 중심으로 한 값들의 퍼짐 정도  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "febc2895",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 정규분포 난수 생성\n",
        "# loc   : 평균 (mean)\n",
        "# scale : 표준편차 (standard deviation)\n",
        "# size  : 생성할 난수의 개수\n",
        "data = np.random.normal(loc=0, scale=1, size=1000)\n",
        "\n",
        "print(\"평균:\", np.mean(data))\n",
        "print(\"분산:\", np.var(data))\n",
        "\n",
        "# 히스토그램 시각화\n",
        "plt.hist(data, bins=30, density=True, alpha=0.6, color=\"b\", edgecolor=\"black\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ff2214",
      "metadata": {},
      "source": [
        "### 조건부 확률과 베이즈 정리\n",
        "- **조건부 확률**: 어떤 사건 B가 일어났다는 전제하에 A가 일어날 확률  \n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
        "$$\n",
        "\n",
        "- **베이즈 정리**: 새로운 정보가 주어졌을 때 확률을 갱신  \n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "👉 머신러닝 예시: 스팸메일 필터 (단어가 등장했을 때 메일이 스팸일 확률)  \n",
        "\n",
        "\n",
        "\n",
        "### 머신러닝에서 확률·통계의 역할\n",
        "- 데이터의 **불확실성**을 수학적으로 모델링  \n",
        "- 예측 결과에 **신뢰도(probability)** 부여  \n",
        "- 확률 분포 기반 알고리즘:  \n",
        "  - 나이브 베이즈 분류기  \n",
        "  - 가우시안 혼합 모델 (GMM)  \n",
        "  - Hidden Markov Model (HMM)  \n",
        "\n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 확률은 사건이 일어날 가능성을 나타내며, 0~1 사이 값이다.  \n",
        "- 확률변수는 이산형과 연속형으로 나뉜다.  \n",
        "- 기대값과 분산은 데이터의 중심과 퍼짐을 나타낸다.  \n",
        "- 조건부 확률과 베이즈 정리는 머신러닝에서 스팸 필터, 분류 문제 등에서 핵심적이다.  \n",
        "- 머신러닝의 많은 모델은 확률적 사고를 기반으로 한다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db52d7ca",
      "metadata": {},
      "source": [
        "## 2.3 미적분학 기초\n",
        "\n",
        "머신러닝은 **최적화(Optimization)** 문제와 깊은 관련이 있습니다.  \n",
        "최적화를 이해하기 위해서는 **함수, 미분, 적분** 개념이 필요합니다.  \n",
        "\n",
        "\n",
        "\n",
        "### 함수와 그래프\n",
        "- 함수: 입력 x → 출력 y  \n",
        "- 머신러닝의 모델은 대부분 **함수 형태**로 표현됨 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d275d378",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = x**2\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title(\"y = x^2 함수 그래프\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb34aa4",
      "metadata": {},
      "source": [
        "#### 미분 (Derivative)\n",
        "- 함수의 순간 변화율 = 기울기  \n",
        "- 머신러닝에서는 **손실 함수(Loss Function)** 의 최소화를 위해 활용됨  \n",
        "  - 손실 함수 = 모델의 예측값과 실제 정답의 차이를 수치로 표현  \n",
        "  - 손실이 작을수록 모델이 데이터를 잘 설명한다는 의미  \n",
        "  - 따라서 **최솟값 = 가장 잘 학습된 모델**을 뜻함  \n",
        "\n",
        "<p style=\"background-color:white; display:inline-block; padding:5px;\">\n",
        "  <img src=\"image/Derivative.svg\" alt=\"Derivative\">\n",
        "</p>  \n",
        "\n",
        "이미지 출처 : https://commons.wikimedia.org/wiki/File:Derivative_-_geometric_meaning.svg  \n",
        "\n",
        "예: $f(x) = x^2 \\quad \\Rightarrow \\quad f'(x) = 2x$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5d5da7",
      "metadata": {},
      "source": [
        "#### 미분이 쓰이는 이유\n",
        "- **미분 전 값 $f(x)$**: 현재 손실의 크기 → “지금 모델이 얼마나 틀렸는가”  \n",
        "- **미분 후 값 $f'(x)$**: 기울기(gradient) → “어느 방향으로, 얼마나 이동해야 손실이 줄어드는가”  \n",
        "\n",
        "👉 손실 함수의 최솟값을 직접 계산하기는 어렵지만,  \n",
        "“기울기(미분값)”을 이용하면 조금씩 줄여 나가면서 결국 최솟값에 수렴할 수 있음.  \n",
        "\n",
        "#### 극소점과 최적화\n",
        "- **극소점/극대점**은 미분값이 0인 지점에서 나타남  \n",
        "- 머신러닝의 학습은 결국 **극소점(손실 최소점)** 을 찾아가는 과정  \n",
        "\n",
        "<img src=\"image/Maxmin.png\" width=\"550\">\n",
        "\n",
        "이미지 출처 : https://commons.wikimedia.org/wiki/File:Derivative_-_geometric_meaning.svg  \n",
        "\n",
        "### 경사 하강법 (Gradient Descent)\n",
        "1. 현재 위치에서 기울기(미분값)를 계산  \n",
        "2. 기울기의 **반대 방향**으로 이동 (손실 감소 방향)  \n",
        "3. 이를 반복하면 손실이 점점 줄어들어 극소점 근처에 도달  \n",
        "\n",
        "✅ 정리  \n",
        "- $f(x)$ = 현재 손실 값 (얼마나 틀렸는지)  \n",
        "- $f'(x)$ = 기울기 (손실을 줄이기 위해 이동할 방향)  \n",
        "- 최솟값 = 손실이 가장 작은 지점 → 학습된 모델 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3084b721",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 코드 예시 1: 함수와 도함수\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = x**2\n",
        "dy = 2*x  # 도함수\n",
        "\n",
        "plt.plot(x, y, label=\"y = x^2\")\n",
        "plt.plot(x, dy, label=\"y' = 2x\")\n",
        "plt.legend()\n",
        "#plt.title(\"함수와 도함수\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6d7b33",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 예: 간단한 경사 하강법 (Gradient Descent)\n",
        "# 목표: f(x) = x^2 함수의 최솟값을 찾는 과정 시뮬레이션\n",
        "\n",
        "x = 5     # 초기값 (시작점). 처음에는 x=5에서 출발\n",
        "lr = 0.1  # 학습률 (learning rate). 한 번 이동할 때의 보폭 크기\n",
        "\n",
        "x_history = [x]  # 반복 과정에서의 x 값을 저장 (시각화를 위해)\n",
        "\n",
        "for i in range(10):\n",
        "    grad = 2*x          # 미분 f'(x) = 2x → 현재 위치에서의 기울기\n",
        "    x = x - lr*grad     # 새로운 위치 = 현재 위치 - (학습률 × 기울기)\n",
        "                        # → 기울기의 반대 방향으로 이동 (손실을 줄이는 방향)\n",
        "    x_history.append(x) # 기록 저장\n",
        "    print(f\"{i+1}번째 반복 후 x={x:.4f}\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 시각화\n",
        "# f(x) = x^2 그래프 위에 경사 하강법의 이동 경로를 표시\n",
        "# --------------------------------------------------------\n",
        "\n",
        "# f(x) 정의\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "# x 범위 설정\n",
        "xs = np.linspace(-6, 6, 200)\n",
        "ys = f(xs)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(xs, ys, label=\"$f(x) = x^2$\", color=\"blue\")        # 함수 그래프\n",
        "plt.scatter(x_history, [f(val) for val in x_history],\n",
        "            color=\"red\", marker=\"o\", label=\"Gradient Descent Path\")  # 반복 경로\n",
        "plt.plot(x_history, [f(val) for val in x_history], color=\"red\", linestyle=\"--\")\n",
        "\n",
        "# 시각적 보조 요소\n",
        "plt.title(\"Gradient Descent (f(x)=x^2)\")\n",
        "plt.xlabel(\"x 값\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb223f1",
      "metadata": {},
      "source": [
        "### 편미분과 기울기 (Gradient)\n",
        "\n",
        "머신러닝에서 모델을 학습시킨다는 것은 **손실 함수(loss function)를 최소화하는 과정**입니다.  \n",
        "이 최소화 과정을 수행할 때, 우리가 사용하는 핵심 알고리즘이 바로 **경사 하강법(Gradient Descent)** 입니다.  \n",
        "\n",
        "- 경사 하강법에서는 함수의 **기울기(Gradient)** 정보를 이용해, 손실이 줄어드는 방향으로 파라미터를 조정합니다.  \n",
        "- 신경망(Neural Network)에서는 수천, 수만 개의 파라미터가 있기 때문에, 각 파라미터에 대해 **편미분(Partial Derivative)**을 계산해야 합니다.  \n",
        "- 이때, 모든 파라미터에 대한 편미분을 동시에 계산하는 과정이 바로 **역전파(Backpropagation)** 알고리즘입니다.\n",
        "\n",
        "\n",
        "##### **편미분 (Partial Derivative)**: 여러 변수를 가진 함수에서, **한 변수만 변하게 하고 나머지는 고정한 채 변화율을 구하는 것**  \n",
        "  - 예: $f(x,y) = x^2 + y^2$  \n",
        "    - $\\frac{\\partial f}{\\partial x} = 2x$  \n",
        "    \n",
        "    - $\\frac{\\partial f}{\\partial y} = 2y$  \n",
        "\n",
        "##### **기울기 벡터 (Gradient Vector)**: 함수의 모든 편미분을 모아 벡터로 표현  \n",
        "  $$\n",
        "  \\nabla f(x,y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
        "  $$\n",
        "  - 기울기는 **함수가 가장 빠르게 증가하는 방향**을 가리킴  \n",
        "  - 반대로, $-\\nabla f$는 가장 빠르게 감소하는 방향 → 경사 하강법에서 이동 방향 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9cc751b",
      "metadata": {},
      "source": [
        "### 문제 1. 편미분 기본\n",
        "함수 $( f(x,y) = x^2y + 3y $) 에 대하여  \n",
        "- $(\\frac{\\partial f}{\\partial x}$)  \n",
        "\n",
        "\n",
        "- $(\\frac{\\partial f}{\\partial y}$)  \n",
        "를 구하세요.\n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "- $(\\frac{\\partial f}{\\partial x} = 2xy$)  \n",
        "- $(\\frac{\\partial f}{\\partial y} = x^2 + 3$)  \n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cbc328",
      "metadata": {},
      "source": [
        "### 문제 2. 기울기 벡터\n",
        "함수 $( f(x,y) = x^2 + y^2 $) 에서  \n",
        "점 $(x,y) = (1,2)$에서의 기울기 벡터 $ \\nabla f(x,y)\\ $를 구하세요.\n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "- $(\\frac{\\partial f}{\\partial x} = 2x = 2(1) = 2$)  \n",
        "\n",
        "- $(\\frac{\\partial f}{\\partial y} = 2y = 2(2) = 4$)  \n",
        "\n",
        "따라서, $\\nabla f(1,2) = (2,4) $ \n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f0b39e",
      "metadata": {},
      "source": [
        "### 문제 3. 다변수 함수의 해석\n",
        "함수 $ f(x,y,z) = xyz $에 대하여  \n",
        "- $(\\frac{\\partial f}{\\partial x}$),   \n",
        "\n",
        "- $(\\frac{\\partial f}{\\partial y}$),  \n",
        "\n",
        "- $(\\frac{\\partial f}{\\partial z}$)  \n",
        "\n",
        "를 구하세요.\n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "- $\\frac{\\partial f}{\\partial x} = yz$  \n",
        "\n",
        "- $\\frac{\\partial f}{\\partial y} = xz$  \n",
        "\n",
        "- $\\frac{\\partial f}{\\partial z} = xy$  \n",
        "\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b8ea2d",
      "metadata": {},
      "source": [
        "### 딥러닝의 역전파 (Backpropagation)와 행렬곱\n",
        "- 신경망은 보통 **행렬곱 연산**으로 입력 → 출력 변환을 수행함  \n",
        "  $$\n",
        "  z = W x + b, \\quad y = \\sigma(z)\n",
        "  $$  \n",
        "  - $x$: 입력 벡터  \n",
        "  - $W$: 가중치 행렬 (파라미터)  \n",
        "  - $b$: 편향 벡터  \n",
        "  - $\\sigma$: 활성화 함수  \n",
        "\n",
        "- **손실 함수 $L$** 에 대해 각 파라미터에 대한 기울기를 계산하면:  \n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial W} = \\delta \\cdot x^T, \\quad \n",
        "  \\frac{\\partial L}{\\partial b} = \\delta\n",
        "  $$  \n",
        "  - 여기서 $\\delta$는 오차 신호 (loss의 기울기)  \n",
        "\n",
        "- **업데이트 규칙 (경사 하강법)**  \n",
        "  $$\n",
        "  W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}, \\quad \n",
        "  b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}\n",
        "  $$  \n",
        "  - $\\eta$: 학습률(learning rate)  \n",
        "  - 즉, 가중치 행렬 $W$도 **행렬 연산**을 통해 기울기에 따라 반복적으로 갱신됨  \n",
        "\n",
        "👉 요약: 역전파는 **행렬곱으로 연결된 파라미터들에 대해 편미분을 계산**하고,  \n",
        "이를 바탕으로 $W$와 $b$를 업데이트하여 손실을 줄이는 과정이다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76a091ae",
      "metadata": {},
      "source": [
        "### 역전파와 행렬 예시\n",
        "\n",
        "간단히 입력 $x$, 가중치 행렬 $W$, 출력 $z$를 예로 들어보자.\n",
        "\n",
        "$$\n",
        "x = \n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "W = \n",
        "\\begin{bmatrix}\n",
        "0.5 & -0.2 \\\\\n",
        "0.3 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "앞전파(Forward Pass):\n",
        "\n",
        "$$\n",
        "z = W x =\n",
        "\\begin{bmatrix}\n",
        "0.5 & -0.2 \\\\\n",
        "0.3 & 0.8\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "1.9\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca46aa85",
      "metadata": {},
      "source": [
        "손실 함수 $L$에 대한 기울기(오차 신호) $\\delta$가 다음과 같다고 하자.\n",
        "\n",
        "$$\n",
        "\\delta =\n",
        "\\begin{bmatrix}\n",
        "0.2 \\\\\n",
        "-0.1\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bcd4f56",
      "metadata": {},
      "source": [
        "가중치에 대한 편미분은 외적 형태로 계산됨:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\delta \\cdot x^T =\n",
        "\\begin{bmatrix}\n",
        "0.2 \\\\\n",
        "-0.1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.4 \\\\\n",
        "-0.1 & -0.2\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e620f5d8",
      "metadata": {},
      "source": [
        "업데이트 규칙 (학습률 $\\eta = 0.1$):\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{\\text{new}} =\n",
        "\\begin{bmatrix}\n",
        "0.5 & -0.2 \\\\\n",
        "0.3 & 0.8\n",
        "\\end{bmatrix}\n",
        "-\n",
        "0.1 \\times\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.4 \\\\\n",
        "-0.1 & -0.2\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.48 & -0.24 \\\\\n",
        "0.31 & 0.82\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c880e634",
      "metadata": {},
      "source": [
        "### ✅ 정리:  \n",
        "- 입력 $x$와 오차 신호 $\\delta$로부터 기울기 행렬을 계산  \n",
        "- 그 기울기를 이용해 $W$를 조금씩 갱신  \n",
        "- 이 과정이 모든 층(layer)에서 반복되며, **신경망의 학습**이 이루어진다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991e67a4",
      "metadata": {},
      "source": [
        "경사하강법과 오차역전파(Backpropagation)에 대한 이해 및 정리를 위해 아래 영상을 참고하시기 바랍니다.  \n",
        "\n",
        "🔗 [신박 AI님의 역전파 알고리즘 강의](https://youtu.be/DMCJ_GjBXwc?si=a37mmPdPTjMeo9-M)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a731e4c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c285b766",
      "metadata": {},
      "source": [
        "### 적분 (Integration)\n",
        "- **적분의 의미**: 함수 아래의 넓이(누적값)를 계산하는 것  \n",
        "  $\\int_a^b f(x)\\, dx \\;=\\; \\text{f(x)의 } a \\text{부터 } b \\text{까지의 누적값}$\n",
        "\n",
        "- **머신러닝에서의 활용**\n",
        "  1. **확률분포**  \n",
        "     - 확률밀도함수(pdf) $f(x)$의 전체 면적 = 1  \n",
        "     - 어떤 구간의 확률 = 구간에 대한 면적  \n",
        "       $$\n",
        "       P(a \\leq X \\leq b) = \\int_a^b f(x)\\, dx\n",
        "       $$  \n",
        "\n",
        "  2. **기대값 계산**  \n",
        "     - 연속확률변수의 기대값은 적분으로 구함  \n",
        "       $$\n",
        "       E[X] = \\int_{-\\infty}^\\infty x f(x)\\, dx\n",
        "       $$  \n",
        "     - 예: $X \\sim U(0,1)$ (균등분포)라면  \n",
        "       $$\n",
        "       E[X] = \\int_0^1 x \\, dx = 0.5\n",
        "       $$  \n",
        "\n",
        "👉 정리  \n",
        "- **미분**: 기울기를 구해 최솟값(손실 최소점)을 찾는 도구  \n",
        "- **편미분·기울기**: 고차원 문제에서 모든 변수에 대한 기울기를 구해 학습에 사용  \n",
        "- **역전파**: 행렬곱으로 연결된 신경망의 파라미터에 대해 편미분을 계산하고 업데이트  \n",
        "- **적분**: 확률분포와 기대값 계산에 필수, 데이터가 어떻게 분포하는지 이해하는 기초 도구  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d95448",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import integrate\n",
        "\n",
        "result, _ = integrate.quad(lambda x: x**2, 0, 1)  # ∫0~1 x^2 dx\n",
        "print(\"적분 결과:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85249896",
      "metadata": {},
      "source": [
        "### 머신러닝에서의 필요성\n",
        "- **미분**: 손실 함수 최소화 (경사하강법)  \n",
        "- **편미분**: 고차원 파라미터 최적화 (신경망 학습)  \n",
        "- **적분**: 확률 분포에서 면적, 누적 확률 계산  \n",
        "\n",
        "\n",
        "\n",
        "### ✅ 체크포인트\n",
        "- 함수는 입력을 출력으로 바꾸는 규칙이며, 머신러닝 모델은 대부분 함수로 표현된다.  \n",
        "- 미분은 기울기를 계산하여 최적화에 활용된다.  \n",
        "- 편미분과 기울기는 고차원 데이터에서 필수적이다.  \n",
        "- 적분은 확률 분포와 기대값 계산에 자주 사용된다.  \n",
        "- 머신러닝 학습 과정(경사하강법, 역전파)은 미적분 개념에 기반한다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c535667d",
      "metadata": {},
      "source": [
        "## 3. 종합 연결: 수학과 머신러닝의 만남\n",
        "\n",
        "앞서 살펴본 **선형대수학, 확률·통계, 미적분학**은 단순한 이론이 아니라  \n",
        "머신러닝 알고리즘의 **핵심 도구**입니다.\n",
        "\n",
        "## 3.1 선형대수학 → 데이터 표현과 변환\n",
        "- 데이터는 벡터/행렬 형태로 저장됨  \n",
        "- 행렬 곱을 통해 입력 데이터 × 가중치 = 예측값  \n",
        "- 차원 축소 (PCA) → 고차원 데이터를 저차원으로 투영  \n",
        "- 거리와 내적 → 데이터 간 유사도 계산  \n",
        "\n",
        "👉 예: KNN 분류 (거리 기반), 추천 시스템 (코사인 유사도), PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe3ed5c",
      "metadata": {},
      "source": [
        "## 3.2 확률·통계 → 불확실성 모델링\n",
        "- 현실의 데이터는 항상 불확실성을 포함  \n",
        "- 확률분포는 데이터를 설명하는 기본 틀  \n",
        "- 조건부 확률과 베이즈 정리 → 분류 문제에 활용  \n",
        "- 통계적 추론(평균, 분산, 표준편차)은 데이터 탐색과 모델 평가의 기본  \n",
        "\n",
        "👉 예: 스팸메일 필터(베이즈 분류기), GMM(군집화), 로지스틱 회귀(확률 기반 분류)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f57e4bc",
      "metadata": {},
      "source": [
        "## 3.3 미적분학 → 최적화와 학습\n",
        "- 머신러닝의 목표 = **손실 함수 최소화**  \n",
        "- 미분: 함수의 기울기를 계산 → 경사하강법으로 최적화  \n",
        "- 편미분과 기울기 벡터: 다차원 파라미터 최적화에 사용  \n",
        "- 적분: 확률 분포의 면적 계산, 기대값 구하기  \n",
        "\n",
        "👉 예: 신경망 학습(역전파), 회귀분석(최소제곱법), 경사하강법"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229038cf",
      "metadata": {},
      "source": [
        "## 3.4 전체 그림\n",
        "머신러닝 = **데이터(선형대수학) + 불확실성(확률·통계) + 최적화(미적분학)**\n",
        "\n",
        "- **선형대수학**: 데이터를 구조화하고 변환  \n",
        "- **확률·통계**: 불확실성을 수학적으로 모델링  \n",
        "- **미적분학**: 모델을 학습시키는 최적화 과정 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b17b33b3",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 머신러닝은 수학적 개념들의 종합적인 응용이다.  \n",
        "- 벡터/행렬(선형대수학)은 데이터를 다루는 언어이다.  \n",
        "- 확률·통계는 데이터의 불확실성을 다루는 도구이다.  \n",
        "- 미적분학은 모델을 학습시키는 최적화의 핵심이다.  \n",
        "- 이 세 가지가 결합되어 현대 머신러닝 알고리즘이 동작한다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89ce8ff",
      "metadata": {},
      "source": [
        "### 머신러닝 기본기 실습 문제\n",
        "\n",
        "\n",
        "\n",
        "### 문제 1. 벡터와 거리\n",
        "두 벡터 `x = [3, 4]`, `y = [0, 0]`의 유클리드 거리를 계산하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([3, 4])\n",
        "y = np.array([0, 0])\n",
        "\n",
        "dist = np.linalg.norm(x - y)\n",
        "print(\"거리:\", dist)  # 5.0\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc0d946b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([3, 4])\n",
        "y = np.array([0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c751eea",
      "metadata": {},
      "source": [
        "### 문제 2. 내적과 코사인 유사도\n",
        "벡터 `a = [1, 2]`, `b = [2, 3]`의 내적과 코사인 유사도를 계산하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "a = np.array([1, 2])\n",
        "b = np.array([2, 3])\n",
        "\n",
        "dot = np.dot(a, b)\n",
        "cos_sim = dot / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "print(\"내적:\", dot)            # 8\n",
        "print(\"코사인 유사도:\", cos_sim)  # 약 0.98\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f985bf7",
      "metadata": {},
      "source": [
        "### 문제 3. 확률 시뮬레이션\n",
        "동전을 100번 던졌을 때 앞면이 나오는 비율을 시뮬레이션하세요. (난수 시드 고정)  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "np.random.seed(42)\n",
        "trials = np.random.choice([\"앞\", \"뒤\"], size=100, p=[0.5, 0.5])\n",
        "ratio = np.mean(trials == \"앞\")\n",
        "print(\"앞면 비율:\", ratio)\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd82bc7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7c7e5f",
      "metadata": {},
      "source": [
        "### 문제 4. 정규분포 난수 생성\n",
        "평균 50, 표준편차 10인 정규분포에서 1000개의 표본을 생성하고, 평균과 분산을 출력하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "print(\"표본 평균:\", np.mean(data))\n",
        "print(\"표본 분산:\", np.var(data))\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "47c50dcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f091f1",
      "metadata": {},
      "source": [
        "### 문제 5. 조건부 확률\n",
        "학생 100명 중 40명이 수학을 좋아하고, 그 중 30명이 과학도 좋아합니다.  \n",
        "이때, 수학을 좋아하는 학생이 과학도 좋아할 확률 \\(P(과학|수학)\\)을 구하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "P_math = 40/100\n",
        "P_math_and_science = 30/100\n",
        "P_science_given_math = P_math_and_science / P_math\n",
        "\n",
        "print(\"P(과학|수학):\", P_science_given_math)  # 0.75\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d2d6e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코딩 없이 풀 수 있는 문제입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181e5adb",
      "metadata": {},
      "source": [
        "### 문제 6. 미분\n",
        "함수 $ f(x) = x^2 $의 도함수는 $ f'(x) = 2x $ 입니다.  \n",
        "x=3일 때 도함수 값을 직접 계산하세요.  \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "x = 3\n",
        "grad = 2*x\n",
        "print(\"f'(3) =\", grad)  # 6\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf01c74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 코딩 없이 풀 수 있는 문제입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85bc7ccd",
      "metadata": {},
      "source": [
        "### 문제 7. 경사하강법\n",
        "다음 함수의 최소값을 구하고자 합니다.  \n",
        "$ f(x) = x^2 $\n",
        "\n",
        "1. 경사하강법을 이용하여 $f(x)$를 최소화하세요.\n",
        "2. 초기값은 x=5, 학습률 lr는 0.1로 설정합니다. \n",
        "3. 업데이트 식은 다음과 같습니다.  \n",
        "\n",
        "    $ x_{t+1} = x_t - lr \\cdot f'(x_t), \\quad \\text{단 } f'(x) = 2x $\n",
        "\n",
        "4. 위 식을 이용하여 **10회 반복 후의 $x$ 값과 $f(x)$ 값**을 구하세요 \n",
        "\n",
        "<details>\n",
        "<summary>정답 보기</summary>\n",
        "\n",
        "```python\n",
        "x = 5\n",
        "lr = 0.1\n",
        "\n",
        "for i in range(10):\n",
        "    grad = 2*x\n",
        "    x = x - lr*grad\n",
        "    print(f\"{i+1}회차: x={x:.4f}, f(x)={x**2:.4f}\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ab22cec8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 정답을 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f3be53b",
      "metadata": {},
      "source": [
        "### ✅ 체크포인트\n",
        "- 선형대수학 → 벡터, 거리, 내적, 유사도  \n",
        "- 확률·통계 → 분포, 조건부 확률, 시뮬레이션  \n",
        "- 미적분학 → 함수의 변화율, 최적화(경사하강법)  \n",
        "- 이 수학적 기초들이 합쳐져 머신러닝 모델 학습이 가능해진다.  "
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "MS04X+uouOyLoOufrOuLnV/quLDrs7jquLAuaXB5bmI=",
    "inserted_date": [
      "2025-09-19"
    ],
    "kernelspec": {
      "display_name": "py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
